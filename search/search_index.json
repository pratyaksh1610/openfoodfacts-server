{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Open Food Facts - Product Opener (Web Server) # Tests # What is Product Opener? # Product Opener is the server software for Open Food Facts and Open Beauty Facts . It is released under the AGPL license and is being developed in Perl, HTML and JavaScript as Free and Open-Source Software . It works together with Robotoff , Open Food Facts' AI system (in Python, which can also be installed locally) and the Open Food Facts apps (which can work with your local instance after enabling dev mode) What is Open Food Facts? # A food product database # Open Food Facts is a database of food products with ingredients, allergens, nutritional facts and all the tidbits of information that is available on various product labels. Made by everyone # Open Food Facts is a non-profit association of volunteers. 25.000+ contributors like you have added 1.7 million + products from 150 countries using our Android, iPhone or Windows Phone app or their camera to scan barcodes and upload pictures of products and their labels. For everyone # Data about food is of public interest and has to be open (i.e available to everyone). The complete database is published as open data and can be reused by anyone and for any use. Check-out the cool reuses or make your own! https://world.openfoodfacts.org Priorities # Top issues P1 problems P1 candidates Please add roadmaps here How do I get started? # Join us on Slack at https://openfoodfacts.slack.com/ in the channels: #api , #productopener , #dev . API Documentation NEW API Documentation (WIP) ( source ) Developer documentation: Quick start guide (Docker) Developer guide (Docker) Developer guide (Gitpod) Configuration [TBA] Dependencies [TBA] Database configuration [TBA] How to run tests [TBA] Perl modules documentation (POD) Note: documentation follows the Di\u00e1taxis Framework Contribution guidelines # If you're new to Open-Source, we recommend you to check out our Contributing Guidelines . Feel free to fork the project and send us a pull request. Writing tests Code review Other guidelines Please add new features to the CHANGELOG.md file before or after merge to make testing easier Reporting problems or asking for a feature # Have a bug or a feature request? Please search for existing and closed issues. If your problem or idea is not addressed yet, please open a new issue . You can ask directly in the discussion room if you're not sure Translate Open Food Facts in your language # You can help translate the Open Food Facts web version and the app at : https://translate.openfoodfacts.org/ (no technical knowledge required, takes a minute to signup) Helping with HTML and CSS # We have templatized Product Opener, we use Gulp and NPM, but you'll need to run the Product Opener docker to be able to see the result (see the How do I get set up? section). In particular, you can help with issues on the new design . Who do I talk to? # Join our discussion room at https://slack.openfoodfacts.org/ Make sure to join the #productopener and #productopener-alerts channels. St\u00e9phane, Pierre, Charles or Hangy will be around to help you get started. Contributors # This project exists thanks to all the people who contribute. Backers # Thank you to all our backers! \ud83d\ude4f [ Become a backer ] Sponsors # Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [ Become a sponsor ] Open Food Facts Personal Search project was funded through the NGI0 Discovery Fund, a fund established by NLnet with financial support from the European Commission's Next Generation Internet programme.","title":"Home"},{"location":"#open-food-facts-product-opener-web-server","text":"","title":"Open Food Facts - Product Opener (Web Server)"},{"location":"#tests","text":"","title":"Tests"},{"location":"#what-is-product-opener","text":"Product Opener is the server software for Open Food Facts and Open Beauty Facts . It is released under the AGPL license and is being developed in Perl, HTML and JavaScript as Free and Open-Source Software . It works together with Robotoff , Open Food Facts' AI system (in Python, which can also be installed locally) and the Open Food Facts apps (which can work with your local instance after enabling dev mode)","title":"What is Product Opener?"},{"location":"#what-is-open-food-facts","text":"","title":"What is Open Food Facts?"},{"location":"#a-food-product-database","text":"Open Food Facts is a database of food products with ingredients, allergens, nutritional facts and all the tidbits of information that is available on various product labels.","title":"A food product database"},{"location":"#made-by-everyone","text":"Open Food Facts is a non-profit association of volunteers. 25.000+ contributors like you have added 1.7 million + products from 150 countries using our Android, iPhone or Windows Phone app or their camera to scan barcodes and upload pictures of products and their labels.","title":"Made by everyone"},{"location":"#for-everyone","text":"Data about food is of public interest and has to be open (i.e available to everyone). The complete database is published as open data and can be reused by anyone and for any use. Check-out the cool reuses or make your own! https://world.openfoodfacts.org","title":"For everyone"},{"location":"#priorities","text":"Top issues P1 problems P1 candidates Please add roadmaps here","title":"Priorities"},{"location":"#how-do-i-get-started","text":"Join us on Slack at https://openfoodfacts.slack.com/ in the channels: #api , #productopener , #dev . API Documentation NEW API Documentation (WIP) ( source ) Developer documentation: Quick start guide (Docker) Developer guide (Docker) Developer guide (Gitpod) Configuration [TBA] Dependencies [TBA] Database configuration [TBA] How to run tests [TBA] Perl modules documentation (POD) Note: documentation follows the Di\u00e1taxis Framework","title":"How do I get started?"},{"location":"#contribution-guidelines","text":"If you're new to Open-Source, we recommend you to check out our Contributing Guidelines . Feel free to fork the project and send us a pull request. Writing tests Code review Other guidelines Please add new features to the CHANGELOG.md file before or after merge to make testing easier","title":"Contribution guidelines"},{"location":"#reporting-problems-or-asking-for-a-feature","text":"Have a bug or a feature request? Please search for existing and closed issues. If your problem or idea is not addressed yet, please open a new issue . You can ask directly in the discussion room if you're not sure","title":"Reporting problems or asking for a feature"},{"location":"#translate-open-food-facts-in-your-language","text":"You can help translate the Open Food Facts web version and the app at : https://translate.openfoodfacts.org/ (no technical knowledge required, takes a minute to signup)","title":"Translate Open Food Facts in your language"},{"location":"#helping-with-html-and-css","text":"We have templatized Product Opener, we use Gulp and NPM, but you'll need to run the Product Opener docker to be able to see the result (see the How do I get set up? section). In particular, you can help with issues on the new design .","title":"Helping with HTML and CSS"},{"location":"#who-do-i-talk-to","text":"Join our discussion room at https://slack.openfoodfacts.org/ Make sure to join the #productopener and #productopener-alerts channels. St\u00e9phane, Pierre, Charles or Hangy will be around to help you get started.","title":"Who do I talk to?"},{"location":"#contributors","text":"This project exists thanks to all the people who contribute.","title":"Contributors"},{"location":"#backers","text":"Thank you to all our backers! \ud83d\ude4f [ Become a backer ]","title":"Backers"},{"location":"#sponsors","text":"Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [ Become a sponsor ] Open Food Facts Personal Search project was funded through the NGI0 Discovery Fund, a fund established by NLnet with financial support from the European Commission's Next Generation Internet programme.","title":"Sponsors"},{"location":"explanations/packaging-data/","text":"Packaging data # This document explains how packaging data is currently added, updated and structured in the Open Food Facts database, and how it could be improved. Introduction # Types of packaging data # Food products typically have 1 or more packaging components (e.g. milk may have a bottle and a cap). For each product, we aim to have a comprehensive list of all its packaging components, with detailed information about each packaging component. Data about packaging components # For each packaging component, we want data for different attributes, like its shape (e.g. a bottle) and its size (e.g. plastic). There are many different attributes that can be interesting for specific uses. For instance, researchers in epidemiology are interested in knowing which packaging component is in contact with the food itself, and which one can be put in the microwave oven, so that they can study the long term effects of some plastics on health. Sources of packaging data # We can get packaging data from different sources: Users # Users of the Open Food Facts website and app, and users of 3rd party apps, can enter packaging data. Manufacturers # Some manufacturers send product data through GS1, which currently has limited support for packaging information (but this is likely to be improved in the years to come). Some manufacturers send us more detailed packaging data (e.g. recycling instructions) through the Producers Platform. Some manufacturers send us data used to compute the Eco-Score using the Eco-Score spreadsheet template, which has fields like \"Packaging 1\", \"Material 1\", \"Packaging 2\", \"Material 2\" etc. Product photos and machine learning # We can extract logos related to packaging, or parse the text recognized from product photos to recognize packaging information or recycling instructions. How packaging data is currently added, updated and structured in Open Food Facts # In Open Food Facts, we currently have a number of input fields related to packaging. The data in those fields is parsed and analyzed to create a structured list of packaging components with specific attributes. Current input fields # Packaging tag field (READ and WRITE) # At the start of Open Food Facts in 2012, we had a \"packaging\" tag field where users could enter comma separated free text entries about the packaging (e.g. \"Plastic\", \"Bag\" or \"Plastic bag\") in different languages. In 2020, we made this field a taxonomized field. As a result, we now store the language used to fill this field, so that we can match its value to the multilingual packaging taxonomy. So \"plastique\" in French will be mapped to the canonical \"en:plastic\" entry. Packaging information / recycling instructions text field (READ and WRITE) # In 2020, we also added a language specific field (\"packaging_text_[language code]\" e.g. \"packaging_text_en\" for English) to store free text data about the packaging. It can contain the text of the recycling instructions printed on the packaging (e.g. \"bottle to recycle, cap to discard\"), or can be filled in by users (e.g. \"1 PET plastic bottle to recycle, 1 plastic cap\"). Current resulting packagings data structure (READ only) # The input fields are analyzed and combined to create the \"packagings\" data structure. The structure is an array of packaging components. Each packaging component can have values for different attributes: number: the number of units for the packaging component (e.g. a pack of beers may contain 6 bottles) shape: the general shape of the packaging component (e.g. \"bottle\", \"box\") material: the material of the packaging component quantity: how much product the packaging component contains (e.g. \"25 cl\") recycling: whether the packaging component should be recycled, discarded or reused The \"shape\" and \"material\" fields are taxonomized using the packaging_shapes and packaging_materials taxonomies. How the the resulting packagings data structure is created # Extract attributes that relate to different packaging components # The values for each input field (\"packaging\" tag field and \"packaging_text_[language code]\" packaging information text field) are analyzed 1 to recognize packaging components and their attributes. One product may have multiple \"packaging_text_[language code]\" values in different languages. Only the value for the main product of the language is currently analyzed. For instance, if the \"packaging\" field contains \"Plastic bottle, box, cardboard\", we will use the packaging shapes, materials and recycling taxonomies to create a list of 3 packaging components: {shape:\"en:bottle\", material:\"en:plastic\"}, {shape:\"en:box\"}, {material:\"en:cardboard\"}. And if the \"packaging_text_en\" field contains \"PET bottle to recycle, box to reuse\", we will create 2 more packaging components: {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"}, {shape:\"box\", recycling:\"reuse\"}. Merge packaging components # The 3 + 2 = 5 resulting packaging components are then added one by one in the packagings structure. When their attributes are compatible, the packaging units are merged 2 . For instance {shape:\"en:box\"} and {material:\"en:cardboard\"} have non conflicting attributes, so they are merged into {shape:\"en:box\", material:\"en:cardboard\"}. Note that it is possible that this is a mistake, and that the \"box\" and \"cardboard\" tags concern in fact different components. Similarly, as \"en:plastic\" is a parent of \"en:pet-polyethylene-terephthalate\" in the packaging_materials taxonomy, we can merge {shape:\"en:bottle\", material:\"en:plastic\"} with {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"} into {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"}. The resulting structure is: packagings: [ { material: \"en:pet-polyethylene-terephthalate\", recycling: \"en:recycle\", shape: \"en:bottle\" }, { recycling: \"en:reuse\", shape: \"en:box\" }, { shape: \"en:container\" } ] Taxonomies # We have created a number of multilingual taxonomies related to packagings: Packaging shapes taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_shapes.txt Packaging materials taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_materials.txt Packaging recycling taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_recycling.txt Preservation methods taxonomy (related) : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/preservation.txt Those taxonomies are used to structure packaging data in Open Food Facts, and to analyze unstructured input data. How we could improve it # Extend the attributes of the packaging components in the \"packagings\" data structure # Weight # We need to add an attribute for the weight of the packaging component. We might need to add different fields to distinguish values that have been entered by users that weight the packaging, versus values provided by the manufacturer, or average values that we have determined from other products, or that we got from external sources. Make the \"packagings\" data structure READ and WRITE # The \"packagings\" data structure is currently a READ only field. We could create an API to make it a READ and WRITE field. For new products, clients (website and apps) could ask users to enter data about all packaging components of the product. For existing products, clients could display the packaging components and let users change them (e.g. adding or removing components, entering values for new attributes, editing attributes to add more precise values (e.g. which type of plastic) etc.). Add a way to indicate that the \"packagings\" data structure contains all the packaging components of the product # We currently have no way to know if the packaging data we have for a product is complete, or if we may be missing some packaging components. We could have a way (e.g. a checkbox) that users could use to indicate all components are accounted for. And we could also do the reverse, and indicate that it is very likely that we are missing some packaging components (e.g. if we have a \"cap\" but no other component to put the cap on). Deprecate the \"packaging\" tags field # We could discard the existing \"packaging\" tags field, and replace it with an API to allow clients to add partial information about packaging components. For instance, if Robotoff detects that the product is in plastic bottle by analyzing a product photo, it could send {shape:\"bottle\", material:\"en:plastic\"} and it would be added / combined with the existing \"packagings\" data. Keep the \"packaging_text_[language code]\" field # It is important to keep this field, as we can display it as-is, use it as input data, and it may contain interesting data that we do not analyze yet. When filled, the values for this field can be analyzed and added to / combined with the \"packagings\" data structure. Similarly to ingredient text analysis, we could keep information about which parts of the text were recognized as attributes of a packaging component, and which parts were not recognized and were therefore ignored. Changing the \"packagings\" value will not change the \"packaging_text_[language code]\" values. Challenges # Incomplete lists of packaging components # Slightly mismatched data from different sources # For a single product, we might get partial packaging data from different sources that we map to similar but distinct shapes, like \"bottle\", \"jar\" and \"jug\". It may be difficult to determine if the data concerns a single packaging component, or different components. Products with packaging changes # Ressources # 2020 project to start structuring packaging data: https://wiki.openfoodfacts.org/Packagings_data_structure parse_packaging_from_text_phrase() function in /lib/ProductOpener/Packagings.pm \u21a9 analyze_and_combine_packaging_data() function in /lib/ProductOpener/Packagings.pm \u21a9","title":"Packaging data"},{"location":"explanations/packaging-data/#packaging-data","text":"This document explains how packaging data is currently added, updated and structured in the Open Food Facts database, and how it could be improved.","title":"Packaging data"},{"location":"explanations/packaging-data/#introduction","text":"","title":"Introduction"},{"location":"explanations/packaging-data/#types-of-packaging-data","text":"Food products typically have 1 or more packaging components (e.g. milk may have a bottle and a cap). For each product, we aim to have a comprehensive list of all its packaging components, with detailed information about each packaging component.","title":"Types of packaging data"},{"location":"explanations/packaging-data/#data-about-packaging-components","text":"For each packaging component, we want data for different attributes, like its shape (e.g. a bottle) and its size (e.g. plastic). There are many different attributes that can be interesting for specific uses. For instance, researchers in epidemiology are interested in knowing which packaging component is in contact with the food itself, and which one can be put in the microwave oven, so that they can study the long term effects of some plastics on health.","title":"Data about packaging components"},{"location":"explanations/packaging-data/#sources-of-packaging-data","text":"We can get packaging data from different sources:","title":"Sources of packaging data"},{"location":"explanations/packaging-data/#users","text":"Users of the Open Food Facts website and app, and users of 3rd party apps, can enter packaging data.","title":"Users"},{"location":"explanations/packaging-data/#manufacturers","text":"Some manufacturers send product data through GS1, which currently has limited support for packaging information (but this is likely to be improved in the years to come). Some manufacturers send us more detailed packaging data (e.g. recycling instructions) through the Producers Platform. Some manufacturers send us data used to compute the Eco-Score using the Eco-Score spreadsheet template, which has fields like \"Packaging 1\", \"Material 1\", \"Packaging 2\", \"Material 2\" etc.","title":"Manufacturers"},{"location":"explanations/packaging-data/#product-photos-and-machine-learning","text":"We can extract logos related to packaging, or parse the text recognized from product photos to recognize packaging information or recycling instructions.","title":"Product photos and machine learning"},{"location":"explanations/packaging-data/#how-packaging-data-is-currently-added-updated-and-structured-in-open-food-facts","text":"In Open Food Facts, we currently have a number of input fields related to packaging. The data in those fields is parsed and analyzed to create a structured list of packaging components with specific attributes.","title":"How packaging data is currently added, updated and structured in Open Food Facts"},{"location":"explanations/packaging-data/#current-input-fields","text":"","title":"Current input fields"},{"location":"explanations/packaging-data/#packaging-tag-field-read-and-write","text":"At the start of Open Food Facts in 2012, we had a \"packaging\" tag field where users could enter comma separated free text entries about the packaging (e.g. \"Plastic\", \"Bag\" or \"Plastic bag\") in different languages. In 2020, we made this field a taxonomized field. As a result, we now store the language used to fill this field, so that we can match its value to the multilingual packaging taxonomy. So \"plastique\" in French will be mapped to the canonical \"en:plastic\" entry.","title":"Packaging tag field (READ and WRITE)"},{"location":"explanations/packaging-data/#packaging-information-recycling-instructions-text-field-read-and-write","text":"In 2020, we also added a language specific field (\"packaging_text_[language code]\" e.g. \"packaging_text_en\" for English) to store free text data about the packaging. It can contain the text of the recycling instructions printed on the packaging (e.g. \"bottle to recycle, cap to discard\"), or can be filled in by users (e.g. \"1 PET plastic bottle to recycle, 1 plastic cap\").","title":"Packaging information / recycling instructions text field (READ and WRITE)"},{"location":"explanations/packaging-data/#current-resulting-packagings-data-structure-read-only","text":"The input fields are analyzed and combined to create the \"packagings\" data structure. The structure is an array of packaging components. Each packaging component can have values for different attributes: number: the number of units for the packaging component (e.g. a pack of beers may contain 6 bottles) shape: the general shape of the packaging component (e.g. \"bottle\", \"box\") material: the material of the packaging component quantity: how much product the packaging component contains (e.g. \"25 cl\") recycling: whether the packaging component should be recycled, discarded or reused The \"shape\" and \"material\" fields are taxonomized using the packaging_shapes and packaging_materials taxonomies.","title":"Current resulting packagings data structure (READ only)"},{"location":"explanations/packaging-data/#how-the-the-resulting-packagings-data-structure-is-created","text":"","title":"How the the resulting packagings data structure is created"},{"location":"explanations/packaging-data/#extract-attributes-that-relate-to-different-packaging-components","text":"The values for each input field (\"packaging\" tag field and \"packaging_text_[language code]\" packaging information text field) are analyzed 1 to recognize packaging components and their attributes. One product may have multiple \"packaging_text_[language code]\" values in different languages. Only the value for the main product of the language is currently analyzed. For instance, if the \"packaging\" field contains \"Plastic bottle, box, cardboard\", we will use the packaging shapes, materials and recycling taxonomies to create a list of 3 packaging components: {shape:\"en:bottle\", material:\"en:plastic\"}, {shape:\"en:box\"}, {material:\"en:cardboard\"}. And if the \"packaging_text_en\" field contains \"PET bottle to recycle, box to reuse\", we will create 2 more packaging components: {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"}, {shape:\"box\", recycling:\"reuse\"}.","title":"Extract attributes that relate to different packaging components"},{"location":"explanations/packaging-data/#merge-packaging-components","text":"The 3 + 2 = 5 resulting packaging components are then added one by one in the packagings structure. When their attributes are compatible, the packaging units are merged 2 . For instance {shape:\"en:box\"} and {material:\"en:cardboard\"} have non conflicting attributes, so they are merged into {shape:\"en:box\", material:\"en:cardboard\"}. Note that it is possible that this is a mistake, and that the \"box\" and \"cardboard\" tags concern in fact different components. Similarly, as \"en:plastic\" is a parent of \"en:pet-polyethylene-terephthalate\" in the packaging_materials taxonomy, we can merge {shape:\"en:bottle\", material:\"en:plastic\"} with {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"} into {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"}. The resulting structure is: packagings: [ { material: \"en:pet-polyethylene-terephthalate\", recycling: \"en:recycle\", shape: \"en:bottle\" }, { recycling: \"en:reuse\", shape: \"en:box\" }, { shape: \"en:container\" } ]","title":"Merge packaging components"},{"location":"explanations/packaging-data/#taxonomies","text":"We have created a number of multilingual taxonomies related to packagings: Packaging shapes taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_shapes.txt Packaging materials taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_materials.txt Packaging recycling taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_recycling.txt Preservation methods taxonomy (related) : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/preservation.txt Those taxonomies are used to structure packaging data in Open Food Facts, and to analyze unstructured input data.","title":"Taxonomies"},{"location":"explanations/packaging-data/#how-we-could-improve-it","text":"","title":"How we could improve it"},{"location":"explanations/packaging-data/#extend-the-attributes-of-the-packaging-components-in-the-packagings-data-structure","text":"","title":"Extend the attributes of the packaging components in the \"packagings\" data structure"},{"location":"explanations/packaging-data/#weight","text":"We need to add an attribute for the weight of the packaging component. We might need to add different fields to distinguish values that have been entered by users that weight the packaging, versus values provided by the manufacturer, or average values that we have determined from other products, or that we got from external sources.","title":"Weight"},{"location":"explanations/packaging-data/#make-the-packagings-data-structure-read-and-write","text":"The \"packagings\" data structure is currently a READ only field. We could create an API to make it a READ and WRITE field. For new products, clients (website and apps) could ask users to enter data about all packaging components of the product. For existing products, clients could display the packaging components and let users change them (e.g. adding or removing components, entering values for new attributes, editing attributes to add more precise values (e.g. which type of plastic) etc.).","title":"Make the \"packagings\" data structure READ and WRITE"},{"location":"explanations/packaging-data/#add-a-way-to-indicate-that-the-packagings-data-structure-contains-all-the-packaging-components-of-the-product","text":"We currently have no way to know if the packaging data we have for a product is complete, or if we may be missing some packaging components. We could have a way (e.g. a checkbox) that users could use to indicate all components are accounted for. And we could also do the reverse, and indicate that it is very likely that we are missing some packaging components (e.g. if we have a \"cap\" but no other component to put the cap on).","title":"Add a way to indicate that the \"packagings\" data structure contains all the packaging components of the product"},{"location":"explanations/packaging-data/#deprecate-the-packaging-tags-field","text":"We could discard the existing \"packaging\" tags field, and replace it with an API to allow clients to add partial information about packaging components. For instance, if Robotoff detects that the product is in plastic bottle by analyzing a product photo, it could send {shape:\"bottle\", material:\"en:plastic\"} and it would be added / combined with the existing \"packagings\" data.","title":"Deprecate the \"packaging\" tags field"},{"location":"explanations/packaging-data/#keep-the-packaging_text_language-code-field","text":"It is important to keep this field, as we can display it as-is, use it as input data, and it may contain interesting data that we do not analyze yet. When filled, the values for this field can be analyzed and added to / combined with the \"packagings\" data structure. Similarly to ingredient text analysis, we could keep information about which parts of the text were recognized as attributes of a packaging component, and which parts were not recognized and were therefore ignored. Changing the \"packagings\" value will not change the \"packaging_text_[language code]\" values.","title":"Keep the \"packaging_text_[language code]\" field"},{"location":"explanations/packaging-data/#challenges","text":"","title":"Challenges"},{"location":"explanations/packaging-data/#incomplete-lists-of-packaging-components","text":"","title":"Incomplete lists of packaging components"},{"location":"explanations/packaging-data/#slightly-mismatched-data-from-different-sources","text":"For a single product, we might get partial packaging data from different sources that we map to similar but distinct shapes, like \"bottle\", \"jar\" and \"jug\". It may be difficult to determine if the data concerns a single packaging component, or different components.","title":"Slightly mismatched data from different sources"},{"location":"explanations/packaging-data/#products-with-packaging-changes","text":"","title":"Products with packaging changes"},{"location":"explanations/packaging-data/#ressources","text":"2020 project to start structuring packaging data: https://wiki.openfoodfacts.org/Packagings_data_structure parse_packaging_from_text_phrase() function in /lib/ProductOpener/Packagings.pm \u21a9 analyze_and_combine_packaging_data() function in /lib/ProductOpener/Packagings.pm \u21a9","title":"Ressources"},{"location":"explanations/pro-dev-setup/","text":"Docker Setup of pro platform for development # This explains how we setup docker file for pro platform development. For explanations on how to use it, see: how-to-guides/pro-development off is the public facing application (world.openfoodfacts.org) off-pro is the producers platform (world.pro.openfoodfacts.org) When we work on the pro platform for development we want: * off containers to talk between each other, and have their own volumes * off-pro containers to talk between each other, and, generally, have their own volumes * minion and backend from both app access to the same postgres database (which stores tasks queues) * off and off-pro backends / minion needs to share some volumes : orgs, users ands some files living in podata Still we would like to avoid having different clone of the repository, but we can isolate projects thanks to COMPOSE_PROJECT_NAME , which will prefix containers names, volumes and default network, thus isolate each projects. This is achieved by sourcing the .env-pro which setup some environment variables that will superseed the .env variables. The main one being setting COMPOSE_PROJECT_NAME and PRODUCERS_PLATFORM , but also other like MINION_QUEUE . On volume side, we will simply give hard-coded names to volumes that should be shared between off and pro platform, thus they will be shared. Ideally we should not have to share single files but this is a work in progress, we will live without it as a first approx. To satisfy the access to the same database, we will use postgres database from off as the common database. In order to achieve that: * we use profiles, so we won't start postgres in pro docker-compose * we connect postgres , backend and minion services to a shared network, called minion_db Fortunately this works, but note that there is a pitfall: on minion_db network both backend services ( off and off-pro ) will respond to same name. For the moment it is not a problem for we don't need to communicate directly between instances. If it was, we would have to define custom aliases for those services on the minion_db network. network OFF network PRO network po_default containers minion_db containers po_pro_default | | | +------postgres------------+ | | | | | | | +-----backend--------------+ | | +----------backend-------------+ | | | +------minion--------------+ | | +----------minion--------------+ | | | | | | +------frontend | frontend------------+ +------mongodb | mongodb-------------+ | | |","title":"Docker Setup of pro platform for development"},{"location":"explanations/pro-dev-setup/#docker-setup-of-pro-platform-for-development","text":"This explains how we setup docker file for pro platform development. For explanations on how to use it, see: how-to-guides/pro-development off is the public facing application (world.openfoodfacts.org) off-pro is the producers platform (world.pro.openfoodfacts.org) When we work on the pro platform for development we want: * off containers to talk between each other, and have their own volumes * off-pro containers to talk between each other, and, generally, have their own volumes * minion and backend from both app access to the same postgres database (which stores tasks queues) * off and off-pro backends / minion needs to share some volumes : orgs, users ands some files living in podata Still we would like to avoid having different clone of the repository, but we can isolate projects thanks to COMPOSE_PROJECT_NAME , which will prefix containers names, volumes and default network, thus isolate each projects. This is achieved by sourcing the .env-pro which setup some environment variables that will superseed the .env variables. The main one being setting COMPOSE_PROJECT_NAME and PRODUCERS_PLATFORM , but also other like MINION_QUEUE . On volume side, we will simply give hard-coded names to volumes that should be shared between off and pro platform, thus they will be shared. Ideally we should not have to share single files but this is a work in progress, we will live without it as a first approx. To satisfy the access to the same database, we will use postgres database from off as the common database. In order to achieve that: * we use profiles, so we won't start postgres in pro docker-compose * we connect postgres , backend and minion services to a shared network, called minion_db Fortunately this works, but note that there is a pitfall: on minion_db network both backend services ( off and off-pro ) will respond to same name. For the moment it is not a problem for we don't need to communicate directly between instances. If it was, we would have to define custom aliases for those services on the minion_db network. network OFF network PRO network po_default containers minion_db containers po_pro_default | | | +------postgres------------+ | | | | | | | +-----backend--------------+ | | +----------backend-------------+ | | | +------minion--------------+ | | +----------minion--------------+ | | | | | | +------frontend | frontend------------+ +------mongodb | mongodb-------------+ | | |","title":"Docker Setup of pro platform for development"},{"location":"how-to-guides/deploy/","text":"Prod environment deployment guide # Note: prod deployment is very manual and not automated yet. Login to the off1 server, as the \"off\" user cd /home/off/openfoodfacts-server Check that you are on the main branch git pull Copy changed files (don't copy everything, in particular not the lang directory that is being moved to the openfoodfacts-web repository) e.g. cp cgi scripts lib po taxonomies templates /srv/off/ cd /srv/off export NPM_CONFIG_PREFIX=~/.npm-global npm install npm run build cd /srv/off/cgi export PERL5LIB=. ./build_lang.pl as the root user: systemctl stop apache2@off systemctl start apache2@off systemctl stop minion-off systemctl start minion-off","title":"Prod environment deployment guide"},{"location":"how-to-guides/deploy/#prod-environment-deployment-guide","text":"Note: prod deployment is very manual and not automated yet. Login to the off1 server, as the \"off\" user cd /home/off/openfoodfacts-server Check that you are on the main branch git pull Copy changed files (don't copy everything, in particular not the lang directory that is being moved to the openfoodfacts-web repository) e.g. cp cgi scripts lib po taxonomies templates /srv/off/ cd /srv/off export NPM_CONFIG_PREFIX=~/.npm-global npm install npm run build cd /srv/off/cgi export PERL5LIB=. ./build_lang.pl as the root user: systemctl stop apache2@off systemctl start apache2@off systemctl stop minion-off systemctl start minion-off","title":"Prod environment deployment guide"},{"location":"how-to-guides/docker-developer-guide/","text":"Docker Developer's Guide # This guide is for developers and newcomers to help them debug and explore Docker. This page describes how to test and debug your changes once you have set up the project, Product Opener with Docker using dev environment quick start guide . Checking logs # Tail Docker Compose logs # make log You will get logs from nginx, mongodb, postgres, etc. Tail other logs # Most logs from perl are not (yet ?) displayed on the docker logs, but are instead available in specific directories. To see them use: make tail It will tail -f all the files present in the logs/ directory: apache2/error.log apache2/log4perl.log apache2/modperl_error.log apache2/other_vhosts_access.log nginx/access.log nginx/error.log You can also simply run: tail -f <FILEPATH> to check a specific log. One of the most important is log4perl.log . Increasing log verbosity # By default, the log4perl configuration conf/log.conf matches production settings. You can tweak that file with your own dev configuration settings and run make restart to reload the changes. A setting useful for local environments is to set TRACE log level: log4perl.rootLogger=TRACE, LOGFILE Opening a shell in a Docker container # Run the following to open a bash shell within the backend container: docker-compose exec backend bash You should see root@<CONTAINER_ID>:/# (opened root shell): you are now within the Docker container and can begin typing some commands ! Checking permissions # Navigate to the directory the specific directory and run ls -lrt It will list all the directories and their permission status. Creating directory # Navigate to your specific directory using cd command and run mkdir directory-name Running minion jobs # Minion is a high-performance job queue for Perl. Minion is used in openfoodfacts-server for time-consuming import and export tasks. These tasks are processed and queued using the minion jobs queue. Therefore, they are called minion jobs. Go to /opt/product-opener/scripts and run ./minion_producers.pl minion job The above command will show the status of minion jobs. Run the following command to launch the minion jobs. ./minion_producers.pl minion worker -m production -q pro.openfoodfacts.org Restarting Apache # Sometimes restarting the whole backend container is overkill, and you can just restart Apache from inside the container: apache2ctl -k restart Exiting the container # Use exit to exit the container. Making code changes # In the dev environment, any code change to the local directory will be written on the container. That said, some code changes require a restart of the backend container, or rebuilding the NPM assets. Getting away from make up # make up is a good command for starters, but it's not the right one to use if you develop on a daily bases, because it maybe slow, as it does a full rebuild, which, in dev mode, should only be necessary in a few cases. On a daily bases you could better run those: docker-compose up to start and monitor the stack. docker-compose restart backend to account for a code change in a .pm file (cgi pl files do not need a restart) docker-compose stop to stop them all If some important file changed (like Dockerfile or cpanfile, etc.), or in case of doubt, you can run docker-compose build (or maybe it's a good time to use make up once) You should explore the docker-compose commands most are useful! Live reload # To automate the live reload on code changes, you can install the Python package when-changed : pip3 install when-changed when-changed -r docker/ docker-compose.yml .env -c \"make restart\" # restart backend container on compose changes when-changed -r lib/ -r docker/ docker-compose.yml -c \"docker-compose backend restart\" # restart Apache on code changes when-changed -r html/ Dockerfile Dockerfile.frontend package.json -c \"make up\" # rebuild containers on asset or Dockerfile changes An alternative to when-changed is inotifywait . Run queries on MongoDB database # docker-compose exec mongodb mongo The above command will open a MongoDB shell, allowing you to use all the mongo commands to interact with the database: show dbs use off db.products.count() db.products.find({_id: \"5053990155354\"}) db.products.deleteOne({_id: \"5053990155354\"}) See the mongo shell docs for more commands. Adding environment variables # If you need some value to be configurable, it is best to set is as an environment variable. To add a new environment variable TEST : In .env file, add TEST=test_val [local]. In .github/workflows/container-deploy.yml , add echo \"TEST=${{ secrets.TEST }}\" >> .env to the \"Set environment variables\" build step [remote]. Also add the corresponding GitHub secret TEST=test_val . In docker-compose.yml file, add it under the backend > environment section. In conf/apache.conf file, add PerlPassEnv TEST . In lib/Config2.pm , add $test = $ENV{TEST}; . Also add $test to the EXPORT_OK list at the top of the file to avoid a compilation error. The call stack goes like this: make up > docker-compose > loads .env > pass env variables to the backend container > pass to mod_perl > initialized in Config2.pm . Managing multiple deployments # To juggle between multiple local deployments (e.g: to run different flavors of Open Food Facts on the same host), there are different possible strategies. a set env script # Docker-compose takes it settings from, in order of priority: * the environment * the .env file So one strategy to have a different instance, can be to keep same .env file, but super-seed some env variables to tweak the configuration. This is a good strategy for the pro plateform. For this case we have a setenv-pro.sh script. To use it, open a terminal, where you want to be in pro environment and simply use: . setenv-pro.sh then you can use whatever docker-compose command. Note: This terminal will remain in pro mode until you end its session. See also Developing on the producers platform different .env file # This strategy might be the right one if your settings differs a lot. You will need: Multiple .env files (one per deployment), such as: .env.off : configuration for Open Food Facts dev env. .env.off-pro : configuration for Open Food Facts Producer's Platform dev env. .env.obf : configuration for Open Beauty Facts dev env. .env.opff : configuration for Open Ped Food Facts dev env. COMPOSE_PROJECT_NAME set to different values in each .env file, so that container names across deployments are unique. FRONTEND_PORT and MONGODB_PORT set to different values in each .env file, so that frontend containers don't port-conflict with each other. To switch between configurations, set ENV_FILE before running make commands, (or docker-compose command): ENV_FILE=.env.off-pro make up # starts the OFF Producer's Platform containers. ENV_FILE=.env.obf make up # starts the OBF containers. ENV_FILE=.env.opff make up # starts the OPFF containers. or export it to keep it for a while: export ENV_FILE=.env.off # going to work on OFF for a while make up make restart make down make log A good strategy is to have multiple terminals open, one for each deployment: off [Terminal 1]: export ENV_FILE=.env.off make up off-pro [Terminal 2]: export ENV_FILE=.env.off-pro make up obf [Terminal 3]: export ENV_FILE=.env.obf make up opff [Terminal 3]: export ENV_FILE=.env.opff make up Note: the above case of 4 deployments is a bit ambitious , since ProductOpener's backend container takes about ~6GB of RAM to run, meaning that the above 4 deployments would require a total of 24GB of RAM available.","title":"Docker Developer's Guide"},{"location":"how-to-guides/docker-developer-guide/#docker-developers-guide","text":"This guide is for developers and newcomers to help them debug and explore Docker. This page describes how to test and debug your changes once you have set up the project, Product Opener with Docker using dev environment quick start guide .","title":"Docker Developer's Guide"},{"location":"how-to-guides/docker-developer-guide/#checking-logs","text":"","title":"Checking logs"},{"location":"how-to-guides/docker-developer-guide/#tail-docker-compose-logs","text":"make log You will get logs from nginx, mongodb, postgres, etc.","title":"Tail Docker Compose logs"},{"location":"how-to-guides/docker-developer-guide/#tail-other-logs","text":"Most logs from perl are not (yet ?) displayed on the docker logs, but are instead available in specific directories. To see them use: make tail It will tail -f all the files present in the logs/ directory: apache2/error.log apache2/log4perl.log apache2/modperl_error.log apache2/other_vhosts_access.log nginx/access.log nginx/error.log You can also simply run: tail -f <FILEPATH> to check a specific log. One of the most important is log4perl.log .","title":"Tail other logs"},{"location":"how-to-guides/docker-developer-guide/#increasing-log-verbosity","text":"By default, the log4perl configuration conf/log.conf matches production settings. You can tweak that file with your own dev configuration settings and run make restart to reload the changes. A setting useful for local environments is to set TRACE log level: log4perl.rootLogger=TRACE, LOGFILE","title":"Increasing log verbosity"},{"location":"how-to-guides/docker-developer-guide/#opening-a-shell-in-a-docker-container","text":"Run the following to open a bash shell within the backend container: docker-compose exec backend bash You should see root@<CONTAINER_ID>:/# (opened root shell): you are now within the Docker container and can begin typing some commands !","title":"Opening a shell in a Docker container"},{"location":"how-to-guides/docker-developer-guide/#checking-permissions","text":"Navigate to the directory the specific directory and run ls -lrt It will list all the directories and their permission status.","title":"Checking permissions"},{"location":"how-to-guides/docker-developer-guide/#creating-directory","text":"Navigate to your specific directory using cd command and run mkdir directory-name","title":"Creating directory"},{"location":"how-to-guides/docker-developer-guide/#running-minion-jobs","text":"Minion is a high-performance job queue for Perl. Minion is used in openfoodfacts-server for time-consuming import and export tasks. These tasks are processed and queued using the minion jobs queue. Therefore, they are called minion jobs. Go to /opt/product-opener/scripts and run ./minion_producers.pl minion job The above command will show the status of minion jobs. Run the following command to launch the minion jobs. ./minion_producers.pl minion worker -m production -q pro.openfoodfacts.org","title":"Running minion jobs"},{"location":"how-to-guides/docker-developer-guide/#restarting-apache","text":"Sometimes restarting the whole backend container is overkill, and you can just restart Apache from inside the container: apache2ctl -k restart","title":"Restarting Apache"},{"location":"how-to-guides/docker-developer-guide/#exiting-the-container","text":"Use exit to exit the container.","title":"Exiting the container"},{"location":"how-to-guides/docker-developer-guide/#making-code-changes","text":"In the dev environment, any code change to the local directory will be written on the container. That said, some code changes require a restart of the backend container, or rebuilding the NPM assets.","title":"Making code changes"},{"location":"how-to-guides/docker-developer-guide/#getting-away-from-make-up","text":"make up is a good command for starters, but it's not the right one to use if you develop on a daily bases, because it maybe slow, as it does a full rebuild, which, in dev mode, should only be necessary in a few cases. On a daily bases you could better run those: docker-compose up to start and monitor the stack. docker-compose restart backend to account for a code change in a .pm file (cgi pl files do not need a restart) docker-compose stop to stop them all If some important file changed (like Dockerfile or cpanfile, etc.), or in case of doubt, you can run docker-compose build (or maybe it's a good time to use make up once) You should explore the docker-compose commands most are useful!","title":"Getting away from make up"},{"location":"how-to-guides/docker-developer-guide/#live-reload","text":"To automate the live reload on code changes, you can install the Python package when-changed : pip3 install when-changed when-changed -r docker/ docker-compose.yml .env -c \"make restart\" # restart backend container on compose changes when-changed -r lib/ -r docker/ docker-compose.yml -c \"docker-compose backend restart\" # restart Apache on code changes when-changed -r html/ Dockerfile Dockerfile.frontend package.json -c \"make up\" # rebuild containers on asset or Dockerfile changes An alternative to when-changed is inotifywait .","title":"Live reload"},{"location":"how-to-guides/docker-developer-guide/#run-queries-on-mongodb-database","text":"docker-compose exec mongodb mongo The above command will open a MongoDB shell, allowing you to use all the mongo commands to interact with the database: show dbs use off db.products.count() db.products.find({_id: \"5053990155354\"}) db.products.deleteOne({_id: \"5053990155354\"}) See the mongo shell docs for more commands.","title":"Run queries on MongoDB database"},{"location":"how-to-guides/docker-developer-guide/#adding-environment-variables","text":"If you need some value to be configurable, it is best to set is as an environment variable. To add a new environment variable TEST : In .env file, add TEST=test_val [local]. In .github/workflows/container-deploy.yml , add echo \"TEST=${{ secrets.TEST }}\" >> .env to the \"Set environment variables\" build step [remote]. Also add the corresponding GitHub secret TEST=test_val . In docker-compose.yml file, add it under the backend > environment section. In conf/apache.conf file, add PerlPassEnv TEST . In lib/Config2.pm , add $test = $ENV{TEST}; . Also add $test to the EXPORT_OK list at the top of the file to avoid a compilation error. The call stack goes like this: make up > docker-compose > loads .env > pass env variables to the backend container > pass to mod_perl > initialized in Config2.pm .","title":"Adding environment variables"},{"location":"how-to-guides/docker-developer-guide/#managing-multiple-deployments","text":"To juggle between multiple local deployments (e.g: to run different flavors of Open Food Facts on the same host), there are different possible strategies.","title":"Managing multiple deployments"},{"location":"how-to-guides/docker-developer-guide/#a-set-env-script","text":"Docker-compose takes it settings from, in order of priority: * the environment * the .env file So one strategy to have a different instance, can be to keep same .env file, but super-seed some env variables to tweak the configuration. This is a good strategy for the pro plateform. For this case we have a setenv-pro.sh script. To use it, open a terminal, where you want to be in pro environment and simply use: . setenv-pro.sh then you can use whatever docker-compose command. Note: This terminal will remain in pro mode until you end its session. See also Developing on the producers platform","title":"a set env script"},{"location":"how-to-guides/docker-developer-guide/#different-env-file","text":"This strategy might be the right one if your settings differs a lot. You will need: Multiple .env files (one per deployment), such as: .env.off : configuration for Open Food Facts dev env. .env.off-pro : configuration for Open Food Facts Producer's Platform dev env. .env.obf : configuration for Open Beauty Facts dev env. .env.opff : configuration for Open Ped Food Facts dev env. COMPOSE_PROJECT_NAME set to different values in each .env file, so that container names across deployments are unique. FRONTEND_PORT and MONGODB_PORT set to different values in each .env file, so that frontend containers don't port-conflict with each other. To switch between configurations, set ENV_FILE before running make commands, (or docker-compose command): ENV_FILE=.env.off-pro make up # starts the OFF Producer's Platform containers. ENV_FILE=.env.obf make up # starts the OBF containers. ENV_FILE=.env.opff make up # starts the OPFF containers. or export it to keep it for a while: export ENV_FILE=.env.off # going to work on OFF for a while make up make restart make down make log A good strategy is to have multiple terminals open, one for each deployment: off [Terminal 1]: export ENV_FILE=.env.off make up off-pro [Terminal 2]: export ENV_FILE=.env.off-pro make up obf [Terminal 3]: export ENV_FILE=.env.obf make up opff [Terminal 3]: export ENV_FILE=.env.opff make up Note: the above case of 4 deployments is a bit ambitious , since ProductOpener's backend container takes about ~6GB of RAM to run, meaning that the above 4 deployments would require a total of 24GB of RAM available.","title":"different .env file"},{"location":"how-to-guides/pro-development/","text":"Developing on the producers platform # Here is how to develop for the producers platform using docker. It suppose you already have setup docker for dev . You should have two kind of shell: - the shell for openfoodfacts - the shell for openfoodfacts-pro, this is a shell where you have source the setenv-pro.sh , that is you run . setenv-pro.sh . Your prompt, should now contains a (pro) to recall you you are in producers environment. (this simply sets some environment variables that will overides the one in .env) To develop, on producers plateform, you can then us a shell for openfoodfacts-pro and simply do a make dev and everything as usual. If you need to work on product import/export, or interacting with public platform, you have to start postgres and the minion on off side. That is, in a non pro shell, run docker-compose up postgres minion mongodb . Note that the setup does not currently support running the http server for both public and pro platform at the same time. So as you need the public platform: - in your pro shell , run a docker-compose stop backend - in your non pro shell , run a docker-compose up backend Now openfoodfacts.localhost is the public database. Of course, do this inside-out to access the pro http server. Note that if you use direnv , it should be fine, if you did not redefine variables set by setenv-pro.sh . An explanation of the setup can be found at pro-dev-setup.md If you want to see state of tasks, you can run: docker-compose exec minion /opt/product-opener/scripts/minion.pl minion job (add --help to see all options), or refer to https://docs.mojolicious.org/Minion/Command/minion/job You may also inspect database by running: docker-compose exec postgres psql -U productopener -W minion (password is given by POSTGRES_PASSWORD in .env and defaults to productopener ) Inspecting table minion, should help.","title":"Developing on the producers platform"},{"location":"how-to-guides/pro-development/#developing-on-the-producers-platform","text":"Here is how to develop for the producers platform using docker. It suppose you already have setup docker for dev . You should have two kind of shell: - the shell for openfoodfacts - the shell for openfoodfacts-pro, this is a shell where you have source the setenv-pro.sh , that is you run . setenv-pro.sh . Your prompt, should now contains a (pro) to recall you you are in producers environment. (this simply sets some environment variables that will overides the one in .env) To develop, on producers plateform, you can then us a shell for openfoodfacts-pro and simply do a make dev and everything as usual. If you need to work on product import/export, or interacting with public platform, you have to start postgres and the minion on off side. That is, in a non pro shell, run docker-compose up postgres minion mongodb . Note that the setup does not currently support running the http server for both public and pro platform at the same time. So as you need the public platform: - in your pro shell , run a docker-compose stop backend - in your non pro shell , run a docker-compose up backend Now openfoodfacts.localhost is the public database. Of course, do this inside-out to access the pro http server. Note that if you use direnv , it should be fine, if you did not redefine variables set by setenv-pro.sh . An explanation of the setup can be found at pro-dev-setup.md If you want to see state of tasks, you can run: docker-compose exec minion /opt/product-opener/scripts/minion.pl minion job (add --help to see all options), or refer to https://docs.mojolicious.org/Minion/Command/minion/job You may also inspect database by running: docker-compose exec postgres psql -U productopener -W minion (password is given by POSTGRES_PASSWORD in .env and defaults to productopener ) Inspecting table minion, should help.","title":"Developing on the producers platform"},{"location":"how-to-guides/use-direnv/","text":"Use direnv # As a developer, it can be better not to think too much about setting right env variables as you enter a project. direnv aims at providing a solution. As a quick guide as an openfoodfacts developer: install direnv on your system using usual package manager in your .bashrc add: # direnv eval \" $( direnv hook bash ) \" you have adapt the direnv line according to what you use, see direnv doc In your project directory add a file, where you superseed variables from .env that you wan't to echo \"setting up docker-compose env\" export DOCKER_BUILDKIT=1 export USER_UID=${UID} export USER_UID=$(id -g) in project directory, run direnv allow . in a new shell: go in project directory you should have direnv trigger and load your variables","title":"Use direnv"},{"location":"how-to-guides/use-direnv/#use-direnv","text":"As a developer, it can be better not to think too much about setting right env variables as you enter a project. direnv aims at providing a solution. As a quick guide as an openfoodfacts developer: install direnv on your system using usual package manager in your .bashrc add: # direnv eval \" $( direnv hook bash ) \" you have adapt the direnv line according to what you use, see direnv doc In your project directory add a file, where you superseed variables from .env that you wan't to echo \"setting up docker-compose env\" export DOCKER_BUILDKIT=1 export USER_UID=${UID} export USER_UID=$(id -g) in project directory, run direnv allow . in a new shell: go in project directory you should have direnv trigger and load your variables","title":"Use direnv"},{"location":"how-to-guides/use-gitpod/","text":"Using Gitpod for Remote Development # Gitpod provides powerful ready-to-code developer environments in the cloud eliminating the friction of setting up local environments and IDEs with Perl, Docker and plugins, making it possible for even new contributors to OpenFoodFacts Server to get started in minutes instead of hours! Note that while this how-to is tailored for Gitpod, using alternatives like GitHub Codespaces should be similar. For the most part, development on Gitpod is similar to developing locally as documented in the quickstart guide and docker-developer-guide , however accessing your dev-deployment of openfoodfacts-server requires an extra step. Get Started # Gitpod will automatically clone and open the repository for you in VSCode by default. It will also automatically build the project for you on opening and comes with Docker and other tools pre-installed making it one of the fastest ways to spin up an environment for openfoodfacts-server . Once the repository is open in Gitpod, other instructions in the quick-start guide can be generally followed. Accessing your development instance of OpenFoodFacts Web # Since Gitpod runs your code in a remote machine, your dev-deployment spun up with make dev or make up will not accessible when you open the default http://openfoodfacts.localhost in your browser. This occurs because the server running on the remote machine is not accessible on your local network interface. To overcome this, we can make use of SSH tunnel that listens to your local port 80 and forwards traffic to the port 80 of the remote machine. Gitpod makes it really simple to SSH into your dev environment by letting you copy the ssh command required to reach your remote environment. To start, follow the ssh instructions on Gitpod's official guide: SSH for workspaces as easy as copy/paste . Once you have copied the ssh command and ensure it works as-is, add a -L 80:localhost:80 to the command to make it look like: ssh -L 80:localhost:80 'openfoodfac-openfoodfac-tok-openfoodfac-r9f61214h9vt.ssh.ws-c.gitpod.io' . Once you execute the altered command in your terminal, you should be able to access OpenFoodFacts on http://openfoodfacts.localhost just as documented in the quickstart guide!","title":"Using Gitpod for Remote Development"},{"location":"how-to-guides/use-gitpod/#using-gitpod-for-remote-development","text":"Gitpod provides powerful ready-to-code developer environments in the cloud eliminating the friction of setting up local environments and IDEs with Perl, Docker and plugins, making it possible for even new contributors to OpenFoodFacts Server to get started in minutes instead of hours! Note that while this how-to is tailored for Gitpod, using alternatives like GitHub Codespaces should be similar. For the most part, development on Gitpod is similar to developing locally as documented in the quickstart guide and docker-developer-guide , however accessing your dev-deployment of openfoodfacts-server requires an extra step.","title":"Using Gitpod for Remote Development"},{"location":"how-to-guides/use-gitpod/#get-started","text":"Gitpod will automatically clone and open the repository for you in VSCode by default. It will also automatically build the project for you on opening and comes with Docker and other tools pre-installed making it one of the fastest ways to spin up an environment for openfoodfacts-server . Once the repository is open in Gitpod, other instructions in the quick-start guide can be generally followed.","title":"Get Started"},{"location":"how-to-guides/use-gitpod/#accessing-your-development-instance-of-openfoodfacts-web","text":"Since Gitpod runs your code in a remote machine, your dev-deployment spun up with make dev or make up will not accessible when you open the default http://openfoodfacts.localhost in your browser. This occurs because the server running on the remote machine is not accessible on your local network interface. To overcome this, we can make use of SSH tunnel that listens to your local port 80 and forwards traffic to the port 80 of the remote machine. Gitpod makes it really simple to SSH into your dev environment by letting you copy the ssh command required to reach your remote environment. To start, follow the ssh instructions on Gitpod's official guide: SSH for workspaces as easy as copy/paste . Once you have copied the ssh command and ensure it works as-is, add a -L 80:localhost:80 to the command to make it look like: ssh -L 80:localhost:80 'openfoodfac-openfoodfac-tok-openfoodfac-r9f61214h9vt.ssh.ws-c.gitpod.io' . Once you execute the altered command in your terminal, you should be able to access OpenFoodFacts on http://openfoodfacts.localhost just as documented in the quickstart guide!","title":"Accessing your development instance of OpenFoodFacts Web"},{"location":"how-to-guides/use-repl/","text":"Using Repl # On your local dev instance, the \"backend\" container comes with Devel::REPL installed. Thanks to PERL5LIB variable which is already configured, you can load any module of ProductOpener from within it. Also it as the right Launch Repl # Just run docker-compose run --rm docker-compose re.pl If you want to access external services (like mongodb), do not forget to start them. Testing perl code # It can be a handy way to get your hand into perl by testing some code patterns, or seeing how they react. For example one can test a regular expression: $ my $text = \"Hello World\" ; Hello World $ $ text =~ /Hello (\\w+)/i World Reading a sto # Another use case is reading a sto file to see what it contains. Eg. for a user: $ use ProductOpener:: Store qw/:all/ ; $ my $user_id = \"xxxx\" ; $ my $user_ref = retrieve ( \"/mnt/podata/users/$user_id.sto\" );","title":"Using Repl"},{"location":"how-to-guides/use-repl/#using-repl","text":"On your local dev instance, the \"backend\" container comes with Devel::REPL installed. Thanks to PERL5LIB variable which is already configured, you can load any module of ProductOpener from within it. Also it as the right","title":"Using Repl"},{"location":"how-to-guides/use-repl/#launch-repl","text":"Just run docker-compose run --rm docker-compose re.pl If you want to access external services (like mongodb), do not forget to start them.","title":"Launch Repl"},{"location":"how-to-guides/use-repl/#testing-perl-code","text":"It can be a handy way to get your hand into perl by testing some code patterns, or seeing how they react. For example one can test a regular expression: $ my $text = \"Hello World\" ; Hello World $ $ text =~ /Hello (\\w+)/i World","title":"Testing perl code"},{"location":"how-to-guides/use-repl/#reading-a-sto","text":"Another use case is reading a sto file to see what it contains. Eg. for a user: $ use ProductOpener:: Store qw/:all/ ; $ my $user_id = \"xxxx\" ; $ my $user_ref = retrieve ( \"/mnt/podata/users/$user_id.sto\" );","title":"Reading a sto"},{"location":"how-to-guides/use-vscode/","text":"Using VSCode # VSCode (or better the open source version VSCodium ) may be used to edit files. Here are some useful tricks. Perlcritic # One way to have perlcritic work is the following: install the perlcritic extension add a perlcritic.sh at the root of your project with following content: #!/usr/bin/env bash . .envrc >/dev/null 2 > & 1 docker-compose run --rm --no-deps backend perlcritic \" $@ \" 2 >/dev/null the second line is useful only if you use direnv chmod +x perlcritic.sh patch perlcritic by editing its files, following sfodje/perlcritic issue #26 the edit perlcritic configuration in workspace to set those values: Executable: /home/alex/docker/off-server/perlcritic.sh Perl Language Server # The extension Language Server and Debugger is less easy to work with ! Note: This setup does not work yet, but might not be so far. It is probably due to https://github.com/richterger/Perl-LanguageServer/issues/131 install the extension add a script shell-into-appserver.sh in the project: #!/usr/bin/env bash declare -x PATH = $PATH :/usr/local/bin/ source .envrc COMMAND = $( echo \" $@ \" | sed 's/^.*perl /perl /' ) > & 2 echo \"launching $COMMAND \" docker-compose run --rm --no-deps -T -p 127 .0.0. 1 :13603:13603 backend $COMMAND Note: the second line is useful only if you use direnv chmod +x shell-into-appserver.sh Edit workspace settings to have those settings: \"perl\" : { \"enable\" : true , \"perlInc\" : [ \"/opt/product-opener/lib\" , \"/opt/perl/local/lib/perl5\" ], \"ignoreDirs\" : [ \"/opt/perl/local/lib/perl5\" , \". vscode\" ], \"fileFilter\" : [ \".pm\" , \".pl\" , \".t\" ], \"sshAddr\" : \"dummy\" , \"sshUser\" : \"dummy\" , \"sshCmd\" : \"./shell-into-appserver.sh\" , \"sshWorkspaceRoot\" : \"/opt/product-opener\" , \"logLevel\" : 2 }, Remote container ? # Note: at the moment we do not support the Remote Container extension. While we can consider using it, it has some drawback because not all the project is contained within the \"backend\" container. For example all that concern nodejs is in the \"frontend\" container. So it means making a quite complete Docker image on its own with all the tooling necessary.","title":"Using VSCode"},{"location":"how-to-guides/use-vscode/#using-vscode","text":"VSCode (or better the open source version VSCodium ) may be used to edit files. Here are some useful tricks.","title":"Using VSCode"},{"location":"how-to-guides/use-vscode/#perlcritic","text":"One way to have perlcritic work is the following: install the perlcritic extension add a perlcritic.sh at the root of your project with following content: #!/usr/bin/env bash . .envrc >/dev/null 2 > & 1 docker-compose run --rm --no-deps backend perlcritic \" $@ \" 2 >/dev/null the second line is useful only if you use direnv chmod +x perlcritic.sh patch perlcritic by editing its files, following sfodje/perlcritic issue #26 the edit perlcritic configuration in workspace to set those values: Executable: /home/alex/docker/off-server/perlcritic.sh","title":"Perlcritic"},{"location":"how-to-guides/use-vscode/#perl-language-server","text":"The extension Language Server and Debugger is less easy to work with ! Note: This setup does not work yet, but might not be so far. It is probably due to https://github.com/richterger/Perl-LanguageServer/issues/131 install the extension add a script shell-into-appserver.sh in the project: #!/usr/bin/env bash declare -x PATH = $PATH :/usr/local/bin/ source .envrc COMMAND = $( echo \" $@ \" | sed 's/^.*perl /perl /' ) > & 2 echo \"launching $COMMAND \" docker-compose run --rm --no-deps -T -p 127 .0.0. 1 :13603:13603 backend $COMMAND Note: the second line is useful only if you use direnv chmod +x shell-into-appserver.sh Edit workspace settings to have those settings: \"perl\" : { \"enable\" : true , \"perlInc\" : [ \"/opt/product-opener/lib\" , \"/opt/perl/local/lib/perl5\" ], \"ignoreDirs\" : [ \"/opt/perl/local/lib/perl5\" , \". vscode\" ], \"fileFilter\" : [ \".pm\" , \".pl\" , \".t\" ], \"sshAddr\" : \"dummy\" , \"sshUser\" : \"dummy\" , \"sshCmd\" : \"./shell-into-appserver.sh\" , \"sshWorkspaceRoot\" : \"/opt/product-opener\" , \"logLevel\" : 2 },","title":"Perl Language Server"},{"location":"how-to-guides/use-vscode/#remote-container","text":"Note: at the moment we do not support the Remote Container extension. While we can consider using it, it has some drawback because not all the project is contained within the \"backend\" container. For example all that concern nodejs is in the \"frontend\" container. So it means making a quite complete Docker image on its own with all the tooling necessary.","title":"Remote container ?"},{"location":"how-to-guides/using-pages-from-openfoodfacts-web/","text":"Using pages from openfoodfacts-web # To avoid messing product-opener repository with translations of web-pages, we moved most pages in openfoodfacts-web repository specificly in the lang/ directory. This repo only has a really minimal lang directory named lang-default. If you want to have all contents locally, you should first clone openfoodfacts-web repo locally, and then: if you are using docker, you can set the WEB_LANG_PATH env variable to a relative or absolute path leading to openfoodfacts-web lang directory. else, make symlink lang point to openfoodfacts-web lang directory.","title":"Using pages from openfoodfacts-web"},{"location":"how-to-guides/using-pages-from-openfoodfacts-web/#using-pages-from-openfoodfacts-web","text":"To avoid messing product-opener repository with translations of web-pages, we moved most pages in openfoodfacts-web repository specificly in the lang/ directory. This repo only has a really minimal lang directory named lang-default. If you want to have all contents locally, you should first clone openfoodfacts-web repo locally, and then: if you are using docker, you can set the WEB_LANG_PATH env variable to a relative or absolute path leading to openfoodfacts-web lang directory. else, make symlink lang point to openfoodfacts-web lang directory.","title":"Using pages from openfoodfacts-web"},{"location":"introduction/api/","text":"Open Food Facts API Documentation # Everything you need to know about Open Food Facts API. Overview # Open Food Facts is a food products database made by everyone, for everyone, that can help you make better food choices. Seeing it is open data, anyone can reuse it for any purpose. For example, you are building a nutrition app. The Open Food Facts API enables developers to add to the products database and retrieve information about existing products. You may use the API to build applications allowing users to contribute to the database and make healthier food choices. The current version of the API is 2 . Data in the Open Food Facts database is provided voluntarily by users who want to support the program. As a result, there are no assurances that the data is accurate, complete, or reliable. The user assumes the entire risk of using the data. Before You Start # The Open Food Facts database is available under the Open Database License . The individual contents of the database are available under the Database Contents License . Product images are available under the Creative Commons Attribution ShareAlike license. They may contain graphical elements subject to copyright or other rights that may, in some cases, be reproduced (quotation rights or fair use). Please read the Terms and conditions of use and reuse before reusing the data. We are interested in learning what the Open Food Facts data is used for. It is not mandatory, but we would very much appreciate it if you tell us about your reuses so that we can share them with the Open Food Facts community. How to Best Use the API # General principles # You can search for product information, including many useful computed values. If you can't get the information on a specific product, you can get your user to send photos and data that will then be processed by Open Food Facts AI and contributors to get the computed result you want to show them. You can also implement the complete flow so that they immediately get the result with some effort on their side. If your users do not expect a result immediately (e.g., Inventory apps) # Submit photos (front/nutrition/ingredients): the most painless thing for your users The Open Food Facts AI Robotoff will generate some derived data from the photos. Over time, other apps and the Open Food Facts community will fill the data gaps. If your users expect a result immediately (e.g., Nutrition apps) # Submit nutrition facts + category > get Nutri-Score Submit ingredients > get the NOVA group (about food ultra-processing), additives, allergens, normalized ingredients, vegan, vegetarian\u2026 Submit category + labels > soon get the Eco-Score (about environmental impact) Environment # The OpenFoodFacts API has two environments. Production: https://world.openfoodfacts.org Staging: https://world.openfoodfacts.net Consider using the staging environment if you are not in a production scenario. While testing your applications, make all API requests to the staging environment. This way, we can ensure the product database is safe. Warning : The staging environment has an extra level of authentication (username: off, password: off). When making API requests to staging, you may use https://off:off@world.openfoodfacts.net/ as the base URL to include the authentication. Authentication # All requests do not require authentication except for WRITE operations (Editing an Existing Product, Uploading images\u2026). Create an account on the Open Food Facts app . You then have to alternative: The preferred one: use the login API to get a session cookie and use this cookie in your subsequent request to be authenticated. Note however that the session must always be used from the same IP address, and that you have a maximum of session per user. If session conditions are too restrictive for your use case, include your account credentials as parameters for authenticated requests where user_id is your username and password is your password (do this on POST / PUT / DELETE request, not on GET) To allow users of your app to contribute without registering individual accounts on the Open Food Facts website, you can create a global account. This way, we know that these contributions came from your application. The account you create in the production environment will only work for requests in production. You need to create an account in the staging environment if you want to make authenticated requests in staging. Reference Documentation (OpenAPI) # Tutorials # Help # Try the FAQ - to answer most of your questions. Didn't get a satisfactory answer? Contact the Team on the #api Slack Channel. Report Bugs on the Open Food Facts Database. Do you have an issue or feature request? You can submit it here on GitHub . Are you interested in contributing to this project? See our Contribution Guidelines . SDKS # SDKs are available for specific languages to facilitate the usage of the API. We probably have a wrapper for your favorite programming language. If we do, you can use it and improve it. If we don't, you can help create it. They will let you consume data and let your users contribute new data. Open-source contributors develop our SDKs, and more contributions are welcome to improve these SDKs. You can start by checking the existing issues in their respective repositories. Warning : Before exploring any SDK, endeavor to read the Before You Start section . Also remember, in case of problem, to check the API Reference Documentation first to verify if the problem is in SDK implementation or in the API itself. Cordova DART , Published on pub.dev Elixir Go NodeJS PHP Laravel Python , Published on pyipi React Native Ruby Java RUST R","title":"Open Food Facts API Documentation"},{"location":"introduction/api/#open-food-facts-api-documentation","text":"Everything you need to know about Open Food Facts API.","title":"Open Food Facts API Documentation"},{"location":"introduction/api/#overview","text":"Open Food Facts is a food products database made by everyone, for everyone, that can help you make better food choices. Seeing it is open data, anyone can reuse it for any purpose. For example, you are building a nutrition app. The Open Food Facts API enables developers to add to the products database and retrieve information about existing products. You may use the API to build applications allowing users to contribute to the database and make healthier food choices. The current version of the API is 2 . Data in the Open Food Facts database is provided voluntarily by users who want to support the program. As a result, there are no assurances that the data is accurate, complete, or reliable. The user assumes the entire risk of using the data.","title":"Overview"},{"location":"introduction/api/#before-you-start","text":"The Open Food Facts database is available under the Open Database License . The individual contents of the database are available under the Database Contents License . Product images are available under the Creative Commons Attribution ShareAlike license. They may contain graphical elements subject to copyright or other rights that may, in some cases, be reproduced (quotation rights or fair use). Please read the Terms and conditions of use and reuse before reusing the data. We are interested in learning what the Open Food Facts data is used for. It is not mandatory, but we would very much appreciate it if you tell us about your reuses so that we can share them with the Open Food Facts community.","title":"Before You Start"},{"location":"introduction/api/#how-to-best-use-the-api","text":"","title":"How to Best Use the API"},{"location":"introduction/api/#general-principles","text":"You can search for product information, including many useful computed values. If you can't get the information on a specific product, you can get your user to send photos and data that will then be processed by Open Food Facts AI and contributors to get the computed result you want to show them. You can also implement the complete flow so that they immediately get the result with some effort on their side.","title":"General principles"},{"location":"introduction/api/#if-your-users-do-not-expect-a-result-immediately-eg-inventory-apps","text":"Submit photos (front/nutrition/ingredients): the most painless thing for your users The Open Food Facts AI Robotoff will generate some derived data from the photos. Over time, other apps and the Open Food Facts community will fill the data gaps.","title":"If your users do not expect a result immediately (e.g., Inventory apps)"},{"location":"introduction/api/#if-your-users-expect-a-result-immediately-eg-nutrition-apps","text":"Submit nutrition facts + category > get Nutri-Score Submit ingredients > get the NOVA group (about food ultra-processing), additives, allergens, normalized ingredients, vegan, vegetarian\u2026 Submit category + labels > soon get the Eco-Score (about environmental impact)","title":"If your users expect a result immediately (e.g., Nutrition apps)"},{"location":"introduction/api/#environment","text":"The OpenFoodFacts API has two environments. Production: https://world.openfoodfacts.org Staging: https://world.openfoodfacts.net Consider using the staging environment if you are not in a production scenario. While testing your applications, make all API requests to the staging environment. This way, we can ensure the product database is safe. Warning : The staging environment has an extra level of authentication (username: off, password: off). When making API requests to staging, you may use https://off:off@world.openfoodfacts.net/ as the base URL to include the authentication.","title":"Environment"},{"location":"introduction/api/#authentication","text":"All requests do not require authentication except for WRITE operations (Editing an Existing Product, Uploading images\u2026). Create an account on the Open Food Facts app . You then have to alternative: The preferred one: use the login API to get a session cookie and use this cookie in your subsequent request to be authenticated. Note however that the session must always be used from the same IP address, and that you have a maximum of session per user. If session conditions are too restrictive for your use case, include your account credentials as parameters for authenticated requests where user_id is your username and password is your password (do this on POST / PUT / DELETE request, not on GET) To allow users of your app to contribute without registering individual accounts on the Open Food Facts website, you can create a global account. This way, we know that these contributions came from your application. The account you create in the production environment will only work for requests in production. You need to create an account in the staging environment if you want to make authenticated requests in staging.","title":"Authentication"},{"location":"introduction/api/#reference-documentation-openapi","text":"","title":"Reference Documentation (OpenAPI)"},{"location":"introduction/api/#tutorials","text":"","title":"Tutorials"},{"location":"introduction/api/#help","text":"Try the FAQ - to answer most of your questions. Didn't get a satisfactory answer? Contact the Team on the #api Slack Channel. Report Bugs on the Open Food Facts Database. Do you have an issue or feature request? You can submit it here on GitHub . Are you interested in contributing to this project? See our Contribution Guidelines .","title":"Help"},{"location":"introduction/api/#sdks","text":"SDKs are available for specific languages to facilitate the usage of the API. We probably have a wrapper for your favorite programming language. If we do, you can use it and improve it. If we don't, you can help create it. They will let you consume data and let your users contribute new data. Open-source contributors develop our SDKs, and more contributions are welcome to improve these SDKs. You can start by checking the existing issues in their respective repositories. Warning : Before exploring any SDK, endeavor to read the Before You Start section . Also remember, in case of problem, to check the API Reference Documentation first to verify if the problem is in SDK implementation or in the API itself. Cordova DART , Published on pub.dev Elixir Go NodeJS PHP Laravel Python , Published on pyipi React Native Ruby Java RUST R","title":"SDKS"},{"location":"introduction/dev-environment-quick-start-guide/","text":"Dev environment quick start guide # This guide will allow you to rapidly build a ready-to-use development environment for Product Opener running in Docker. As an alternative to setting up your environment locally, follow the Gitpod how-to guide to instantly provision a ready-to-code development environment in the cloud. First setup time estimate is ~10min with the following specs: * 8 GB of RAM dedicated to Docker client * 6 cores dedicated to Docker client * 12 MB/s internet speed 1. Prerequisites # Docker is the easiest way to install the Open Food Facts server, play with it, and even modify the code. Docker provides an isolated environment, very close to a Virtual Machine. This environment contains everything required to launch the Open Food Facts server. There is no need to install Perl, Perl modules, Nginx, nor Apache separately. Installation steps: - Install Docker CE If you run e.g. Debian, don't forget to add your user to the docker group! - Install Docker Compose - Enable command-line completion - Install Make for Windows (if running on Windows) 2. Clone the repository from GitHub # You must have a GitHub account if you want to contribute to Open Food Facts development, but it\u2019s not required if you just want to see how it works. Be aware Open Food Facts server takes more than 1.3 GB (2019/11). Choose your prefered way to clone, either: On Windows: # If you are running Docker on Windows, please use the following git clone command: git clone -c core.symlinks=true https://github.com/openfoodfacts/openfoodfacts-server.git or git clone -c core.symlinks=true git@github.com:openfoodfacts/openfoodfacts-server.git On other systems: # git clone git@github.com:openfoodfacts/openfoodfacts-server.git or git clone https://github.com/openfoodfacts/openfoodfacts-server.git Go to the cloned directory: cd openfoodfacts-server/ 3. [Optional] Review Product Opener's environment # Note: you can skip this step for the first setup since the default .env in the repo contains all the default values required to get started. Before running the docker-compose deployment, you can review and configure Product Opener's environment ( .env file). The .env file contains ProductOpener default settings: | Field | Description | | ----------------------------------------------------------------- | --- | | PRODUCT_OPENER_DOMAIN | Can be set to different values based on which OFF flavor is run.| | PRODUCT_OPENER_PORT | can be modified to run NGINX on a different port. Useful when running multiple OFF flavors on different ports on the same host. Default port: 80 .| | PRODUCT_OPENER_FLAVOR | Can be modified to run different flavors of OpenFoodFacts, amongst openfoodfacts (default), openbeautyfacts , openpetfoodfacts , openproductsfacts .| | PRODUCT_OPENER_FLAVOR_SHORT | can be modified to run different flavors of OpenFoodFacts, amongst off (default), obf , oppf , opf .| | PRODUCERS_PLATFORM | can be set to 1 to build / run the producer platform .| | ROBOTOFF_URL | can be set to connect with a Robotoff instance .| | REDIS_URL | can be set to connect with a Redis instance for populating the search index .| | GOOGLE_CLOUD_VISION_API_KEY | can be set to enable OCR using Google Cloud Vision .| | CROWDIN_PROJECT_IDENTIFIER and CROWDIN_PROJECT_KEY | can be set to run translations .| | GEOLITE2_PATH , GEOLITE2_ACCOUNT_ID and GEOLITE2_LICENSE_KEY | can be set to enable Geolite2 .| | TAG | Is set to latest by default, but you can specify any Docker Hub tag for the frontend / backend images. Note that this is useful only if you use pre-built images from the Docker Hub ( docker/prod.yml override); the default dev setup ( docker/dev.yml ) builds images locally| The .env file also contains some useful Docker Compose variables: * COMPOSE_PROJECT_NAME is the compose project name that sets the prefix to every container name . Do not update this unless you know what you're doing. * COMPOSE_FILE is the ; -separated list of Docker compose files that are included in the deployment: * For a development -like environment, set it to docker-compose.yml;docker/dev.yml (default) * For a production -like environment, set it to docker-compose.yml;docker/prod.yml;docker/mongodb.yml * For more features, you can add: * docker/admin-uis.yml : add the Admin UIS container * docker/geolite2.yml : add the Geolite2 container * docker/perldb.yml : add the Perl debugger container * COMPOSE_SEPARATOR is the separator used for COMPOSE_FILE . Note: Instead of modifying .env (with the risk commit it inadvertently), You can also set needed variables in your shell, they will override .env values. Consider creating a .envrc file that you source each time you need to work on the project. On linux and macOS, you can automatically do it if you use direnv . 4. Build your dev environment # From the repository root, run: make dev Note: If you are using Windows, you may encounter issues regarding this command. Take a look at the Troubleshooting section further in this tutorial. Note: If docker complains about ERROR: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network It can be solved by adding {\"base\": \"172.80.0.0/16\",\"size\": 24}, {\"base\": \"172.90.0.0/16\",\"size\": 24} to default-address-pools in /etc/docker/daemon.json and then restarting the docker daemon. Credits to https://theorangeone.net/posts/increase-docker-ip-space/ for this solution. The command will run 2 subcommands: * make up : Build and run containers from the local directory and bind local code files, so that you do not have to rebuild everytime. * make import_sample_data : Load sample data into mongodb container (~100 products). Notes: The first build can take between 10 and 30 minutes depending on your machine and internet connection (broadband connection heavily recommended, as this will download Docker base images, install Debian and Perl modules in preparation of the final container image). You might not immediately see the test products: create an account, login, and they should appear. For a full description of available make targets, see docker/README.md Hosts file: Since the default PRODUCT_OPENER_DOMAIN in the .env file is set to openfoodfacts.localhost , add the following to your hosts file (Windows: C:\\Windows\\System32\\drivers\\etc\\hosts ; Linux/MacOSX: /etc/hosts ): 127.0.0.1 world.openfoodfacts.localhost fr.openfoodfacts.localhost static.openfoodfacts.localhost ssl-api.openfoodfacts.localhost fr-en.openfoodfacts.localhost You're done ! Check http://openfoodfacts.localhost/ ! Going further # To learn more about developing with Docker, see the Docker developer's guide . To have all site page on your dev instance, see Using pages from openfoodfacts-web Using Repl offers you a way to play with perl. Visual Studio Code # WARNING : for now this is deprecated, some work needs to be done. This repository comes with a configuration for Visual Studio Code (VS Code) development containers (devcontainer) . This enables some Perl support in VS Code without the need to install the correct Perl version and modules on your local machine. To use the devcontainer, install prerequisites , clone the repository from GitHub , and (optionally) review Product Opener's environment . Additionally, install Visual Studio Code . VS Code will automatically recommend some extensions, but if you don't want to install all of them, please do install Remote - Containers manually. You can then use the extension command Remote-Containers: Reopen Folder in Container , which will automatically build the container and start the services. No need to use make ! Troubleshooting # make dev error: make: command not found # When running \"make dev\": bash: make: command not found Solution: Click the Windows button, then type \u201cenvironment properties\u201d into the search bar and hit Enter. Click Environment Variables, then under System variables choose Path and click Edit. Click New and insert C:\\Program Files (x86)\\GnuWin32\\bin, then save the changes. Open a new terminal and test that the command works. (see Make Windows for more) make dev error: [build_lang] Error 2 - Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto # When running \"make dev\": <h1>Software error:</h1> <pre>Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto at /opt/product-opener/lib/ProductOpener/Tags.pm line 1976. Compilation failed in require at /opt/product-opener/scripts/build_lang.pl line 31, &lt;DATA&gt; line 2104. BEGIN failed--compilation aborted at /opt/product-opener/scripts/build_lang.pl line 31, &lt;DATA&gt; line 2104. </pre> <p> For help, please send mail to this site's webmaster, giving this error message and the time and date of the error. </p> [Tue Apr 5 19:36:40 2022] build_lang.pl: Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto at /opt/product-opener/lib/ProductOpener/Tags.pm line 1976. [Tue Apr 5 19:36:40 2022] build_lang.pl: Compilation failed in require at /opt/product-opener/scripts/build_lang.pl line 31, <DATA> line 2104. [Tue Apr 5 19:36:40 2022] build_lang.pl: BEGIN failed--compilation aborted at /opt/product-opener/scripts/build_lang.pl line 31, <DATA> line 2104. make: *** [build_lang] Error 2 Solution: Project needs Symlinks to be enabled. traces.result.sto is a symlink to allergens.result.sto On Windows, You have to enable the 'Developer Mode' in order to use the symlinks. To enable Developer Mode: Go under Settings > Update & Security > 'For developers', and turn on the toggle for Developer Mode. On Windows systems, the git repository needs to be cloned with symlinks enabled. You need to remove current directory where you clone the project, and clone the project again, using right options: ```console git clone -c core.symlinks=true git@github.com:openfoodfacts/openfoodfacts-server.git","title":"Dev environment quick start guide"},{"location":"introduction/dev-environment-quick-start-guide/#dev-environment-quick-start-guide","text":"This guide will allow you to rapidly build a ready-to-use development environment for Product Opener running in Docker. As an alternative to setting up your environment locally, follow the Gitpod how-to guide to instantly provision a ready-to-code development environment in the cloud. First setup time estimate is ~10min with the following specs: * 8 GB of RAM dedicated to Docker client * 6 cores dedicated to Docker client * 12 MB/s internet speed","title":"Dev environment quick start guide"},{"location":"introduction/dev-environment-quick-start-guide/#1-prerequisites","text":"Docker is the easiest way to install the Open Food Facts server, play with it, and even modify the code. Docker provides an isolated environment, very close to a Virtual Machine. This environment contains everything required to launch the Open Food Facts server. There is no need to install Perl, Perl modules, Nginx, nor Apache separately. Installation steps: - Install Docker CE If you run e.g. Debian, don't forget to add your user to the docker group! - Install Docker Compose - Enable command-line completion - Install Make for Windows (if running on Windows)","title":"1. Prerequisites"},{"location":"introduction/dev-environment-quick-start-guide/#2-clone-the-repository-from-github","text":"You must have a GitHub account if you want to contribute to Open Food Facts development, but it\u2019s not required if you just want to see how it works. Be aware Open Food Facts server takes more than 1.3 GB (2019/11). Choose your prefered way to clone, either:","title":"2. Clone the repository from GitHub"},{"location":"introduction/dev-environment-quick-start-guide/#on-windows","text":"If you are running Docker on Windows, please use the following git clone command: git clone -c core.symlinks=true https://github.com/openfoodfacts/openfoodfacts-server.git or git clone -c core.symlinks=true git@github.com:openfoodfacts/openfoodfacts-server.git","title":"On Windows:"},{"location":"introduction/dev-environment-quick-start-guide/#on-other-systems","text":"git clone git@github.com:openfoodfacts/openfoodfacts-server.git or git clone https://github.com/openfoodfacts/openfoodfacts-server.git Go to the cloned directory: cd openfoodfacts-server/","title":"On other systems:"},{"location":"introduction/dev-environment-quick-start-guide/#3-optional-review-product-openers-environment","text":"Note: you can skip this step for the first setup since the default .env in the repo contains all the default values required to get started. Before running the docker-compose deployment, you can review and configure Product Opener's environment ( .env file). The .env file contains ProductOpener default settings: | Field | Description | | ----------------------------------------------------------------- | --- | | PRODUCT_OPENER_DOMAIN | Can be set to different values based on which OFF flavor is run.| | PRODUCT_OPENER_PORT | can be modified to run NGINX on a different port. Useful when running multiple OFF flavors on different ports on the same host. Default port: 80 .| | PRODUCT_OPENER_FLAVOR | Can be modified to run different flavors of OpenFoodFacts, amongst openfoodfacts (default), openbeautyfacts , openpetfoodfacts , openproductsfacts .| | PRODUCT_OPENER_FLAVOR_SHORT | can be modified to run different flavors of OpenFoodFacts, amongst off (default), obf , oppf , opf .| | PRODUCERS_PLATFORM | can be set to 1 to build / run the producer platform .| | ROBOTOFF_URL | can be set to connect with a Robotoff instance .| | REDIS_URL | can be set to connect with a Redis instance for populating the search index .| | GOOGLE_CLOUD_VISION_API_KEY | can be set to enable OCR using Google Cloud Vision .| | CROWDIN_PROJECT_IDENTIFIER and CROWDIN_PROJECT_KEY | can be set to run translations .| | GEOLITE2_PATH , GEOLITE2_ACCOUNT_ID and GEOLITE2_LICENSE_KEY | can be set to enable Geolite2 .| | TAG | Is set to latest by default, but you can specify any Docker Hub tag for the frontend / backend images. Note that this is useful only if you use pre-built images from the Docker Hub ( docker/prod.yml override); the default dev setup ( docker/dev.yml ) builds images locally| The .env file also contains some useful Docker Compose variables: * COMPOSE_PROJECT_NAME is the compose project name that sets the prefix to every container name . Do not update this unless you know what you're doing. * COMPOSE_FILE is the ; -separated list of Docker compose files that are included in the deployment: * For a development -like environment, set it to docker-compose.yml;docker/dev.yml (default) * For a production -like environment, set it to docker-compose.yml;docker/prod.yml;docker/mongodb.yml * For more features, you can add: * docker/admin-uis.yml : add the Admin UIS container * docker/geolite2.yml : add the Geolite2 container * docker/perldb.yml : add the Perl debugger container * COMPOSE_SEPARATOR is the separator used for COMPOSE_FILE . Note: Instead of modifying .env (with the risk commit it inadvertently), You can also set needed variables in your shell, they will override .env values. Consider creating a .envrc file that you source each time you need to work on the project. On linux and macOS, you can automatically do it if you use direnv .","title":"3. [Optional] Review Product Opener's environment"},{"location":"introduction/dev-environment-quick-start-guide/#4-build-your-dev-environment","text":"From the repository root, run: make dev Note: If you are using Windows, you may encounter issues regarding this command. Take a look at the Troubleshooting section further in this tutorial. Note: If docker complains about ERROR: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network It can be solved by adding {\"base\": \"172.80.0.0/16\",\"size\": 24}, {\"base\": \"172.90.0.0/16\",\"size\": 24} to default-address-pools in /etc/docker/daemon.json and then restarting the docker daemon. Credits to https://theorangeone.net/posts/increase-docker-ip-space/ for this solution. The command will run 2 subcommands: * make up : Build and run containers from the local directory and bind local code files, so that you do not have to rebuild everytime. * make import_sample_data : Load sample data into mongodb container (~100 products). Notes: The first build can take between 10 and 30 minutes depending on your machine and internet connection (broadband connection heavily recommended, as this will download Docker base images, install Debian and Perl modules in preparation of the final container image). You might not immediately see the test products: create an account, login, and they should appear. For a full description of available make targets, see docker/README.md Hosts file: Since the default PRODUCT_OPENER_DOMAIN in the .env file is set to openfoodfacts.localhost , add the following to your hosts file (Windows: C:\\Windows\\System32\\drivers\\etc\\hosts ; Linux/MacOSX: /etc/hosts ): 127.0.0.1 world.openfoodfacts.localhost fr.openfoodfacts.localhost static.openfoodfacts.localhost ssl-api.openfoodfacts.localhost fr-en.openfoodfacts.localhost You're done ! Check http://openfoodfacts.localhost/ !","title":"4. Build your dev environment"},{"location":"introduction/dev-environment-quick-start-guide/#going-further","text":"To learn more about developing with Docker, see the Docker developer's guide . To have all site page on your dev instance, see Using pages from openfoodfacts-web Using Repl offers you a way to play with perl.","title":"Going further"},{"location":"introduction/dev-environment-quick-start-guide/#visual-studio-code","text":"WARNING : for now this is deprecated, some work needs to be done. This repository comes with a configuration for Visual Studio Code (VS Code) development containers (devcontainer) . This enables some Perl support in VS Code without the need to install the correct Perl version and modules on your local machine. To use the devcontainer, install prerequisites , clone the repository from GitHub , and (optionally) review Product Opener's environment . Additionally, install Visual Studio Code . VS Code will automatically recommend some extensions, but if you don't want to install all of them, please do install Remote - Containers manually. You can then use the extension command Remote-Containers: Reopen Folder in Container , which will automatically build the container and start the services. No need to use make !","title":"Visual Studio Code"},{"location":"introduction/dev-environment-quick-start-guide/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"introduction/dev-environment-quick-start-guide/#make-dev-error-make-command-not-found","text":"When running \"make dev\": bash: make: command not found Solution: Click the Windows button, then type \u201cenvironment properties\u201d into the search bar and hit Enter. Click Environment Variables, then under System variables choose Path and click Edit. Click New and insert C:\\Program Files (x86)\\GnuWin32\\bin, then save the changes. Open a new terminal and test that the command works. (see Make Windows for more)","title":"make dev error: make: command not found"},{"location":"introduction/dev-environment-quick-start-guide/#make-dev-error-build_lang-error-2-could-not-load-taxonomy-mntpodatataxonomiestracesresultsto","text":"When running \"make dev\": <h1>Software error:</h1> <pre>Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto at /opt/product-opener/lib/ProductOpener/Tags.pm line 1976. Compilation failed in require at /opt/product-opener/scripts/build_lang.pl line 31, &lt;DATA&gt; line 2104. BEGIN failed--compilation aborted at /opt/product-opener/scripts/build_lang.pl line 31, &lt;DATA&gt; line 2104. </pre> <p> For help, please send mail to this site's webmaster, giving this error message and the time and date of the error. </p> [Tue Apr 5 19:36:40 2022] build_lang.pl: Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto at /opt/product-opener/lib/ProductOpener/Tags.pm line 1976. [Tue Apr 5 19:36:40 2022] build_lang.pl: Compilation failed in require at /opt/product-opener/scripts/build_lang.pl line 31, <DATA> line 2104. [Tue Apr 5 19:36:40 2022] build_lang.pl: BEGIN failed--compilation aborted at /opt/product-opener/scripts/build_lang.pl line 31, <DATA> line 2104. make: *** [build_lang] Error 2 Solution: Project needs Symlinks to be enabled. traces.result.sto is a symlink to allergens.result.sto On Windows, You have to enable the 'Developer Mode' in order to use the symlinks. To enable Developer Mode: Go under Settings > Update & Security > 'For developers', and turn on the toggle for Developer Mode. On Windows systems, the git repository needs to be cloned with symlinks enabled. You need to remove current directory where you clone the project, and clone the project again, using right options: ```console git clone -c core.symlinks=true git@github.com:openfoodfacts/openfoodfacts-server.git","title":"make dev error: [build_lang] Error 2 - Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto"},{"location":"introduction/knowledge-panels/","text":"Knowledge panels # The Open Food Facts API allows clients (such as the Open Food Facts website and mobile app) to request ready to display bits of information about an object (such as a product or a facet like a category). Clients do not have to know in advance what kind of information is displayed (e.g. the ingredients of a product, nutrition data, the Nutri-Score or the Eco-Score), they only have to know how to display basic types of data such as texts, grades, images, and tables. The structure of the knowledge panels data returned by the API is described in the panels JSON schema.","title":"Knowledge panels"},{"location":"introduction/knowledge-panels/#knowledge-panels","text":"The Open Food Facts API allows clients (such as the Open Food Facts website and mobile app) to request ready to display bits of information about an object (such as a product or a facet like a category). Clients do not have to know in advance what kind of information is displayed (e.g. the ingredients of a product, nutrition data, the Nutri-Score or the Eco-Score), they only have to know how to display basic types of data such as texts, grades, images, and tables. The structure of the knowledge panels data returned by the API is described in the panels JSON schema.","title":"Knowledge panels"},{"location":"reference/API_TODO/","text":"Open Api Documentation Todo List # Include these missing fields into the product response # url , created_datetime , last_modified_datetime , categories_fr , labels_fr , first_packaging_code_geo , cities , countries_fr , no_nutriments , additives , ingredients_from_palm_oil , main_category , main_category_fr , image_small_url , casein_100g , serum-proteins_100g , nucleotides_100g , sucrose_100g , glucose_100g , fructose_100g , lactose_100g , maltose_100g , maltodextrins_100g , starch_100g , polyols_100g , fat_100g , butyric-acid_100g , caproic-acid_100g , caprylic-acid_100g , capric-acid_100g , lauric-acid_100g , myristic-acid_100g , palmitic-acid_100g , stearic-acid_100g , arachidic-acid_100g , behenic-acid_100g , lignoceric-acid_100g , cerotic-acid_100g , montanic-acid_100g , melissic-acid_100g , melissic-acid_100g , monounsaturated-fat_100g , polyunsaturated-fat_100g , omega-3-fat_100g , alpha-linolenic-acid_100g , eicosapentaenoic-acid_100g , docosahexaenoic-acid_100g , omega-6-fat_100g , linoleic-acid_100g , arachidonic-acid_100g , gamma-linolenic-acid_100g , dihomo-gamma-linolenic-acid_100g , omega-9-fat_100g , oleic-acid_100g , elaidic-acid_100g , gondoic-acid_100g , mead-acid_100g , erucic-acid_100g , nervonic-acid_100g , trans-fat_100g , cholesterol_100g , fiber_100g , vitamin-a_100g , vitamin-a_100g , v itamin-d_100g , vitamin-e_100g , vitamin-k_100g , vitamin-c_100g , vitamin-b1_100g , vitamin-b2_100g , vitamin-pp_100g , vitamin-b6_100g , vitamin-b9_100g , vitamin-b12_100g , biotin_100g , pantothenic-acid_100g , silica_100g , bicarbonate_100g , potassium_100g , chloride_100g , calcium_100g , phosphorus_100g , iron_100g , magnesium_100g , zinc_100g , copper_100g , manganese_100g , fluoride_100g , selenium_100g , chromium_100g , molybdenum_100g , iodine_100g , caffeine_100g , taurine_100g , ph_100g , fruits-vegetables-nuts_100g , carbon-footprint_100g , nutrition-score-fr_100g , nutrition-score-uk_100g Sections to be Added # Authentication # Describe the authentication procees for the differnet enviroments. The error code for wrong user name and password is 403 for now but you still get html response. API Conventions # List the readable and writeable fields. Specify that in adding or editing a product only the writable fields can be modified. Also list existing API conventions. Country Code # Specify that you can use ll and cc to limit the results you need to a particular country or language. List all possible country codes and language codes. External Links # Open api allows you to add external link to reference where more info can be found. Description # Go through all parameters and response fields , to see if they have been properly described with suitable examples. Reuses # Take out reuses section from introduction, We are working on something else for that.","title":"Open Api Documentation Todo List"},{"location":"reference/API_TODO/#open-api-documentation-todo-list","text":"","title":"Open Api Documentation Todo List"},{"location":"reference/API_TODO/#include-these-missing-fields-into-the-product-response","text":"url , created_datetime , last_modified_datetime , categories_fr , labels_fr , first_packaging_code_geo , cities , countries_fr , no_nutriments , additives , ingredients_from_palm_oil , main_category , main_category_fr , image_small_url , casein_100g , serum-proteins_100g , nucleotides_100g , sucrose_100g , glucose_100g , fructose_100g , lactose_100g , maltose_100g , maltodextrins_100g , starch_100g , polyols_100g , fat_100g , butyric-acid_100g , caproic-acid_100g , caprylic-acid_100g , capric-acid_100g , lauric-acid_100g , myristic-acid_100g , palmitic-acid_100g , stearic-acid_100g , arachidic-acid_100g , behenic-acid_100g , lignoceric-acid_100g , cerotic-acid_100g , montanic-acid_100g , melissic-acid_100g , melissic-acid_100g , monounsaturated-fat_100g , polyunsaturated-fat_100g , omega-3-fat_100g , alpha-linolenic-acid_100g , eicosapentaenoic-acid_100g , docosahexaenoic-acid_100g , omega-6-fat_100g , linoleic-acid_100g , arachidonic-acid_100g , gamma-linolenic-acid_100g , dihomo-gamma-linolenic-acid_100g , omega-9-fat_100g , oleic-acid_100g , elaidic-acid_100g , gondoic-acid_100g , mead-acid_100g , erucic-acid_100g , nervonic-acid_100g , trans-fat_100g , cholesterol_100g , fiber_100g , vitamin-a_100g , vitamin-a_100g , v itamin-d_100g , vitamin-e_100g , vitamin-k_100g , vitamin-c_100g , vitamin-b1_100g , vitamin-b2_100g , vitamin-pp_100g , vitamin-b6_100g , vitamin-b9_100g , vitamin-b12_100g , biotin_100g , pantothenic-acid_100g , silica_100g , bicarbonate_100g , potassium_100g , chloride_100g , calcium_100g , phosphorus_100g , iron_100g , magnesium_100g , zinc_100g , copper_100g , manganese_100g , fluoride_100g , selenium_100g , chromium_100g , molybdenum_100g , iodine_100g , caffeine_100g , taurine_100g , ph_100g , fruits-vegetables-nuts_100g , carbon-footprint_100g , nutrition-score-fr_100g , nutrition-score-uk_100g","title":"Include these missing fields into the product response"},{"location":"reference/API_TODO/#sections-to-be-added","text":"","title":"Sections to be Added"},{"location":"reference/API_TODO/#authentication","text":"Describe the authentication procees for the differnet enviroments. The error code for wrong user name and password is 403 for now but you still get html response.","title":"Authentication"},{"location":"reference/API_TODO/#api-conventions","text":"List the readable and writeable fields. Specify that in adding or editing a product only the writable fields can be modified. Also list existing API conventions.","title":"API Conventions"},{"location":"reference/API_TODO/#country-code","text":"Specify that you can use ll and cc to limit the results you need to a particular country or language. List all possible country codes and language codes.","title":"Country Code"},{"location":"reference/API_TODO/#external-links","text":"Open api allows you to add external link to reference where more info can be found.","title":"External Links"},{"location":"reference/API_TODO/#description","text":"Go through all parameters and response fields , to see if they have been properly described with suitable examples.","title":"Description"},{"location":"reference/API_TODO/#reuses","text":"Take out reuses section from introduction, We are working on something else for that.","title":"Reuses"},{"location":"reference/perl/","text":"Perl reference documentation # The documentation in Plain Old Format (aka POD) for perl module is compiled from in file documentation. See the Perl reference documentation","title":"Perl reference documentation"},{"location":"reference/perl/#perl-reference-documentation","text":"The documentation in Plain Old Format (aka POD) for perl module is compiled from in file documentation. See the Perl reference documentation","title":"Perl reference documentation"},{"location":"tutorials/using-the-OFF-API-tutorial/","text":"Using the Open Food Facts API # Scan A Product To Get Nutri-score # This basic tutorial shows you can get the Nutri-score of a product, for instance, to display it in a mobile app after scanning the product barcode. Let's use Nutella Ferrero as the product example for this tutorial. To get a product nutriscore, you need to make a call to the Get A Product By Barcode Endpoint. Authentication # No Authentication is required to make a query to Get A Product Nutri-score. Describing the Request # Make a GET request to the Get A Product By Barcode endpoint. https://world.openfoodfacts.org/api/v2/product/ { barcode } The {barcode} is the barcode number of the product you are trying to get. The barcode for Nutella Ferrero is 3017624010701 . Then the request path to get product data for Nutella Ferrero will look like this: https://world.openfoodfacts.org/api/v2/product/3017624010701 The response returns every data about Nutella Ferrero on the database. To get the nutriscore, we need to limit the response by specifying the nutriscore field, which is the nutrition_grades and product_name . Query Parameters # To limit the response of the Get A Product By Barcode response, use query parameters to specify the product fields to be returned. In this example, you need one query parameter called field with the value product_name,nutrition_grades . The request path will now look like this: https://world.openfoodfacts.org/api/v2/product/3017624010701?fields = product_name,nutriscore_data Nutri-score Response # The response returned contains an object of the code , product , status_verbose , and status . The product object contains the fields specified in the query: the product_name and the nutrition_grades . The status also states if the product was found or not. { \"code\" : \"3017624010701\" , \"product\" : { \"nutrition_grades\" : \"e\" , \"product_name\" : \"Nutella\" }, \"status\" : 1 , \"status_verbose\" : \"product found\" } Nutri-score Computation # If you would like to be able to show how the score is computed, add some extra fields like nutriscore_data and nutriments . The request path to get the nutriscore computation for Nutella-Ferroro will be : https://world.openfoodfacts.org/api/v2/product/3017624010701?fields = product_name,nutriscore_data,nutriments,nutrition_grades The product object in the response now contains the extra fields to show how the nutriscore was computed. { \"code\" : \"3017624010701\" , \"product\" : { \"nutriments\" : { \"carbohydrates\" : 57.5 , \"carbohydrates_100g\" : 57.5 , \"carbohydrates_unit\" : \"g\" , \"carbohydrates_value\" : 57.5 , \"energy\" : 2255 , \"energy-kcal\" : 539 , \"energy-kcal_100g\" : 539 , \"energy-kcal_unit\" : \"kcal\" , ... , ... , \"sugars\" : 56.3 , \"sugars_100g\" : 56.3 , \"sugars_unit\" : \"g\" , \"sugars_value\" : 56.3 }, \"nutriscore_data\" : { \"energy\" : 2255 , \"energy_points\" : 6 , \"energy_value\" : 2255 , ... , ... , \"sugars_points\" : 10 , \"sugars_value\" : 56.3 }, \"nutrition_grades\" : \"e\" , \"product_name\" : \"Nutella\" }, \"status\" : 1 , \"status_verbose\" : \"product found\" } For more details, see the reference documentation for Get A Product By Barcode","title":"Using the Open Food Facts API"},{"location":"tutorials/using-the-OFF-API-tutorial/#using-the-open-food-facts-api","text":"","title":"Using the Open Food Facts API"},{"location":"tutorials/using-the-OFF-API-tutorial/#scan-a-product-to-get-nutri-score","text":"This basic tutorial shows you can get the Nutri-score of a product, for instance, to display it in a mobile app after scanning the product barcode. Let's use Nutella Ferrero as the product example for this tutorial. To get a product nutriscore, you need to make a call to the Get A Product By Barcode Endpoint.","title":"Scan A Product To Get Nutri-score"},{"location":"tutorials/using-the-OFF-API-tutorial/#authentication","text":"No Authentication is required to make a query to Get A Product Nutri-score.","title":"Authentication"},{"location":"tutorials/using-the-OFF-API-tutorial/#describing-the-request","text":"Make a GET request to the Get A Product By Barcode endpoint. https://world.openfoodfacts.org/api/v2/product/ { barcode } The {barcode} is the barcode number of the product you are trying to get. The barcode for Nutella Ferrero is 3017624010701 . Then the request path to get product data for Nutella Ferrero will look like this: https://world.openfoodfacts.org/api/v2/product/3017624010701 The response returns every data about Nutella Ferrero on the database. To get the nutriscore, we need to limit the response by specifying the nutriscore field, which is the nutrition_grades and product_name .","title":"Describing the Request"},{"location":"tutorials/using-the-OFF-API-tutorial/#query-parameters","text":"To limit the response of the Get A Product By Barcode response, use query parameters to specify the product fields to be returned. In this example, you need one query parameter called field with the value product_name,nutrition_grades . The request path will now look like this: https://world.openfoodfacts.org/api/v2/product/3017624010701?fields = product_name,nutriscore_data","title":"Query Parameters"},{"location":"tutorials/using-the-OFF-API-tutorial/#nutri-score-response","text":"The response returned contains an object of the code , product , status_verbose , and status . The product object contains the fields specified in the query: the product_name and the nutrition_grades . The status also states if the product was found or not. { \"code\" : \"3017624010701\" , \"product\" : { \"nutrition_grades\" : \"e\" , \"product_name\" : \"Nutella\" }, \"status\" : 1 , \"status_verbose\" : \"product found\" }","title":"Nutri-score Response"},{"location":"tutorials/using-the-OFF-API-tutorial/#nutri-score-computation","text":"If you would like to be able to show how the score is computed, add some extra fields like nutriscore_data and nutriments . The request path to get the nutriscore computation for Nutella-Ferroro will be : https://world.openfoodfacts.org/api/v2/product/3017624010701?fields = product_name,nutriscore_data,nutriments,nutrition_grades The product object in the response now contains the extra fields to show how the nutriscore was computed. { \"code\" : \"3017624010701\" , \"product\" : { \"nutriments\" : { \"carbohydrates\" : 57.5 , \"carbohydrates_100g\" : 57.5 , \"carbohydrates_unit\" : \"g\" , \"carbohydrates_value\" : 57.5 , \"energy\" : 2255 , \"energy-kcal\" : 539 , \"energy-kcal_100g\" : 539 , \"energy-kcal_unit\" : \"kcal\" , ... , ... , \"sugars\" : 56.3 , \"sugars_100g\" : 56.3 , \"sugars_unit\" : \"g\" , \"sugars_value\" : 56.3 }, \"nutriscore_data\" : { \"energy\" : 2255 , \"energy_points\" : 6 , \"energy_value\" : 2255 , ... , ... , \"sugars_points\" : 10 , \"sugars_value\" : 56.3 }, \"nutrition_grades\" : \"e\" , \"product_name\" : \"Nutella\" }, \"status\" : 1 , \"status_verbose\" : \"product found\" } For more details, see the reference documentation for Get A Product By Barcode","title":"Nutri-score Computation"}]}