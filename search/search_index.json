{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Product Opener (Open Food Facts web server) documentation","text":"<p>Welcome to the documentation of Product Opener, the web server at the heart of Open Food Facts project. </p> <p>It also powers the sibling Open [Beauty|Pet Food|Products] Facts projects</p> <ul> <li>If you want to use the API or are interested in the data, look at API documentation</li> </ul> <ul> <li>If you are a developer, look at Developer documentation</li> </ul> <p>The repository for the project is at https://github.com/openfoodfacts/openfoodfacts-server/</p> <p>Do not hesitate to contribute to this documentation, this is highly appreciated.</p>"},{"location":"api/","title":"Introduction to Open Food Facts API documentation","text":"<p>Everything you need to know about Open Food Facts API.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>Open Food Facts is a food products database made by everyone, for everyone, that can help you make better choices about what you eat. Being open data, anyone can reuse it for any purpose.</p> <p>The Open Food Facts API enables developers to get information like ingredients and nutritional values of products, and even add more facts to the products database. You may use the API to build applications that allow users to contribute to the database and make healthier food choices.</p> <p>The current version of the API is <code>2</code>.</p> <p>Data in the Open Food Facts database is provided voluntarily by users who want to support the program. As a result, there are no assurances that the data is accurate, complete, or reliable. The user assumes the entire risk of using the data.</p>"},{"location":"api/#before-you-start","title":"Before You Start","text":"<ul> <li>The Open Food Facts database is available under the Open Database License.</li> <li>The individual contents of the database are available under the Database Contents License.</li> <li>Product images are available under the Creative Commons Attribution ShareAlike license. They may contain graphical elements subject to copyright or other rights that may, in some cases, be reproduced (quotation rights or fair use).</li> </ul> <p>Please read the Terms and conditions of use and reuse before reusing the data.</p> <p>We want to learn what the Open Food Facts data is used for. It is not mandatory, but we would appreciate it if you  tell us about your use-case so that we can share them with the Open Food Facts community.</p>"},{"location":"api/#how-to-best-use-the-api","title":"How to Best Use the API","text":""},{"location":"api/#general-principles","title":"General principles","text":"<ul> <li>You can search for product information, including many useful computed values.</li> <li>Suppose we don't have the information you need on a specific product. In that case, you (or your users) can upload the product photos, and the backend (and our AI algorithms!) will process them, generating helpful info. The photos will also be available for the users of OpenFoodFacts and every other API user.</li> <li>You could also ask your user to enter some of the information about the product (like name, category, and weight) so that they immediately get the computed info.</li> </ul> <p>Generally, the more information we have about a product, the more we can compute it.</p>"},{"location":"api/#if-your-users-do-not-expect-a-result-immediately-eg-inventory-apps","title":"If your users do not expect a result immediately (e.g., Inventory apps)","text":"<ul> <li>Submit photos (front packaging/nutrition values/ingredients): the most painless thing for your users.</li> <li>The backend (Product Opener) and Open Food Facts AI (Robotoff) will generate some derived data from the photos.</li> <li>Over time, other apps and the Open Food Facts community will fill the data gaps.</li> </ul>"},{"location":"api/#if-your-users-expect-a-result-immediately-eg-nutrition-apps","title":"If your users expect a result immediately (e.g., Nutrition apps)","text":"<ul> <li>If you submit the product's  nutritional values and category, you'll get the Nutri-Score.</li> <li>If you submit the product ingredients, you'll get the NOVA group (about food ultra-processing), additives, allergens, normalized ingredients, vegan, vegetarian\u2026</li> <li>If you submit the product's  category and labels, you'll (soon) get the Eco-Score (a rating of the product environmental impact)</li> </ul>"},{"location":"api/#api-deployments","title":"API Deployments","text":"<p>The OpenFoodFacts API has two deployments.</p> <ul> <li>Production: https://world.openfoodfacts.org</li> <li>Staging: https://world.openfoodfacts.net</li> </ul> <p>Consider using the staging environment if you are not in a production scenario.</p> <p>While testing your applications, make all API requests to the staging environment. This way, we can ensure the product database is safe.</p> <p>Warning: The staging environment has an extra level of HTTP Basic Authentication (username: <code>off</code>, password: <code>off</code>). When making API requests to staging, you may use https://off:off@world.openfoodfacts.net/ as the base URL to include the authentication.</p>"},{"location":"api/#authentication","title":"Authentication","text":"<ul> <li>READ operations (getting info about a product, etc...) do not require authentication, although we recommend using a custom User-Agent if you're developing an application (to not risk being identified as a bot)</li> </ul> <ul> <li>WRITE operations (Editing an Existing Product, Uploading images\u2026) require authentication. We do this as another layer of protection against spam.</li> </ul> <p>Create an account on the Open Food Facts app. From there, you then have two alternatives:</p> <ul> <li>The preferred one:   Use the login API to get a session cookie and use this cookie in your subsequent request to be authenticated. However, the session must always be used from the same IP address, and you have a maximum of sessions per user.</li> <li>If session conditions are too restrictive for your use case, include your account credentials as parameters for authenticated requests where <code>user_id</code> is your username and <code>password</code> is your password (do this on POST / PUT\u00a0/ DELETE request, not on GET)</li> </ul> <p>You can create a global account to allow your app users to contribute without registering individual accounts on the Open Food Facts website. This way, we know that these contributions came from your application.</p> <p>Production and staging have different account databases, so the account you create in the production environment will only work for production requests. If you want to query (WRITE requests) the staging environment, you'll need to create another account there too.</p>"},{"location":"api/#reference-documentation-openapi","title":"Reference Documentation (OpenAPI)","text":"<p>We are building a complete OpenAPI reference. Here is a list of the current API documentation available:</p> <ul> <li>OpenAPI documentation (v2)</li> <li>OpenAPI documentation for v3 (for packaging components only)</li> <li>A cheatsheet to remind some usual patterns.</li> </ul>"},{"location":"api/#tutorials","title":"Tutorials","text":"<ul> <li>A comprehensive introduction to Using the Open Food Facts API.</li> <li>Uploading images to the Open Food Facts API</li> </ul>"},{"location":"api/#help","title":"Help","text":"<ul> <li>Try the FAQ - to answer most of your questions.</li> <li>Didn't get what you wanted? Contact the Team on the #api Slack Channel.</li> <li>Report Bugs on the Open Food Facts GitHub repository.</li> <li>Do you have an issue or feature request? You can submit it on GitHub too.</li> <li>Are you interested in contributing to this project? See our Contribution Guidelines.   </li> </ul>"},{"location":"api/#sdks","title":"SDKs","text":"<p>SDKs are available for specific languages to facilitate the usage of the API. We may have a wrapper for your favourite programming language. If we do, you can use it and improve it. However, If we don't, you can help us create it!</p> <p>They will let you consume data and let your users contribute new data. Open-source contributors develop our SDKs, and more contributions are welcome to improve these SDKs. You can start by checking the existing issues in their respective repositories.</p> <p>Warning: Before exploring any SDK, please read the Before You Start section.</p> <p>Also, remember to check the API Reference Documentation first to verify if the problem is in SDK implementation or in the API itself.</p> <ul> <li>Cordova</li> <li>Dart, published on pub.dev</li> <li>Elixir</li> <li>Go</li> <li>NodeJS</li> <li>PHP</li> <li>Laravel</li> <li>Python, published on pypi</li> <li>React Native</li> <li>Ruby</li> <li>Java</li> <li>Rust</li> <li>R</li> </ul>"},{"location":"api/aws-images-dataset/","title":"Open Food Facts AWS images dataset","text":"<p>The Open Food Facts images dataset contains all images uploaded to Open Food Facts and the OCR results on these images obtained using Google Cloud Vision.</p> <p>The dataset is stored in the <code>openfoodfacts-images</code> bucket hosted in the <code>eu-west-3</code> region. All data is stored in a single <code>/data</code> folder.</p> <p>Data is synchronized every month between Open Food Facts server and S3 bucket, as such some recent images are likely to be missing. You should not assume all images are present on the S3 bucket.</p> <p>To know the bucket key associated with an image for the product with barcode '4012359114303', you should first split the barcode the following way: <code>/401/235/911/4303</code>.</p> <p>This splitting process is only relevant for EAN13 (barcodes with 13 digits), for barcodes with a smaller number of digit (like EAN8), the directory path is not splitted: <code>/20065034</code>.</p> <p>To get the raw image '1' for barcode '4012359114303', simply add the image ID: <code>/401/235/911/4303/1.jpg</code>. Here, you will get the \"raw\" image, as sent by the contributor. If you don't need the full resolution image, a 400px resized version is also available, by adding the <code>.400</code> suffix after the image ID: <code>/401/235/911/4303/1.400.jpg</code>.</p> <p>The OCR of the image is a gzipped JSON file, and has the same file name as the raw image, but with the <code>.json.gz</code> extension: <code>/401/235/911/4303/1.json.gz</code></p> <p>To download images, you can either use AWS CLI, or perform an HTTP request directly:</p> <p><code>wget https://openfoodfacts-images.s3.eu-west-3.amazonaws.com/data/401/235/911/4303/1.jpg</code></p> <p>You can know all existing objects (images, OCR results) on the bucket by downloading the gzipped text file <code>s3://openfoodfacts-images/data/data_keys.gz</code>:</p> <p><code>wget https://openfoodfacts-images.s3.eu-west-3.amazonaws.com/data/data_keys.gz</code></p> <p>Then you can easily filter the files you want using <code>grep</code> (raw images, OCR JSON) before downloading them. For example, to keep only 400px versions of all images:</p> <p><code>zcat data_keys.gz | grep '.400.jpg'</code></p>"},{"location":"api/explain-knowledge-panels/","title":"Explanation on Knowledge panels","text":"<p>The Open Food Facts API allows clients (such as the Open Food Facts website and mobile app) to request ready-to-display information about an object (such as a product or a facet like a category).</p> <p>Clients do not have to know in advance what kind of information is displayed (for example - the ingredients of a product, nutrition data, Nutri-Score or Eco-Score). They only have to know how to display essential data types such as texts, grades, images, and tables.</p> <p> Knowledge panels in action on the mobile app</p> <p>Main elements are panels, which in turn will contain elements. Elements are typically <code>text_element</code>, <code>image_element</code>, <code>map_element</code>. Some panels are grouping panels together, forming a hierarchy.</p> <p>The structure of the knowledge panels data returned by the API is described in the knowledge panels JSON schema.</p> <p>See the reference documentation for Getting Knowledge panels for a specific product by barcode.</p>"},{"location":"api/how-to-download-images/","title":"How to download product images","text":"<p>The prefered method of downloading Open Food Facts images depends on what you which to achieve.</p> <p>If you want to download a limited number of images, especially if these images have been uploaded recently, you should download the image from Open Food Facts server.</p> <p>If you plan to download a large amount of images, you should on the contrary use Open Food Facts images dataset hosted on AWS.</p>"},{"location":"api/how-to-download-images/#download-from-aws","title":"Download from AWS","text":"<p>If you want to download a large number of images, this is the recommended option, as AWS S3 will be faster and allow concurrent download, contrary to Open Food Facts server, where you should preferably download images one at a time. See AWS Images dataset for more information about how to download images from AWS dataset.</p>"},{"location":"api/how-to-download-images/#download-from-open-food-facts-server","title":"Download from Open Food Facts server","text":"<p>All images can be found on https://images.openfoodfacts.org/images/products/. Images of a product are stored in a single directory. The path of this directory can be inferred easily from the product barcode. If the product barcode length is lower or equal to 8 (ex: \"22222222\"), the directory path is simply the barcode: all images can be found on <code>https://images.openfoodfacts.org/images/products/{barcode}</code>.</p> <p>Otherwise, the following regex is used to split the barcode into subfolders: <code>r\"^(...)(...)(...)(.*)$\"</code>. For example, the barcode <code>3435660768163</code> is split as follows: <code>343/566/076/8163</code>, and all images of the products can be found on https://images.openfoodfacts.org/images/products/343/566/076/8163.</p> <p>To get the image file names, we have to use the database dump or the API. All images information are stored in the <code>images</code> field. For product 3168930010883, we have:</p> <pre><code>    {\n\"4\": {\n\"uploader\": \"openfoodfacts-contributors\",\n\"uploaded_t\": 1548685211,\n\"sizes\": {\n\"400\": {\n\"h\": 400,\n\"w\": 300\n},\n\"100\": {\n\"w\": 75,\n\"h\": 100\n},\n\"full\": {\n\"h\": 3174,\n\"w\": 2380\n}\n}\n},\n\"3\": {\n\"uploader\": \"openfoodfacts-contributors\",\n\"uploaded_t\": 1537002125,\n\"sizes\": {\n\"full\": {\n\"h\": 3302,\n\"w\": 2476\n},\n\"100\": {\n\"h\": 100,\n\"w\": 75\n},\n\"400\": {\n\"w\": 300,\n\"h\": 400\n}\n}\n},\n\"ingredients_fr\": {\n\"rev\": \"7\",\n\"orientation\": \"0\",\n\"ocr\": 1,\n\"imgid\": \"2\",\n\"y2\": null,\n\"white_magic\": \"0\",\n\"angle\": null,\n\"x1\": null,\n\"x2\": null,\n\"geometry\": \"0x0-0-0\",\n\"normalize\": \"0\",\n\"y1\": null,\n\"sizes\": {\n\"100\": {\n\"h\": 100,\n\"w\": 75\n},\n\"400\": {\n\"w\": 300,\n\"h\": 400\n},\n\"200\": {\n\"w\": 150,\n\"h\": 200\n},\n\"full\": {\n\"h\": 1200,\n\"w\": 900\n}\n}\n},\n\"nutrition_fr\": {\n\"sizes\": {\n\"200\": {\n\"h\": 200,\n\"w\": 150\n},\n\"full\": {\n\"w\": 2476,\n\"h\": 3302\n},\n\"100\": {\n\"w\": 75,\n\"h\": 100\n},\n\"400\": {\n\"w\": 300,\n\"h\": 400\n}\n},\n\"y1\": \"-1\",\n\"normalize\": null,\n\"x2\": \"-1\",\n\"geometry\": \"0x0--8--8\",\n\"x1\": \"-1\",\n\"angle\": 0,\n\"imgid\": \"3\",\n\"white_magic\": null,\n\"y2\": \"-1\",\n\"ocr\": 1,\n\"orientation\": \"0\",\n\"rev\": \"11\"\n},\n\"1\": {\n\"sizes\": {\n\"full\": {\n\"w\": 850,\n\"h\": 1200\n},\n\"100\": {\n\"h\": 100,\n\"w\": 71\n},\n\"400\": {\n\"h\": 400,\n\"w\": 283\n}\n},\n\"uploader\": \"kiliweb\",\n\"uploaded_t\": \"1527184614\"\n},\n\"2\": {\n\"sizes\": {\n\"100\": {\n\"h\": 100,\n\"w\": 75\n},\n\"400\": {\n\"h\": 400,\n\"w\": 300\n},\n\"full\": {\n\"h\": 1200,\n\"w\": 900\n}\n},\n\"uploader\": \"kiliweb\",\n\"uploaded_t\": \"1527184615\"\n},\n\"front_fr\": {\n\"x1\": null,\n\"angle\": null,\n\"y2\": null,\n\"white_magic\": \"0\",\n\"imgid\": \"1\",\n\"rev\": \"4\",\n\"sizes\": {\n\"200\": {\n\"w\": 142,\n\"h\": 200\n},\n\"full\": {\n\"w\": 850,\n\"h\": 1200\n},\n\"400\": {\n\"h\": 400,\n\"w\": 283\n},\n\"100\": {\n\"w\": 71,\n\"h\": 100\n}\n},\n\"y1\": null,\n\"normalize\": \"0\",\n\"geometry\": \"0x0-0-0\",\n\"x2\": null\n}\n}\n</code></pre> <p>The keys of the map are the keys of the images. These keys can be:</p> <ul> <li>digits: the image is the raw image sent by the contributor (full resolution).</li> <li>selected images: <code>front_{lang}</code>, <code>nutrition_{lang}</code> and     <code>ingredients_{lang}</code>, selected as front, nutrition and ingredients images     respectively for <code>lang</code>. Here, <code>lang</code> is a 2-letter ISO 639-1 language code     (fr, en, es,...).</li> </ul> <p>Each image is available in different resolutions: <code>100</code>, <code>200</code>, <code>400</code> or <code>full</code>, each corresponding to image height (<code>full</code> means not resized). The available resolutions can be found in the <code>sizes</code> subfield.</p> <p>Selected images have additional fields:</p> <ul> <li><code>rev</code> (as revision) indicates the revision number of the image to use (each     time a new image is selected, cropped or rotated, a new image with an     incremented rev is generated).</li> <li><code>imgid</code>, the image ID of the raw image used to generate the selected image.</li> <li><code>angle</code>, <code>x1</code>, <code>x2</code>, <code>y1</code>, <code>y2</code>: rotation angle and cropping coordinates.</li> </ul> <p>For selected images, the file name is the image key followed by the revision number and the resolution: <code>front_fr.1.400.jpg</code>. For raw images, the file name is either the image ID (<code>1.jpg</code>) or the image ID followed by the resolution (<code>1.100.jpg</code>).</p> <p>To get the full URL, simply concatenate the product directory path and the image name. Examples:</p> <ul> <li>https://images.openfoodfacts.org/images/products/343/566/076/8163/1.jpg</li> <li>https://images.openfoodfacts.org/images/products/343/566/076/8163/1.400.jpg</li> </ul>"},{"location":"api/intro-robotoff/","title":"Introduction to the Robotoff Project","text":"<p>The Robotoff project is intended to complete missing product information by prompting users to confirm predictions inferred by Artificial Intelligence algorithms. These algorithms are calculated based on \"insights\", which are facts about a product that have been extracted or deduced from the product pictures, ingredients, categories, labels, etc.</p>"},{"location":"api/intro-robotoff/#robotoff-documentation","title":"Robotoff documentation","text":"<p>The documentation for Robotoff is located here:</p> <ul> <li>General Documentation</li> <li>API Documentation (OpenApi)</li> </ul>"},{"location":"api/ref-cheatsheet/","title":"Reference: API CheatSheet","text":"<p>This reference cheatsheet gives you a quick reminder to send requests to the OFF API.</p> <p>If you are new to API usage you might look at the tutorial. Also, refer to the API reference documentation for complete information.</p>"},{"location":"api/ref-cheatsheet/#addedit-an-existing-product","title":"Add/Edit an Existing Product","text":""},{"location":"api/ref-cheatsheet/#indicate-the-absence-of-nutrition-facts","title":"Indicate the absence of nutrition facts","text":"<pre><code>no_nutrition_data=on (indicates if the nutrition facts are not indicated on the food label)\n</code></pre>"},{"location":"api/ref-cheatsheet/#add-nutrition-facts-values-units-and-base","title":"Add nutrition facts values, units and base","text":"<pre><code>nutrition_data_per=100g\n\nOR\n\nnutrition_data_per=serving\nserving_size=38g\n</code></pre> <pre><code>nutriment_energy=450\nnutriment_energy_unit=kJ\n</code></pre>"},{"location":"api/ref-cheatsheet/#adding-values-to-a-field-that-is-already-filled","title":"Adding values to a field that is already filled","text":"<p>You just have to prefix <code>add_</code> before the name of the field</p> <pre><code>add_categories\nadd_labels\nadd_brands\n</code></pre>"},{"location":"api/ref-cheatsheet/#search-for-products","title":"Search for Products","text":"<p>Reference documentation for search API</p>"},{"location":"api/ref-cheatsheet/#get-data-for-a-list-of-products","title":"Get data for a list of products","text":"<p>You can use comma to separate multiple values of a query parameter. This allows you to make bulk requests. The product result can also be limited to specified data using <code>fields</code>.</p> <pre><code>https://world.openfoodfacts.org/api/v2/search?code=3263859883713,8437011606013,6111069000451&amp;fields=code,product_name\n</code></pre>"},{"location":"api/ref-v2/","title":"Reference OpenAPI documentation for v2","text":"<p>See api.yml for edition.</p> <p>Do not write anything here, it is meant to be overwritten by html generated from api.yml</p>"},{"location":"api/ref-v3/","title":"Reference OpenAPI documentation for v3","text":"<p>See api-v3.yml for edition.</p> <p>Do not write anything here, it is meant to be overwritten by html generated from api-v3.yml</p>"},{"location":"api/tutorial-dev-journey/","title":"Tutorial - developer journey","text":"<p>Meet Dave.</p> <p>Dave is an active Open Food Facts contributor and a developer who wants to build HealthyFoodChoices, an Android app aimed at conscious consumers that buy healthy products.</p> <p></p> <p>HealthyFoodChoices will query Open Food Facts API and provide information on healthy foods available in the place users are living in. Users can narrow down the results by applying different filters and save their search criteria so that the app shows them the products that match their preferences next time they use it.</p> <p>To identify the potential users' needs, Dave has met with some conscious consumers.</p> <ul> <li>Anna is a 25-year old New Yorker who doesn't drink soda, but her nephew does. She wants to compare the nutrition facts of two cola brands, and its variants (<code>diet</code>, <code>zero</code>, and so on) to decide which one to buy.</li> <li>Stefano is a 36-year old Italian who follows a plant-based diet and wants to avoid the intake of palm oil. He's looking for a breakfast cereal brand that does not use palm oil nor additives and has a great nutriscore (A).</li> </ul> <p>Dev Journey 1: Comparing sodas for Anna Dev Journey 2: Finding healthy breakfast cereals for Stefano Dev Journey 3: Adding missing products Dev Journey 4: Get the Nutri-Score Dev Journey 5 : Get the Eco-Score Dev Journey 6: Get ingredient related analysis on new or existing products (Nova, allergens, additives\u2026)</p>"},{"location":"api/tutorial-off-api/","title":"Tutorial on using the Open Food Facts API","text":"<p>Welcome to this tutorial on basic usage of Open Food Facts API.</p> <p>Fist, be sure to see our Introduction to the API.</p>"},{"location":"api/tutorial-off-api/#scan-a-product-to-get-nutri-score","title":"Scan A Product To Get Nutri-score","text":"<p>This basic tutorial shows you can get the Nutri-score of a product, for instance, to display it in a mobile app after scanning the product barcode. Let's use Nutella Ferrero as the product example for this tutorial.</p> <p>To get a product nutriscore, send a request to the Get A Product By Barcode endpoint.</p>"},{"location":"api/tutorial-off-api/#authentication","title":"Authentication","text":"<p>Usually, no authentication is required to query Get A Product Nutri-score. However, there is a basic auth to avoid content indexation in the staging environment(which is used throughout this tutorial). For more details, visit the Open Food Facts API Environment.</p>"},{"location":"api/tutorial-off-api/#describing-the-get-request","title":"Describing the Get Request","text":"<p>Make a <code>GET</code> request to the <code>Get A Product By Barcode</code> endpoint.</p> <pre><code>https://world.openfoodfacts.net/api/v2/product/{barcode}\n</code></pre> <p>The <code>{barcode}</code> is the barcode number of the product you are trying to get. The barcode for Nutella Ferrero is 3017624010701. Then the request path to get product data for Nutella Ferrero will look like this:</p> <pre><code>https://world.openfoodfacts.net/api/v2/product/3017624010701\n</code></pre> <p>The response returns every data about Nutella Ferrero on the database. To get the nutriscore, we need to limit the response by specifying the nutriscore field, which is the <code>nutrition_grades</code> and <code>product_name</code>.</p>"},{"location":"api/tutorial-off-api/#query-parameters","title":"Query Parameters","text":"<p>To limit the response of the Get A Product By Barcode response, use query parameters to specify the product fields to be returned. In this example, you need one query parameter called <code>field</code> with the value <code>product_name,nutrition_grades</code>.</p> <p>The request path will now look like this:</p> <pre><code>https://world.openfoodfacts.net/api/v2/product/3017624010701?fields=product_name,nutriscore_data\n</code></pre>"},{"location":"api/tutorial-off-api/#nutri-score-response","title":"Nutri-Score Response","text":"<p>The response returned contains an object of the <code>code</code>, <code>product</code>, <code>status_verbose</code>, and <code>status</code>. The <code>product</code> object contains the fields specified in the query: the <code>product_name</code> and the <code>nutrition_grades</code>. The status also states if the product was found or not.</p> <pre><code>{\n\"code\": \"3017624010701\",\n\"product\": {\n\"nutrition_grades\": \"e\",\n\"product_name\": \"Nutella\"\n},\n\"status\": 1,\n\"status_verbose\": \"product found\"\n}\n</code></pre>"},{"location":"api/tutorial-off-api/#nutri-score-computation","title":"Nutri-Score Computation","text":"<p>If you would like to be able to show how the score is computed, add some extra fields like <code>nutriscore_data</code> and <code>nutriments</code>.</p> <p>The request path to get the Nutri-Score computation for Nutella-Ferroro will be :</p> <pre><code>https://world.openfoodfacts.net/api/v2/product/3017624010701?fields=product_name,nutriscore_data,nutriments,nutrition_grades\n</code></pre> <p>The <code>product</code> object in the response now contains the extra fields to show how the nutriscore was computed.</p> <pre><code>{\n\"code\": \"3017624010701\",\n\"product\": {\n\"nutriments\": {\n\"carbohydrates\": 57.5,\n\"carbohydrates_100g\": 57.5,\n\"carbohydrates_unit\": \"g\",\n\"carbohydrates_value\": 57.5,\n\"energy\": 2255,\n\"energy-kcal\": 539,\n\"energy-kcal_100g\": 539,\n\"energy-kcal_unit\": \"kcal\",\n...,\n...,\n\"sugars\": 56.3,\n\"sugars_100g\": 56.3,\n\"sugars_unit\": \"g\",\n\"sugars_value\": 56.3\n},\n\"nutriscore_data\": {\n\"energy\": 2255,\n\"energy_points\": 6,\n\"energy_value\": 2255,\n...,\n...,\n\"sugars_points\": 10,\n\"sugars_value\": 56.3\n},\n\"nutrition_grades\": \"e\",\n\"product_name\": \"Nutella\"\n},\n\"status\": 1,\n\"status_verbose\": \"product found\"\n}\n</code></pre> <p>For more details, see the reference documentation for Get A Product By Barcode.</p>"},{"location":"api/tutorial-off-api/#completing-products-to-get-the-nutri-score","title":"Completing products to get the Nutri-Score","text":""},{"location":"api/tutorial-off-api/#products-without-a-nutri-score","title":"Products without a Nutri-Score","text":"<p>When these fields are missing in a nutriscore computation response, it signifies that the product does not have a Nutri-Score computation due to some missing nutrition data. Let's look at the 100% Real Orange Juice. If the product nutrition data is missing some fields, you can volunteer and contribute to it by getting the missing tags and writing to the OFF API to add them.</p> <p>To know the missing tags, check the <code>misc-tags</code> field from the product response.</p> <p><code>https://world.openfoodfacts.net/api/v2/product/0180411000803/100-real-orange-juice?fields=misc_tags</code></p> <p>The response shows the missing fields and category needed to compute the Nutri-Score.</p> <pre><code>{\n\"code\": \"0180411000803\",\n\"product\": {\n\"misc_tags\": [\n\"en:nutriscore-not-computed\",\n\"en:nutriscore-missing-category\",\n\"en:nutrition-not-enough-data-to-compute-nutrition-score\",\n\"en:nutriscore-missing-nutrition-data\",\n\"en:nutriscore-missing-nutrition-data-sodium\",\n\"en:ecoscore-extended-data-not-computed\",\n\"en:ecoscore-not-computed\",\n\"en:main-countries-new-product\"\n]\n},\n\"status\": 1,\n\"status_verbose\": \"product found\"\n}\n</code></pre> <p>The sample response above for 100% Real Orange Juice <code>misc_tags</code> shows that the Nutri-Score is missing category (<code>en:nutriscore-missing-category</code>) and sodium(salt) (<code>en:nutriscore-missing-nutrition-data-sodium</code>). Now you can write to the OFF API to provide these nutriment data (if you have them) so that the Nutri-Score can be computed.</p>"},{"location":"api/tutorial-off-api/#write-data-to-make-nutri-score-computation-possible","title":"Write data to make Nutri-Score computation possible","text":"<p>The WRITE operations in the OFF API require  authentication. Therefore you need a valid <code>user_id</code> and <code>password</code>  to write the missing nutriment data to 100% Real Orange Juice.</p> <p>Sign up on the Open Food Facts App to get your<code>user_id</code> and <code>password</code>if you don't have one.</p> <p>To write data to a product, make a <code>POST</code> request to the <code>Add or Edit A Product</code> endpoint.</p> <pre><code>https://world.openfoodfacts.net/cgi/product_jqm2.pl\n</code></pre> <p>Add your valid <code>user_id</code> and <code>password</code> as body parameters to your request for authentication. The <code>code</code> (barcode of the product to be added/edited), <code>user_id</code>, and <code>password</code> are required when adding or editing a product. Then, include other product data to be added in the request body.</p> <p>To write <code>sodium</code> and <code>category</code> to 100% Real Orange Juice so that the Nutri-Score can be computed, the request body should contain these fields :</p> Key Value Description user_id *** A valid user_id password *** A valid password code 0180411000803 The barcode of the product to be added/edited nutriment_sodium 0.015 Amount of sodium nutriment_sodium_unit g Unit of sodium relative to the amount categories Orange Juice Category of the Product <p>Using curl:</p> <pre><code>curl -XPOST -u off:off -x POST https://world.openfoodfacts.net/cgi/product_jqm2.pl \\\n-F user_id=your_user_id -F password=your_password \\\n-F code=0180411000803 -F nutriment_sodium=0.015 -F nutriment_sodium_unit=g -F categories=\"Orange Juice\"\n</code></pre> <p>If the request is succesful, it returns a response that indicated that the fields have been saved.</p> <pre><code>{\n\"status_verbose\": \"fields saved\",\n\"status\": 1\n}\n</code></pre>"},{"location":"api/tutorial-off-api/#read-newly-computed-nutri-score","title":"Read newly computed Nutri-Score","text":"<p>Now, let's check if the Nutri-Score for 100% Real Orange Juice has been computed now that we have provided the missing data. Make a GET request to <code>https://world.openfoodfacts.net/api/v2/product/0180411000803?fields=product_name,nutriscore_data,nutriments,nutrition_grades</code> for Nutri-Score of 100% Real Orange Juice. The response now contains the Nutri-Score computation:</p> <pre><code>{\n\"code\": \"0180411000803\",\n\"product\": {\n\"nutriments\": {\n\"carbohydrates\": 11.864406779661,\n.\n.\n.\n\"sugars_unit\": \"g\",\n\"sugars_value\": 11.864406779661\n},\n\"nutriscore_data\": {\n\"energy\": 195,\n\"energy_points\": 7,\n\"energy_value\": 195,\n.\n.\n.\n\"sugars_value\": 11.86\n},\n\"nutrition_grades\": \"c\",\n\"product_name\": \"100% Real Orange Juice\"\n},\n\"status\": 1,\n\"status_verbose\": \"product found\"\n}\n</code></pre> <p>For more details, see the reference documentation for Add or Edit A Product</p> <p>You can also check the reference cheatsheet to know how to add/edit other types of product data.</p>"},{"location":"api/tutorial-off-api/#search-for-a-product-by-nutri-score","title":"Search for a Product by Nutri-score","text":"<p>Using the Open Food Facts API, you can filter products based on different criteria.  To search for products in the Orange Juice category with a nutrition_grade of <code>c</code>, query the Search for Products endpoint.</p>"},{"location":"api/tutorial-off-api/#describing-the-search-request","title":"Describing the Search Request","text":"<p>Make a <code>GET</code> request to the <code>Search for Products</code> endpoint.</p> <pre><code>https://world.openfoodfacts.org/api/v2/search\n</code></pre> <p>Add the search criteria used to filter the products as query parameters.  For Orange Juice with a nutrition_grade of <code>c</code>, add query parameters <code>categories_tags_en</code> to filter <code>Orange Juice</code> while <code>nutrition_grades_tags</code> to filter <code>c</code>.  The response will return all the products in the database with the category <code>Orange Juice</code> and nutrition_grade <code>c</code>.</p> <pre><code>https://world.openfoodfacts.net/api/v2/search?categories_tags_en=Orange Juice&amp;nutrition_grades_tags=c\n</code></pre> <p>To limit the response, add <code>fields</code> to the query parameters to specify the fields to be returned in each product object response.  For this tutorial, limit the response to <code>code</code>, <code>product_name</code>, <code>nutrition_grades</code>, and <code>categories_tags_en</code>.</p> <pre><code>https://world.openfoodfacts.net/api/v2/search?categories_tags_en=Orange Juice&amp;nutrition_grades_tags=c&amp;fields=code,nutrition_grades,categories_tags_en\n</code></pre> <p>The response returns all products that belong to the Orange Juice category, with the nutrition_grade \"c\" and limits each product object response to only the specified fields.  It also returns the count(total number) of products that match the search criteria.</p> <pre><code>{\n\"count\": 1629,\n\"page\": 1,\n\"page_count\": 24,\n\"page_size\": 24,\n\"products\": [\n{\n\"categories_tags_en\": [\n\"Plant-based foods and beverages\",\n\"Beverages\",\n\"Plant-based beverages\",\n\"Fruit-based beverages\",\n\"Juices and nectars\",\n\"Fruit juices\",\n\"Concentrated fruit juices\",\n\"Orange juices\",\n\"Concentrated orange juices\"\n],\n\"code\": \"3123340008288\",\n\"nutrition_grades\": \"c\"\n},\n.\n.\n.\n{\n\"categories_tags_en\": [\n\"Plant-based foods and beverages\",\n\"Beverages\",\n\"Plant-based beverages\",\n\"Fruit-based beverages\",\n\"Juices and nectars\",\n\"Fruit juices\",\n\"Non-Alcoholic beverages\",\n\"Orange juices\",\n\"Squeezed juices\",\n\"Squeezed orange juices\"\n],\n\"code\": \"3608580844136\",\n\"nutrition_grades\": \"c\"\n}\n],\n\"skip\": 0\n}\n</code></pre>"},{"location":"api/tutorial-off-api/#sorting-search-response","title":"Sorting Search Response","text":"<p>You can proceed to also sort the search response by different fields, for example, sort by the product that was modified last or even by the product_name. Now, let's sort the products with Orange Juice and a nutrition_grade of \"c\" by when they were last modified. To sort the search response, add the <code>sort_by</code> with value <code>last_modified_t</code> as a query parameter to the request.</p> <pre><code>https://world.openfoodfacts.net/api/v2/search?nutrition_grades_tags=c&amp;fields=code,nutrition_grades,categories_tags_en&amp;categories_tags_en=Orange Juice&amp;sort_by=last_modified_t\n</code></pre> <p>The date that each product was last modified is now used to order the product response.</p> <pre><code>{\n\"count\": 1629,\n\"page\": 1,\n\"page_count\": 24,\n\"page_size\": 24,\n\"products\": [\n{\n\"categories_tags_en\": [\n\"Plant-based foods and beverages\",\n\"Beverages\",\n\"Plant-based beverages\",\n\"Fruit-based beverages\",\n\"Juices and nectars\",\n\"Fruit juices\",\n\"Orange juices\"\n],\n\"code\": \"3800014268048\",\n\"nutrition_grades\": \"c\"\n},\n'\n'\n'\n{\n\"categories_tags_en\": [\n\"Plant-based foods and beverages\",\n\"Beverages\",\n\"Plant-based beverages\",\n\"Fruit-based beverages\",\n\"Juices and nectars\",\n\"Fruit juices\",\n\"Orange juices\",\n\"Squeezed juices\",\n\"Squeezed orange juices\"\n],\n\"code\": \"4056489641018\",\n\"nutrition_grades\": \"c\"\n}\n],\n\"skip\": 0\n}\n</code></pre> <p>To see other examples of sorting a search response, see the reference documentation for Search for Products.</p>"},{"location":"api/tutorial-uploading-photo-to-a-product/","title":"Tutorial on uploading images to the Open Food Facts API","text":"<p>This basic tutorial shows you how to upload an image of a product to the Open Food Facts API.</p> <p>Be sure to also read the introduction to the API</p>"},{"location":"api/tutorial-uploading-photo-to-a-product/#points-to-consider-before-uploading-photos","title":"Points to consider before uploading photos","text":""},{"location":"api/tutorial-uploading-photo-to-a-product/#image-license","title":"Image license","text":"<p>Product images must be under the Creative Commons Attribution ShareAlike licence.</p> <p>That means you should either upload:</p> <ul> <li>photos that are your own work</li> <li>photos taken by your users, with their consent for this license (should be part of your service terms)</li> <li>photos already under this license or a compatible license (cc-by, cc-0 or public domain)</li> </ul>"},{"location":"api/tutorial-uploading-photo-to-a-product/#image-quality","title":"Image Quality","text":"<p>Uploading quality photos of a product, its ingredients, and the nutrition table is essential because it enables the Open Food Facts OCR system to retrieve important data to analyze the product. The minimal allowed size for photos is 640 x 160 px.</p>"},{"location":"api/tutorial-uploading-photo-to-a-product/#upload-behavior","title":"Upload Behavior","text":"<p>In case you upload more than one photo of the front, the ingredients, the nutrition facts, or the product packaging components, beware that only the latest \"selected\" photo of each category will be displayed on the product page on the website and on the mobile application. The older ones are saved and can be \"selected\" by an API call or via the editing interface (website and mobile application). You can also upload some photos that are neither of that 4 categories, but they will not be displayed by default. However, all photos will be saved.</p>"},{"location":"api/tutorial-uploading-photo-to-a-product/#label-languages","title":"Label Languages","text":"<p>Multilingual products have several photos based on the languages present on the packaging. You can specify the language by adding a lang code suffix to the image field.</p>"},{"location":"api/tutorial-uploading-photo-to-a-product/#authentication","title":"Authentication","text":"<p>The WRITE operations in the OFF API require authentication. Therefore you need a valid <code>user_id</code> and <code>password</code> to write the photo to 100% Real Orange Juice.</p> <p>Sign up on the Open Food Facts App to get your <code>user_id</code> and <code>password</code> if you dont have one. For more details, visit the Open Food Facts Authentication.</p>"},{"location":"api/tutorial-uploading-photo-to-a-product/#parameters","title":"Parameters","text":""},{"location":"api/tutorial-uploading-photo-to-a-product/#code","title":"Code","text":"<p>The barcode of the product.</p>"},{"location":"api/tutorial-uploading-photo-to-a-product/#imagefield","title":"Imagefield","text":"<p><code>imagefield</code> indicates the type of image you are trying to upload for a particular product. It can be either of these: <code>front</code>, <code>ingredients</code>, <code>nutrition</code>, <code>packaging</code> or <code>other</code>. You can also specify the language in that image by adding a suffix of the language code to the <code>imagefield</code> value. For example \u2014 <code>front_en</code>, <code>packaging_fr</code>.</p>"},{"location":"api/tutorial-uploading-photo-to-a-product/#imageupload","title":"ImageUpload","text":"<p><code>imageupload</code> must contain the binary content of the image. This field name is dependent on the  <code>imagefield</code>.  It must be <code>imgupload_</code> suffixed by the value of the <code>imagefield</code> stated earlier. Here are some examples:</p> <ul> <li>imgupload_front (if imagefield=front)</li> <li>imgupload_ingredients_fr (if imagefield=ingredients_fr)</li> <li>imgupload_nutrition (if imagefield=nutrition)</li> <li>imgupload_packaging (if imagefield=packaging)</li> </ul>"},{"location":"api/tutorial-uploading-photo-to-a-product/#describing-the-post-request","title":"Describing the Post Request","text":"<p>To upload photos to a product, make a <code>POST</code> request to the <code>Add a Photo to an Existing Product</code> endpoint.</p> <pre><code>https://off:off@world.openfoodfacts.net/cgi/product_image_upload.pl\n</code></pre>"},{"location":"api/tutorial-uploading-photo-to-a-product/#upload-photo-of-a-product","title":"Upload Photo of a Product","text":"<p>Add a <code>user_id</code> and <code>password</code> as body parameters to the request for authentication. The <code>code</code> (barcode of the product to be updated) is required to indicate the product for the uploaded photo. Then, include other product data to be added in the request body.</p> <p>To add a new image for ingredients in English  to 100% Real Orange Juice, the request body should contain these fields :</p> Key Value Description user_id *** A valid user_id password *** A valid password code 0180411000803 The barcode of the product to be added/edited imagefield ingredients_en The type of image to be uploaded imgupload_ingredients_en file The binary content of the image of the product ingredients in English <p>If the image is in the <code>images/real-orange-juice-ingredients.jpg</code>, we can use curl (thanks to the special '@' attributes, which enables reading from a file):</p> <pre><code>curl -XPOST -u off:off  https://world.openfoodfacts.net/cgi/product_image_upload.pl \\\n-F user_id=your_user_id -F password=your_password \\\n-F code=0180411000803 -F imagefield=ingredients_en -F imgupload_ingredients_en=@images/real-orange-juice-ingredients.jpg\n</code></pre> <p>If the request is successful, it returns a response that indicates that the fields have been saved. You will also get the new image id in <code>imgid</code>.</p> <pre><code>{\n\"files\": [\n{\n\"url\": \"/product/0180411000803/100%-real-orange-juice\",\n\"filename\": \"\",\n\"name\": \"100% Real Orange Juice\",\n\"thumbnailUrl\": \"/images/products/018/041/100/0803.jpg\",\n\"code\": \"0180411000803\"\n}\n],\n\"image\": {\n\"thumb_url\": \"123.100.jpg\",\n\"imgid\": 123,\n\"crop_url\": \"123.400.jpg\"\n},\n\"imgid\": 123,\n\"status\": \"status ok\",\n\"imagefield\": \"ingredients_en\",\n\"code\": \"0180411000803\"\n}\n</code></pre> <p>If the request is unsuccessful, the response returns <code>\"status\": \"status not ok\"</code> and an explanation in <code>debug</code> field.</p>"},{"location":"dev/","title":"Introduction to Product Opener developer documentation","text":"<p>This documentation is for developers who wants to understand technical aspects of Product Opener.</p> <p>To use the API, see API Documentation</p> <p>The repository for the project is at https://github.com/openfoodfacts/openfoodfacts-server/</p> <p>Some documentation to get you started:</p> <ul> <li>Quick start guide (Docker)</li> <li>Developer guide (Docker)</li> <li>Developer guide (Gitpod)</li> </ul> <p>Note: documentation follows the Di\u00e1taxis Framework</p>"},{"location":"dev/explain-nutition-data/","title":"Explain nutition data","text":"<p>Product fields:</p> <ul> <li>nutrition_data_per: Whether it is per \"serving\" or \"100g\"</li> <li>serving_size: Entered data from packaging</li> <li>serving_quantity: Computed value in g</li> </ul> <p>For each nutrient:</p> <ul> <li>_value: What was entered</li> <li>_unit: Unit of what was entered</li> <li>_100g: Amount per 100g in original unit</li> <li>_serving: Amount per serving normalised unit</li> <li>no suffix: What was entered in normalised unit</li> <li>_label: Entered label for an unknown untrient?</li> </ul>"},{"location":"dev/explain-packaging-data/","title":"Explanation on packaging data","text":"<p>This document explains how packaging data is currently added, updated and structured in the Open Food Facts database, and how it could be improved.</p>"},{"location":"dev/explain-packaging-data/#introduction","title":"Introduction","text":""},{"location":"dev/explain-packaging-data/#types-of-packaging-data","title":"Types of packaging data","text":"<p>Food products typically have 1 or more packaging components (e.g. milk may have a bottle and a cap).</p> <p>For each product, we aim to have a comprehensive list of all its packaging components, with detailed information about each packaging component.</p>"},{"location":"dev/explain-packaging-data/#data-about-packaging-components","title":"Data about packaging components","text":"<p>For each packaging component, we want data for different attributes, like its shape (e.g. a bottle) and its size (e.g. plastic).</p> <p>There are many different attributes that can be interesting for specific uses. For instance, researchers in epidemiology are interested in knowing which packaging component is in contact with the food itself, and which one can be put in the microwave oven, so that they can study the long term effects of some plastics on health.</p>"},{"location":"dev/explain-packaging-data/#sources-of-packaging-data","title":"Sources of packaging data","text":"<p>We can get packaging data from different sources:</p>"},{"location":"dev/explain-packaging-data/#users","title":"Users","text":"<p>Users of the Open Food Facts website and app, and users of 3rd party apps, can enter packaging data.</p>"},{"location":"dev/explain-packaging-data/#manufacturers","title":"Manufacturers","text":"<p>Some manufacturers send product data through GS1, which currently has limited support for packaging information (but this is likely to be improved in the years to come).</p> <p>Some manufacturers send us more detailed packaging data (e.g. recycling instructions) through the Producers Platform.</p> <p>Some manufacturers send us data used to compute the Eco-Score using the Eco-Score spreadsheet template, which has fields like \"Packaging 1\", \"Material 1\", \"Packaging 2\", \"Material 2\" etc.</p>"},{"location":"dev/explain-packaging-data/#product-photos-and-machine-learning","title":"Product photos and machine learning","text":"<p>We can extract logos related to packaging, or parse the text recognized from product photos to recognize packaging information or recycling instructions.</p>"},{"location":"dev/explain-packaging-data/#how-packaging-data-is-currently-added-updated-and-structured-in-open-food-facts","title":"How packaging data is currently added, updated and structured in Open Food Facts","text":"<p>In Open Food Facts, we currently have a number of input fields related to packaging. The data in those fields is parsed and analyzed to create a structured list of packaging components with specific attributes.</p>"},{"location":"dev/explain-packaging-data/#current-input-fields","title":"Current input fields","text":""},{"location":"dev/explain-packaging-data/#packaging-tag-field-read-and-write","title":"Packaging tag field (READ and WRITE)","text":"<p>At the start of Open Food Facts in 2012, we had a \"packaging\" tag field where users could enter comma separated free text entries about the packaging (e.g. \"Plastic\", \"Bag\" or \"Plastic bag\") in different languages.</p> <p>In 2020, we made this field a taxonomized field. As a result, we now store the language used to fill this field, so that we can match its value to the multilingual packaging taxonomy. So \"plastique\" in French will be mapped to the canonical \"en:plastic\" entry.</p>"},{"location":"dev/explain-packaging-data/#packaging-information-recycling-instructions-text-field-read-and-write","title":"Packaging information / recycling instructions text field (READ and WRITE)","text":"<p>In 2020, we also added a language specific field (\"packaging_text_[language code]\" e.g. \"packaging_text_en\" for English) to store free text data about the packaging. It can contain the text of the recycling instructions printed on the packaging (e.g. \"bottle to recycle, cap to discard\"), or can be filled in by users (e.g. \"1 PET plastic bottle to recycle, 1 plastic cap\").</p>"},{"location":"dev/explain-packaging-data/#current-resulting-packagings-data-structure-read-only","title":"Current resulting packagings data structure (READ only)","text":"<p>The input fields are analyzed and combined to create the \"packagings\" data structure.</p> <p>The structure is an array of packaging components. Each packaging component can have values for different attributes:</p> <ul> <li>number: the number of units for the packaging component (e.g. a pack of beers may contain 6 bottles)</li> <li>shape: the general shape of the packaging component (e.g. \"bottle\", \"box\")</li> <li>material: the material of the packaging component</li> <li>quantity: how much product the packaging component contains (e.g. \"25 cl\")</li> <li>recycling: whether the packaging component should be recycled, discarded or reused</li> </ul> <p>The \"shape\" and \"material\" fields are taxonomized using the packaging_shapes and packaging_materials taxonomies.</p>"},{"location":"dev/explain-packaging-data/#how-the-the-resulting-packagings-data-structure-is-created","title":"How the the resulting packagings data structure is created","text":""},{"location":"dev/explain-packaging-data/#extract-attributes-that-relate-to-different-packaging-components","title":"Extract attributes that relate to different packaging components","text":"<p>The values for each input field (\"packaging\" tag field and \"packaging_text_[language code]\" packaging information text field) are analyzed1 to recognize packaging components and their attributes. One product may have multiple \"packaging_text_[language code]\" values in different languages. Only the value for the main product of the language is currently analyzed.</p> <p>For instance, if the \"packaging\" field contains \"Plastic bottle, box, cardboard\", we will use the packaging shapes, materials and recycling taxonomies to create a list of 3 packaging components: {shape:\"en:bottle\", material:\"en:plastic\"}, {shape:\"en:box\"}, {material:\"en:cardboard\"}.</p> <p>And if the \"packaging_text_en\" field contains \"PET bottle to recycle, box to reuse\", we will create 2 more packaging components: {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"}, {shape:\"box\", recycling:\"reuse\"}.</p>"},{"location":"dev/explain-packaging-data/#merge-packaging-components","title":"Merge packaging components","text":"<p>The 3 + 2 = 5 resulting packaging components are then added one by one in the packagings structure. When their attributes are compatible, the packaging units are merged2. For instance {shape:\"en:box\"} and {material:\"en:cardboard\"} have non conflicting attributes, so they are merged into {shape:\"en:box\", material:\"en:cardboard\"}. Note that it is possible that this is a mistake, and that the \"box\" and \"cardboard\" tags concern in fact different components.</p> <p>Similarly, as \"en:plastic\" is a parent of \"en:pet-polyethylene-terephthalate\" in the packaging_materials taxonomy, we can merge {shape:\"en:bottle\", material:\"en:plastic\"} with {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"} into {shape:\"en:bottle\", material:\"en:pet-polyethylene-terephthalate\", recycling:\"en:recycle\"}.</p> <p>The resulting structure is:</p> <pre><code>packagings: [\n    {\n        material: \"en:pet-polyethylene-terephthalate\",\n        recycling: \"en:recycle\",\n        shape: \"en:bottle\"\n    },\n    {\n        recycling: \"en:reuse\",\n        shape: \"en:box\"\n    },\n    {\n        shape: \"en:container\"\n    }\n]\n</code></pre>"},{"location":"dev/explain-packaging-data/#taxonomies","title":"Taxonomies","text":"<p>We have created a number of multilingual taxonomies related to packagings:</p> <ul> <li>Packaging shapes taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_shapes.txt</li> <li>Packaging materials taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_materials.txt</li> <li>Packaging recycling taxonomy : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/packaging_recycling.txt</li> <li>Preservation methods taxonomy (related) : https://github.com/openfoodfacts/openfoodfacts-server/blob/main/taxonomies/preservation.txt</li> </ul> <p>Those taxonomies are used to structure packaging data in Open Food Facts, and to analyze unstructured input data.</p>"},{"location":"dev/explain-packaging-data/#how-we-could-improve-it","title":"How we could improve it","text":""},{"location":"dev/explain-packaging-data/#extend-the-attributes-of-the-packaging-components-in-the-packagings-data-structure","title":"Extend the attributes of the packaging components in the \"packagings\" data structure","text":""},{"location":"dev/explain-packaging-data/#weight","title":"Weight","text":"<p>We need to add an attribute for the weight of the packaging component. We might need to add different fields to distinguish values that have been entered by users that weight the packaging, versus values provided by the manufacturer, or average values that we have determined from other products, or that we got from external sources.</p>"},{"location":"dev/explain-packaging-data/#make-the-packagings-data-structure-read-and-write","title":"Make the \"packagings\" data structure READ and WRITE","text":"<p>The \"packagings\" data structure is currently a READ only field. We could create an API to make it a READ and WRITE field.</p> <p>For new products, clients (website and apps) could ask users to enter data about all packaging components of the product.</p> <p>For existing products, clients could display the packaging components and let users change them (e.g. adding or removing components, entering values for new attributes, editing attributes to add more precise values (e.g. which type of plastic) etc.).</p>"},{"location":"dev/explain-packaging-data/#add-a-way-to-indicate-that-the-packagings-data-structure-contains-all-the-packaging-components-of-the-product","title":"Add a way to indicate that the \"packagings\" data structure contains all the packaging components of the product","text":"<p>We currently have no way to know if the packaging data we have for a product is complete, or if we may be missing some packaging components.</p> <p>We could have a way (e.g. a checkbox) that users could use to indicate all components are accounted for. And we could also do the reverse, and indicate that it is very likely that we are missing some packaging components (e.g. if we have a \"cap\" but no other component to put the cap on).</p>"},{"location":"dev/explain-packaging-data/#deprecate-the-packaging-tags-field","title":"Deprecate the \"packaging\" tags field","text":"<p>We could discard the existing \"packaging\" tags field, and replace it with an API to allow clients to add partial information about packaging components.</p> <p>For instance, if Robotoff detects that the product is in plastic bottle by analyzing a product photo, it could send {shape:\"bottle\", material:\"en:plastic\"} and it would be added / combined with the existing \"packagings\" data.</p>"},{"location":"dev/explain-packaging-data/#keep-the-packaging_text_language-code-field","title":"Keep the \"packaging_text_[language code]\" field","text":"<p>It is important to keep this field, as we can display it as-is, use it as input data, and it may contain interesting data that we do not analyze yet.</p> <p>When filled, the values for this field can be analyzed and added to / combined with the \"packagings\" data structure. Similarly to ingredient text analysis, we could keep information about which parts of the text were recognized as attributes of a packaging component, and which parts were not recognized and were therefore ignored.</p> <p>Changing the \"packagings\" value will not change the \"packaging_text_[language code]\" values.</p>"},{"location":"dev/explain-packaging-data/#challenges","title":"Challenges","text":""},{"location":"dev/explain-packaging-data/#incomplete-lists-of-packaging-components","title":"Incomplete lists of packaging components","text":""},{"location":"dev/explain-packaging-data/#slightly-mismatched-data-from-different-sources","title":"Slightly mismatched data from different sources","text":"<p>For a single product, we might get partial packaging data from different sources that we map to similar but distinct shapes, like \"bottle\", \"jar\" and \"jug\". It may be difficult to determine if the data concerns a single packaging component, or different components.</p>"},{"location":"dev/explain-packaging-data/#products-with-packaging-changes","title":"Products with packaging changes","text":""},{"location":"dev/explain-packaging-data/#resources","title":"Resources","text":"<ul> <li>2020 project to start structuring packaging data: https://wiki.openfoodfacts.org/Packagings_data_structure</li> </ul> <ol> <li> <p>parse_packaging_from_text_phrase() function in /lib/ProductOpener/Packagings.pm \u21a9</p> </li> <li> <p>analyze_and_combine_packaging_data() function in /lib/ProductOpener/Packagings.pm \u21a9</p> </li> </ol>"},{"location":"dev/explain-pro-dev-setup/","title":"Explanation on Docker Setup of pro platform for development","text":"<p>This explains how we setup docker file for pro platform development. For explanations on how to use it, see: how-to-guides/pro-development</p> <p>off is the public facing application (world.openfoodfacts.org) off-pro is the producers platform (world.pro.openfoodfacts.org)</p> <p>When we work on the pro platform for development we want:</p> <ul> <li>off containers to talk between each other, and have their own volumes</li> <li>off-pro containers to talk between each other, and, generally, have their own volumes</li> <li>minion and backend from both app access to the same postgres database   (which stores tasks queues)</li> <li>off and off-pro backends / minion needs to share some volumes :   orgs, users ands some files living in podata</li> </ul> <p>Still we would like to avoid having different clone of the repository, but we can isolate projects thanks to <code>COMPOSE_PROJECT_NAME</code>, which will prefix containers names, volumes and default network, thus isolate each projects.</p> <p>This is achieved by sourcing the .env-pro which setup some environment variables that will superseed the .env variables. The main one being setting <code>COMPOSE_PROJECT_NAME</code> and <code>PRODUCERS_PLATFORM</code>, but also other like <code>MINION_QUEUE</code>.</p> <p>On volume side, we will simply give hard-coded names to volumes that should be shared between off and pro platform, thus they will be shared. Ideally we should not have to share single files but this is a work in progress, we will live without it as a first approx.</p> <p>To satisfy the access to the same database, we will use postgres database from off as the common database.</p> <p>In order to achieve that:</p> <ul> <li>we use profiles, so we won't start postgres in pro docker-compose</li> <li>we connect <code>postgres</code>, <code>backend</code> and <code>minion</code> services to a shared network, called <code>minion_db</code> Fortunately this works, but note that there is a pitfall: on <code>minion_db</code> network both <code>backend</code> services (<code>off</code> and <code>off-pro</code>) will respond to same name. For the moment it is not a problem for we don't need to communicate directly between instances. If it was, we would have to define custom aliases for those services on the <code>minion_db</code> network.</li> </ul> <pre><code>network    OFF              network       PRO              network\npo_default containers       minion_db     containers       po_pro_default\n    |                          |                              |\n    +------postgres------------+                              |\n    |                          |                              |\n    |                          |                              |\n    +-----backend--------------+                              |\n    |                          +----------backend-------------+\n    |                          |                              |\n    +------minion--------------+                              |\n    |                          +----------minion--------------+\n    |                          |                              |\n    |                          |                              |\n    +------frontend            |          frontend------------+\n    +------mongodb             |          mongodb-------------+\n    |                          |                              |\n</code></pre>"},{"location":"dev/explain-taxonomy-build-cache/","title":"Explanation on Taxonomy Build Cache","text":"<p>Taxonomies have a significant impact on OFF processing and automated test results so need to be rebuilt before running any tests. However, this process takes some time, so the built taxonomy files are cached in a GitHub repository so that they only need to be rebuilt when there is a genuine change.</p>"},{"location":"dev/explain-taxonomy-build-cache/#how-it-works","title":"How it works","text":"<p>A hash is calculated for all of the source files used to build a particular taxonomy and GitHub is then checked to see if a cache already exists for that hash.</p> <p>If no cached build is found then the taxonomy is rebuilt and cached locally.</p> <p>If the GITHUB_TOKEN environemnt variable is set then the cached build is also uploaded to the https://github.com/openfoodfacts/openfoodfacts-build-cache repository. Note that no token is required to download previous cached builds from the repo.</p>"},{"location":"dev/explain-taxonomy-build-cache/#obtaining-a-token","title":"Obtaining a token","text":"<p>The GITHUB_TOKEN is a personal access token, created here: https://github.com/settings/tokens. Only the public_repo scope is needed.</p>"},{"location":"dev/explain-taxonomy-build-cache/#considerations","title":"Considerations","text":"<p>In maintaining this code be aware of the following complications...</p>"},{"location":"dev/explain-taxonomy-build-cache/#circular-dependencies","title":"Circular Dependencies","text":"<p>There is a cicular dependency between taxonomies, languages and foods. The foods library is used to create the source for the nutrient_levels taxonomy, which uses transalations from languages. However, languages depends on the languages taxonomy...</p> <p>This is currently resolved by building the taxonomy on the fly if it is requested but not currently built.</p>"},{"location":"dev/explain-taxonomy-build-cache/#taxonomy-dependencies","title":"Taxonomy Dependencies","text":"<p>Some taxonomies perform lookups on others, e.g. additives_classes are referenced by additives, so the referenced taxonomy needs to be built first. The build order is determined in the Config_off.pm file.</p>"},{"location":"dev/how-to-deploy/","title":"How to deploy to Prod environment","text":"<p>Note: prod deployment is very manual and not automated yet.</p> <ul> <li>Login to the off1 server, as the \"off\" user</li> <li>cd /home/off/openfoodfacts-server</li> <li>Check that you are on the main branch</li> <li>git pull</li> <li>Copy changed files (don't copy everything, in particular not the lang directory that is being moved to the openfoodfacts-web repository)</li> <li>e.g. cp cgi scripts lib po taxonomies templates /srv/off/</li> <li>cd /srv/off</li> <li>export NPM_CONFIG_PREFIX=~/.npm-global</li> <li>npm install</li> <li>npm run build</li> <li>cd /srv/off/cgi</li> <li>export PERL5LIB=.</li> <li>./build_lang.pl</li> <li>as the root user:</li> <li>systemctl stop apache2@off</li> <li>systemctl start apache2@off</li> <li>systemctl stop minion-off</li> <li>systemctl start minion-off</li> </ul>"},{"location":"dev/how-to-develop-producer-platform/","title":"How to develop on the producers platform","text":"<p>Here is how to develop for the producers platform using docker.</p>"},{"location":"dev/how-to-develop-producer-platform/#prerequisites","title":"Prerequisites:","text":"<ul> <li>Docker should already be set up for development..</li> </ul>"},{"location":"dev/how-to-develop-producer-platform/#shell-setup","title":"Shell Setup:","text":"<p>You will need two types of shells:</p> <ul> <li>Shell for OpenFoodFacts:<ul> <li>Use this shell for general development on the OpenFoodFacts platform.</li> </ul> </li> <li>Shell for OpenFoodFacts-Pro: Use this shell when working on the OpenFoodFacts-Pro platform.<ul> <li>To set up the shell, source the <code>setenv-pro.sh</code> file by running the command: <code>. setenv-pro.sh</code>.</li> <li>Once the shell is set up, your prompt will show <code>(pro)</code> to indicate that you are in the producers environment.(this simply sets some environment variables that will overides the one in <code>.env</code>)</li> </ul> </li> </ul>"},{"location":"dev/how-to-develop-producer-platform/#development-workflow","title":"Development Workflow:","text":"<p>To develop on the producers platform, follow these steps:</p> <ul> <li>Open a shell for OpenFoodFacts-Pro.</li> <li>Run the command <code>make dev</code> to start the development environment. This command functions as usual.<ul> <li>If you encounter any issues with CSS not showing up, you can run <code>make build_lang</code> in the pro shell.</li> </ul> </li> </ul>"},{"location":"dev/how-to-develop-producer-platform/#working-with-product-importexport-and-interacting-with-the-public-platform","title":"Working with Product Import/Export and Interacting with the Public Platform:","text":"<p>If you need to work on product import/export or interact with the public platform, you must start the following services: <code>PostgreSQL</code>, <code>MongoDB</code>, and the <code>Minion</code>. Here's how:</p> <ul> <li>In a non-pro shell (OpenFoodFacts shell), run the command <code>docker-compose up postgres minion mongodb</code>.<ul> <li>This command starts the necessary services in the background.</li> </ul> </li> </ul>"},{"location":"dev/how-to-develop-producer-platform/#note-the-setup-does-not-currently-support-running-the-http-server-for-both-public-and-pro-platform-at-the-same-simultaneously-therefore-to-access-the-public-platform-you-need-to-follow-these-steps","title":"Note: The setup does not currently support running the http server for both public and pro platform at the same simultaneously. Therefore, to access the public platform, you need to follow these steps:","text":"<ul> <li>in your pro shell, run a <code>docker-compose stop frontend</code></li> <li>in your non pro shell, run a <code>docker-compose up frontend</code> Now, the public database can be accessed at <code>openfoodfacts.localhost</code>.If you need to access the pro HTTP server, reverse these steps.</li> </ul> <p>Note that if you use direnv, the setup should work seamlessly without redefining the variables set by <code>setenv-pro.sh</code>.</p> <p>An explanation of the setup can be found at explain-pro-dev-setup.md</p> <ul> <li>If you want to see state of tasks, you can run:</li> </ul> <p><pre><code>docker-compose exec minion /opt/product-opener/scripts/minion_producers.pl  minion job\n</code></pre> (add --help to see all options), or refer to https://docs.mojolicious.org/Minion/Command/minion/job</p> <ul> <li>You may also inspect database by running: <pre><code>docker-compose exec  postgres psql -U productopener -W minion\n</code></pre> The password is given by the <code>POSTGRES_PASSWORD</code> variable in the <code>.env</code> file and defaults to <code>productopener</code>.  Inspecting the minion table can be helpful in understanding the database structure and contents.</li> </ul>"},{"location":"dev/how-to-develop-using-docker/","title":"How to use Docker to Develop - a guide","text":"<p>This guide is for developers and newcomers to help them debug and explore Docker.</p> <p>This page describes how to test and debug your changes once you have set up the project, Product Opener with Docker using dev environment quick start guide.</p>"},{"location":"dev/how-to-develop-using-docker/#checking-logs","title":"Checking logs","text":""},{"location":"dev/how-to-develop-using-docker/#tail-docker-compose-logs","title":"Tail Docker Compose logs","text":"<pre><code>make log\n</code></pre> <p>You will get logs from nginx, mongodb, postgres, etc.</p>"},{"location":"dev/how-to-develop-using-docker/#tail-other-logs","title":"Tail other logs","text":"<p>Most logs from perl are not (yet ?) displayed on the docker logs, but are instead available in specific directories.</p> <p>To see them use:</p> <pre><code>make tail\n</code></pre> <p>It will <code>tail -f</code> all the files present in the <code>logs/</code> directory:</p> <ul> <li><code>apache2/error.log</code></li> <li><code>apache2/log4perl.log</code></li> <li><code>apache2/modperl_error.log</code></li> <li><code>apache2/other_vhosts_access.log</code></li> <li><code>nginx/access.log</code></li> <li><code>nginx/error.log</code></li> </ul> <p>You can also simply run: <pre><code>tail -f &lt;FILEPATH&gt;\n</code></pre> to check a specific log.</p> <p>One of the most important is <code>log4perl.log</code>.</p>"},{"location":"dev/how-to-develop-using-docker/#increasing-log-verbosity","title":"Increasing log verbosity","text":"<p>By default, the <code>log4perl</code> configuration <code>conf/log.conf</code> matches production settings. You can tweak that file with your own dev configuration settings and run <code>make restart</code> to reload the changes.</p> <p>A setting useful for local environments is to set <code>TRACE</code> log level: <pre><code>log4perl.rootLogger=TRACE, LOGFILE\n</code></pre></p>"},{"location":"dev/how-to-develop-using-docker/#opening-a-shell-in-a-docker-container","title":"Opening a shell in a Docker container","text":"<p>Run the following to open a bash shell within the <code>backend</code> container:</p> <pre><code>docker-compose exec backend bash\n</code></pre> <p>You should see <code>root@&lt;CONTAINER_ID&gt;:/#</code> (opened root shell): you are now within the Docker container and can begin typing some commands !</p>"},{"location":"dev/how-to-develop-using-docker/#checking-permissions","title":"Checking permissions","text":"<p>Navigate to the directory the specific directory and run</p> <p><pre><code>ls -lrt\n</code></pre> It will list all the directories and their permission status.</p>"},{"location":"dev/how-to-develop-using-docker/#creating-directory","title":"Creating directory","text":"<p>Navigate to your specific directory using <code>cd</code> command and run</p> <pre><code>mkdir directory-name\n</code></pre>"},{"location":"dev/how-to-develop-using-docker/#running-minion-jobs","title":"Running minion jobs","text":"<p>Minion is a high-performance job queue for Perl. Minion is used in openfoodfacts-server for time-consuming import and export tasks. These tasks are processed and queued using the minion jobs queue. Therefore, they are called minion jobs.</p> <p>Go to <code>/opt/product-opener/scripts</code> and run</p> <pre><code>./minion_producers.pl minion job\n</code></pre> <p>The above command will show the status of minion jobs. Run the following command to launch the minion jobs.</p> <pre><code>./minion_producers.pl minion worker -m production -q pro.openfoodfacts.org\n</code></pre>"},{"location":"dev/how-to-develop-using-docker/#restarting-apache","title":"Restarting Apache","text":"<p>Sometimes restarting the whole <code>backend</code> container is overkill, and you can just restart <code>Apache</code> from inside the container:</p> <pre><code>apache2ctl -k restart\n</code></pre>"},{"location":"dev/how-to-develop-using-docker/#exiting-the-container","title":"Exiting the container","text":"<p>Use <code>exit</code> to exit the container.</p>"},{"location":"dev/how-to-develop-using-docker/#making-code-changes","title":"Making code changes","text":"<p>In the dev environment, any code change to the local directory will be written  on the container. That said, some code changes require a restart of the <code>backend</code>  container, or rebuilding the NPM assets.</p>"},{"location":"dev/how-to-develop-using-docker/#getting-away-from-make-up","title":"Getting away from make up","text":"<p><code>make up</code> is a good command for starters, but it's not the right one to use if you develop on a daily bases, because it maybe slow, as it does a full rebuild, which, in dev mode, should only be necessary in a few cases.</p> <p>On a daily bases you could better run those:</p> <ul> <li><code>docker-compose up</code> to start and monitor the stack.</li> <li><code>docker-compose restart backend</code> to account for a code change in a <code>.pm</code> file   (cgi <code>pl</code> files do not need a restart)</li> <li><code>docker-compose stop</code> to stop them all</li> </ul> <p>If some important file changed (like Dockerfile or cpanfile, etc.), or in case of doubt, you can run <code>docker-compose build</code> (or maybe it's a good time to use <code>make up</code> once)</p> <p>You should explore the docker-compose commands most are useful!</p>"},{"location":"dev/how-to-develop-using-docker/#live-reload","title":"Live reload","text":"<p>To automate the live reload on code changes, you can install the Python package <code>when-changed</code>: <pre><code>pip3 install when-changed\nwhen-changed -r docker/ docker-compose.yml .env -c \"make restart\"                                         # restart backend container on compose changes\nwhen-changed -r lib/ -r docker/ docker-compose.yml -c \"docker-compose backend restart\" # restart Apache on code changes\nwhen-changed -r html/ Dockerfile Dockerfile.frontend package.json -c \"make up\" # rebuild containers on asset or Dockerfile changes\n</code></pre></p> <p>An alternative to <code>when-changed</code> is <code>inotifywait</code>.</p>"},{"location":"dev/how-to-develop-using-docker/#run-queries-on-mongodb-database","title":"Run queries on MongoDB database","text":"<pre><code>docker-compose exec mongodb mongo\n</code></pre> <p>The above command will open a MongoDB shell, allowing you to use all the <code>mongo</code>  commands to interact with the database:</p> <pre><code>show dbs\nuse off\ndb.products.count()\ndb.products.find({_id: \"5053990155354\"})\ndb.products.deleteOne({_id: \"5053990155354\"})\n</code></pre> <p>See the <code>mongo</code> shell docs for more commands.</p>"},{"location":"dev/how-to-develop-using-docker/#adding-environment-variables","title":"Adding environment variables","text":"<p>If you need some value to be configurable, it is best to set is as an environment variable.</p> <p>To add a new environment variable <code>TEST</code>:</p> <ul> <li>In <code>.env</code> file, add <code>TEST=test_val</code> [local].</li> <li>In <code>.github/workflows/container-deploy.yml</code>, add <code>echo \"TEST=${{ secrets.TEST }}\" &gt;&gt; .env</code> to the \"Set environment variables\" build step [remote]. Also add the corresponding GitHub secret <code>TEST=test_val</code>.</li> <li>In <code>docker-compose.yml</code> file, add it under the <code>backend</code> &gt; <code>environment</code> section.</li> <li>In <code>conf/apache.conf</code> file, add <code>PerlPassEnv TEST</code>.</li> <li>In <code>lib/Config2.pm</code>, add <code>$test = $ENV{TEST};</code>. Also add <code>$test</code> to the <code>EXPORT_OK</code> list at the top of the file to avoid a compilation error.</li> </ul> <p>The call stack goes like this:</p> <p><code>make up</code> &gt; <code>docker-compose</code> &gt; loads <code>.env</code> &gt; pass env variables to the <code>backend</code> container &gt; pass to <code>mod_perl</code> &gt; initialized in <code>Config2.pm</code>.</p>"},{"location":"dev/how-to-develop-using-docker/#managing-multiple-deployments","title":"Managing multiple deployments","text":"<p>To juggle between multiple local deployments (e.g: to run different flavors of Open Food Facts on the same host), there are different possible strategies.</p>"},{"location":"dev/how-to-develop-using-docker/#a-set-env-script","title":"a set env script","text":"<p>Docker-compose takes it settings from, in order of priority:</p> <ul> <li>the environment</li> <li>the <code>.env</code> file</li> </ul> <p>So one strategy to have a different instance, can be to keep same <code>.env</code> file, but super-seed some env variables to tweak the configuration. This is a good strategy for the pro plateform.</p> <p>For this case we have a  <code>setenv-pro.sh</code> script.</p> <p>To use it, open a terminal, where you want to be in pro environment and simply use:</p> <pre><code>. setenv-pro.sh\n</code></pre> <p>then you can use whatever docker-compose command.</p> <p>Note: This terminal will remain in <code>pro</code> mode until you end its session.</p> <p>See also Developing on the producers platform</p>"},{"location":"dev/how-to-develop-using-docker/#different-env-file","title":"different .env file","text":"<p>This strategy might be the right one if your settings differs a lot.</p> <p>You will need:</p> <ul> <li> <p>Multiple <code>.env</code> files (one per deployment), such as:</p> <ul> <li><code>.env.off</code> : configuration for Open Food Facts dev env.</li> <li><code>.env.off-pro</code> : configuration for Open Food Facts Producer's Platform dev env.</li> <li><code>.env.obf</code>: configuration for Open Beauty Facts dev env.</li> <li><code>.env.opff</code>: configuration for Open Ped Food Facts dev env.</li> </ul> </li> </ul> <ul> <li><code>COMPOSE_PROJECT_NAME</code> set to different values in each <code>.env</code> file, so that container names across deployments are unique.</li> </ul> <ul> <li><code>FRONTEND_PORT</code> and <code>MONGODB_PORT</code>    set to different values in each <code>.env</code> file,    so that frontend containers don't port-conflict with each other.</li> </ul> <p>To switch between configurations, set <code>ENV_FILE</code> before running <code>make</code> commands, (or <code>docker-compose</code> command):</p> <pre><code>ENV_FILE=.env.off-pro make up # starts the OFF Producer's Platform containers.\nENV_FILE=.env.obf     make up # starts the OBF containers.\nENV_FILE=.env.opff    make up # starts the OPFF containers.\n</code></pre> <p>or export it to keep it for a while:</p> <pre><code>export ENV_FILE=.env.off # going to work on OFF for a while\nmake up\nmake restart\nmake down\nmake log\n</code></pre> <p>A good strategy is to have multiple terminals open, one for each deployment:</p> <ul> <li><code>off</code> [Terminal 1]:   <pre><code>export ENV_FILE=.env.off\nmake up\n</code></pre></li> </ul> <ul> <li><code>off-pro</code> [Terminal 2]:   <pre><code>export ENV_FILE=.env.off-pro\nmake up\n</code></pre></li> </ul> <ul> <li><code>obf</code> [Terminal 3]:   <pre><code>export ENV_FILE=.env.obf\nmake up\n</code></pre></li> </ul> <ul> <li><code>opff</code> [Terminal 3]:   <pre><code>export ENV_FILE=.env.opff\nmake up\n</code></pre></li> </ul> <p>Note: the above case of 4 deployments is a bit ambitious, since ProductOpener's <code>backend</code> container takes about ~6GB of RAM to run, meaning that the above 4 deployments would require a total of 24GB of RAM available.</p>"},{"location":"dev/how-to-learn-perl/","title":"How can I learn the Perl programming language?","text":"<p>Here are some introductory resources to learn Perl:</p>"},{"location":"dev/how-to-learn-perl/#quick-start","title":"Quick start","text":"<ul> <li>Perl Youtube Tutorial - Perl Enough to be dangerous // FULL COURSE 3 HOURS.</li> <li>Perl - Introduction - Introduction to perl from tutorialspoint</li> <li>Impatient Perl - PDF document for people interested in learning perl.</li> </ul>"},{"location":"dev/how-to-learn-perl/#official-documentation","title":"Official Documentation","text":"<ul> <li>Perl.org - Official Perl website with documentation, tutorials, and community resources.</li> <li>Learn Perl - Perl programming language tutorials for beginners.</li> <li>Perl Maven - Perl programming tutorials, tips, and code examples.</li> </ul>"},{"location":"dev/how-to-quick-start-guide/","title":"How to setup the Dev environment (quick start guide)","text":"<p>This guide will allow you to rapidly build a ready-to-use development environment for Product Opener running in Docker. As an alternative to setting up your environment locally, follow the Gitpod how-to guide to instantly provision a ready-to-code development environment in the cloud.</p> <p>First setup time estimate is <code>~10min</code> with the following specs:</p> <ul> <li><code>8 GB</code> of RAM dedicated to Docker client</li> <li><code>6</code> cores dedicated to Docker client</li> <li><code>12 MB/s</code> internet speed</li> </ul>"},{"location":"dev/how-to-quick-start-guide/#1-prerequisites","title":"1. Prerequisites","text":"<p>Docker is the easiest way to install the Open Food Facts server, play with it, and even modify the code.</p> <p>Docker provides an isolated environment, very close to a Virtual Machine. This environment contains everything required to launch the Open Food Facts server. There is no need to install Perl, Perl modules, Nginx, nor Apache separately.</p> <p>NOTE:  New to Perl? Check how to learn perl!</p> <p>Installation steps:</p> <ul> <li>Install Docker CE <p>If you run e.g. Debian, don't forget to add your user to the <code>docker</code> group!</p> </li> <li>Install Docker Compose</li> <li>Enable command-line completion</li> </ul>"},{"location":"dev/how-to-quick-start-guide/#windows-prerequisites","title":"Windows Prerequisites","text":"<p>When running with Windows, install Docker Desktop which will cover all of the above.</p> <p>The Make tasks use a number of Linux commands, such as rm and nproc, so it is recommeded to run Make commands from the Git Bash shell. In addition, the following need to be installed and included in the PATH:</p> <ul> <li>Make for Windows</li> <li>wget for windows (In order to download the full product database).</li> </ul> <p>The process of cloning the repository will create a number of symbolic links which require specific permissions under Windows. In order to do this you can use any one of these alternatives:</p> <ul> <li>Use an Administrative command prompt for all Git commands</li> <li>Completely disable UAC</li> <li>Specifically grant the Create symbolic links permission to your user</li> </ul>"},{"location":"dev/how-to-quick-start-guide/#2-clone-the-repository-from-github","title":"2. Clone the repository from GitHub","text":"<p>You must have a GitHub account if you want to contribute to Open Food Facts development, but it\u2019s not required if you just want to see how it works.</p> <p>Be aware Open Food Facts server takes more than 1.3 GB (2019/11).</p> <p>Choose your prefered way to clone, either:</p>"},{"location":"dev/how-to-quick-start-guide/#on-windows","title":"On Windows:","text":"<p>If you are running Docker on Windows, please use the following git clone command:</p> <p><pre><code>git clone -c core.symlinks=true https://github.com/openfoodfacts/openfoodfacts-server.git\n</code></pre> or <pre><code>git clone -c core.symlinks=true git@github.com:openfoodfacts/openfoodfacts-server.git\n</code></pre></p>"},{"location":"dev/how-to-quick-start-guide/#on-other-systems","title":"On other systems:","text":"<pre><code>git clone git@github.com:openfoodfacts/openfoodfacts-server.git\n</code></pre> <p>or</p> <pre><code>git clone https://github.com/openfoodfacts/openfoodfacts-server.git\n</code></pre> <p>Go to the cloned directory:</p> <pre><code>cd openfoodfacts-server/\n</code></pre>"},{"location":"dev/how-to-quick-start-guide/#3-optional-review-product-openers-environment","title":"3. [Optional] Review Product Opener's environment","text":"<p>Note: you can skip this step for the first setup since the default <code>.env</code> in the repo contains all the default values required to get started.</p> <p>Before running the <code>docker-compose</code> deployment, you can review and configure Product Opener's environment (<code>.env</code> file).</p> <p>The <code>.env</code> file contains ProductOpener default settings: | Field | Description | | ----------------------------------------------------------------- | --- | | <code>PRODUCT_OPENER_DOMAIN</code>                                           | Can be set to different values based on which OFF flavor is run.| | <code>PRODUCT_OPENER_PORT</code>                                             | can be modified to run NGINX on a different port. Useful when running multiple OFF flavors on different ports on the same host.  Default port: <code>80</code>.| | <code>PRODUCT_OPENER_FLAVOR</code>                                           | Can be modified to run different flavors of OpenFoodFacts, amongst <code>openfoodfacts</code> (default), <code>openbeautyfacts</code>, <code>openpetfoodfacts</code>, <code>openproductsfacts</code>.| | <code>PRODUCT_OPENER_FLAVOR_SHORT</code>                                     | can be modified to run different flavors of OpenFoodFacts, amongst <code>off</code> (default), <code>obf</code>, <code>oppf</code>, <code>opf</code>.| | <code>PRODUCERS_PLATFORM</code>                                              | can be set to <code>1</code> to build / run the producer platform.| | <code>ROBOTOFF_URL</code>                                                    | can be set to connect with a Robotoff instance.| | <code>REDIS_URL</code> | can be set to connect with a Redis instance for populating the search index.| | <code>GOOGLE_CLOUD_VISION_API_KEY</code>                                     | can be set to enable OCR using Google Cloud Vision.| | <code>CROWDIN_PROJECT_IDENTIFIER</code> and <code>CROWDIN_PROJECT_KEY</code>            | can be set to run translations.| | <code>GEOLITE2_PATH</code>, <code>GEOLITE2_ACCOUNT_ID</code> and <code>GEOLITE2_LICENSE_KEY</code> | can be set to enable Geolite2.| | <code>TAG</code>                                                             | Is set to <code>latest</code> by default, but you can specify any Docker Hub tag for the <code>frontend</code> / <code>backend</code> images.  Note that this is useful only if you use pre-built images from the Docker Hub (<code>docker/prod.yml</code> override); the default dev setup (<code>docker/dev.yml</code>) builds images locally|</p> <p>The <code>.env</code> file also contains some useful Docker Compose variables:</p> <ul> <li><code>COMPOSE_PROJECT_NAME</code> is the compose project name that sets the prefix to every container name. Do not update this unless you know what you're doing.</li> <li><code>COMPOSE_FILE</code> is the <code>;</code>-separated list of Docker compose files that are included in the deployment:<ul> <li>For a development-like environment, set it to <code>docker-compose.yml;docker/dev.yml</code> (default)</li> <li>For a production-like environment, set it to <code>docker-compose.yml;docker/prod.yml;docker/mongodb.yml</code></li> <li>For more features, you can add:<ul> <li><code>docker/admin-uis.yml</code>: add the Admin UIS container</li> <li><code>docker/geolite2.yml</code>: add the Geolite2 container</li> <li><code>docker/perldb.yml</code>: add the Perl debugger container</li> </ul> </li> </ul> </li> <li><code>COMPOSE_SEPARATOR</code> is the separator used for <code>COMPOSE_FILE</code>.</li> </ul> <p>Note: Instead of modifying <code>.env</code> (with the risk commit it inadvertently), You can also set needed variables in your shell, they will override <code>.env</code> values. Consider creating a <code>.envrc</code> file that you source each time you need to work on the project. On linux and macOS, you can automatically do it if you use direnv.</p>"},{"location":"dev/how-to-quick-start-guide/#4-build-your-dev-environment","title":"4. Build your dev environment","text":"<p>From the repository root, run:</p> <pre><code>make dev\n</code></pre> <p>Note:</p> <p>If you are using Windows, you may encounter issues regarding this command. Take a look at the Troubleshooting section further in this tutorial.</p> <p>Note:</p> <p>If docker complains about <pre><code>ERROR: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network\n</code></pre> It can be solved by adding <code>{\"base\": \"172.80.0.0/16\",\"size\": 24}, {\"base\": \"172.90.0.0/16\",\"size\": 24}</code> to <code>default-address-pools</code> in <code>/etc/docker/daemon.json</code> and then restarting the docker daemon. Credits to https://theorangeone.net/posts/increase-docker-ip-space/ for this solution.</p> <p>The command will run 2 subcommands:</p> <ul> <li><code>make up</code>: Build and run containers from the local directory and bind local code files, so that you do not have to rebuild everytime.</li> <li><code>make import_sample_data</code>: Load sample data into <code>mongodb</code> container (~100 products).</li> </ul> <p>Notes:</p> <ul> <li>The first build can take between 10 and 30 minutes depending on your machine and internet connection (broadband connection heavily recommended, as this will download Docker base images, install Debian and Perl modules in preparation of the final container image).</li> </ul> <ul> <li>You might not immediately see the test products: create an account, login, and they should appear.</li> </ul> <ul> <li>For a full description of available make targets, see docker/README.md</li> </ul> <p>Hosts file:</p> <p>Since the default <code>PRODUCT_OPENER_DOMAIN</code> in the <code>.env</code> file is set to <code>openfoodfacts.localhost</code>, add the following to your hosts file (Windows: <code>C:\\Windows\\System32\\drivers\\etc\\hosts</code>; Linux/MacOSX: <code>/etc/hosts</code>):</p> <pre><code>127.0.0.1 world.openfoodfacts.localhost fr.openfoodfacts.localhost static.openfoodfacts.localhost ssl-api.openfoodfacts.localhost fr-en.openfoodfacts.localhost\n</code></pre> <p>You're done ! Check http://openfoodfacts.localhost/ !</p>"},{"location":"dev/how-to-quick-start-guide/#going-further","title":"Going further","text":"<p>To learn more about developing with Docker, see the Docker developer's guide.</p> <p>To have all site page on your dev instance, see Using pages from openfoodfacts-web</p> <p>Using Repl offers you a way to play with perl.</p> <p>Specific notes are provide on applying AGRIBALYSE updates to support the Ecoscore calculation.</p>"},{"location":"dev/how-to-quick-start-guide/#visual-studio-code","title":"Visual Studio Code","text":"<p>WARNING: for now this is deprecated, some work needs to be done.</p> <p>This repository comes with a configuration for Visual Studio Code (VS Code) development containers (devcontainer). This enables some Perl support in VS Code without the need to install the correct Perl version and modules on your local machine.</p> <p>To use the devcontainer, install prerequisites, clone the repository from GitHub, and (optionally) review Product Opener's environment. Additionally, install Visual Studio Code. VS Code will automatically recommend some extensions, but if you don't want to install all of them, please do install Remote - Containers manually. You can then use the extension command Remote-Containers: Reopen Folder in Container, which will automatically build the container and start the services. No need to use <code>make</code>!</p>"},{"location":"dev/how-to-quick-start-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"dev/how-to-quick-start-guide/#make-dev-error-make-command-not-found","title":"make dev error: make: command not found","text":"<p>When running <code>make dev</code>:</p> <pre><code>bash: make: command not found\n</code></pre> <p>Solution:  Click the Windows button, then type \u201cenvironment properties\u201d into the search bar and hit Enter. Click Environment Variables, then under System variables choose Path and click Edit. Click New and insert C:\\Program Files (x86)\\GnuWin32\\bin, then save the changes. Open a new terminal and test that the command works. (see Make Windows for more)</p>"},{"location":"dev/how-to-quick-start-guide/#make-dev-error-build_lang-error-2-could-not-load-taxonomy-mntpodatataxonomiestracesresultsto","title":"make dev error: [build_lang] Error 2 - Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto","text":"<p>When running <code>make dev</code>:</p> <pre><code>&lt;h1&gt;Software error:&lt;/h1&gt;\n&lt;pre&gt;Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto at /opt/product-opener/lib/ProductOpener/Tags.pm line 1976.\nCompilation failed in require at /opt/product-opener/scripts/build_lang.pl line 31, &amp;lt;DATA&amp;gt; line 2104.\nBEGIN failed--compilation aborted at /opt/product-opener/scripts/build_lang.pl line 31, &amp;lt;DATA&amp;gt; line 2104.\n&lt;/pre&gt;\n&lt;p&gt;\nFor help, please send mail to this site's webmaster, giving this error message\nand the time and date of the error.\n&lt;/p&gt;\n[Tue Apr  5 19:36:40 2022] build_lang.pl: Could not load taxonomy: /mnt/podata/taxonomies/traces.result.sto at /opt/product-opener/lib/ProductOpener/Tags.pm line 1976.\n[Tue Apr  5 19:36:40 2022] build_lang.pl: Compilation failed in require at /opt/product-opener/scripts/build_lang.pl line 31, &lt;DATA&gt; line 2104.\n[Tue Apr  5 19:36:40 2022] build_lang.pl: BEGIN failed--compilation aborted at /opt/product-opener/scripts/build_lang.pl line 31, &lt;DATA&gt; line 2104.\nmake: *** [build_lang] Error 2\n</code></pre> <p>Solution: Project needs Symlinks to be enabled. traces.result.sto is a symlink to allergens.result.sto</p> <p>You have to enable the 'Developer Mode' in order to use the symlinks. To enable Developer Mode:</p> <ul> <li>on windows 10: Settings &gt; Update &amp; Security &gt; 'For developers' \u2026</li> <li>on windows 11: Settings &gt; Privacy &amp; Security &gt; 'For developers' \u2026</li> </ul> <p>and turn on the toggle for Developer Mode.</p> <p>On Windows systems, the git repository needs to be cloned with symlinks enabled.</p> <p>You need to remove current directory where you clone the project, and clone the project again, using right options:</p> <pre><code>git clone -c core.symlinks=true git@github.com:openfoodfacts/openfoodfacts-server.git\n</code></pre>"},{"location":"dev/how-to-quick-start-guide/#rm-is-not-recognized-as-an-internal-or-external-command","title":"'rm' is not recognized as an internal or external command","text":"<p>When running <code>make import_prod_data</code> or some other commands.</p> <p>Solution:</p> <p>Use the Git Bash shell to run the make commands in windows so that programs like nproc and rm are found.</p>"},{"location":"dev/how-to-quick-start-guide/#system-cannot-find-wget","title":"System cannot find wget","text":"<p>When running <code>make import_prod_data</code>.</p> <pre><code>process_begin: CreateProcess(NULL, wget --no-verbose https://static.openfoodfacts.org/data/openfoodfacts-mongodbdump.tar.gz, ...) failed.\nmake (e=2): The system cannot find the file specified.\n</code></pre> <p>You need to install wget for windows. The referenced version is able to use the Windows Certificate Store, whereas the standard gnuwin32 version will give errors about not being able to verify the server certificate.</p>"},{"location":"dev/how-to-quick-start-guide/#make-makefile154-import_sample_data-error-22","title":"make: *** [Makefile:154: import_sample_data] Error 22","text":"<p>When running <code>make import_sample_data</code></p> <pre><code>&lt;hl&gt;Software error:&lt;/h1&gt;\n&lt;pre&gt;MongoDB: :SelectionError: No writable server available. MongoDB server status:\nTopology type: Single; Member status:\nmongodb:27017 (type: Unknown, error: MongoDB::NetworkError: Could not connect to 'mongodb:27017': Temporary failure in name resolution )\n&lt;/pre&gt;\n&lt;p&gt;\nFor help, please send mail to this site's webmaster, giving this error message\nand the time and date of the error.\n&lt;p&gt;\n[Sat Dec 17 19:52:21 2022] update_all_products from_dir_in_mongodb.pl: MongoDB::SelectionError: No writable server available. MongoDB server status:\n\n[Sat Dec 17 19:52:21 2022] update_all_products from_dir_in_mongodb.pl: Topology type: Single; Member status:\n\n[Sat Dec 17 19:52:21 2022] update_all_products from_dir_in_mongodb.pl: mongodb:27017 (type: Unknown, error: MongoDB::NetworkError: Could not connect to 'mongodb:27017': Temporary failure in name resolution )\n\nmake: *** [Makefile:154: import_sample data] Error 22\n</code></pre> <p>Solution: The cause of this issue is that you already have the mongodb database server running on your local machine at port 27017. </p> <p>For linux users:</p> <p>First stop the MongoDB server from your OS <pre><code>sudo systemctl stop mongod\n</code></pre></p> <p>Then check that mongod is stopped with: <pre><code>systemctl status mongod | grep Active\n</code></pre></p> <p>Note: The output of this command should be:   <code>Active: inactive (dead)</code></p> <p>Then, executed this: <pre><code>docker-compose up \n</code></pre></p> <p>Note: To know more about docker-compose commands do read this guide</p>"},{"location":"dev/how-to-quick-start-guide/#make-dev-error-build_lang-error-13-cant-write-into-mntpodatadatalangopenfoodfactslocalhoststo","title":"make dev error: [build_lang] Error 13 - can't write into /mnt/podata/data/Lang.openfoodfacts.localhost.sto","text":"<p>When running <code>make dev</code>:</p> <pre><code>&lt;h1&gt;Software error:&lt;/h1&gt;\n&lt;pre&gt;can't write into /mnt/podata/data/Lang.openfoodfacts.localhost.sto: Permission denied at /opt/product-opener/lib/ProductOpener/Store.pm line 234.\n&lt;/pre&gt;\n&lt;p&gt;\nFor help, please send mail to this site's webmaster, giving this error message\nand the time and date of the error.\n\n&lt;/p&gt;\nmake: *** [Makefile:126: build_lang] Error 13\n</code></pre> <p>Solution:</p> <p>Use the powershell/cmd to run the make dev commands in windows.</p>"},{"location":"dev/how-to-update-agribalyse-ecoscore/","title":"How to update Agribalyse (Ecoscore)","text":"<p>Open Food Facts calculates the Ecoscore of a product from the Categories taxonomy where this has been linked to an AGRIBALYSE food code or proxy.</p> <p>New versions of the AGRIBALYSE database are released from time to time and this document explains how to apply updates. The high-level steps are as follows:</p>"},{"location":"dev/how-to-update-agribalyse-ecoscore/#obtain-and-convert-the-agribalyse-spreadsheet","title":"Obtain and Convert the AGRIBALYSE Spreadsheet","text":"<p>Download the AGRIBALYSE food spreadsheet from the AGRIBALYSE web site (use the French site rather than English as updates on the English site may be delayed), and save it as AGRIBALYSE_vf.xlsm\" in the ecoscore/agribalyse folder.</p> <p>In a backend shell run the ssconvert.sh script. This will re-generate the CSV files, including the AGRIBALYSE_version and AGRIBALYSE_summary files. The AGRIBALYSE_summary file is sorted to make for easier comparison with the previous version.</p> <p>The Ecoscore calculation just uses the data from the \"Detail etape\" tab, which is converted to AGRIBALYSE_vf.csv.2 by ssconvert. The Ecoscore.pm module skips the first three lines of this file to ignore headers. This should be checked for each update as the number of header lines has previously changed. Also check that none of the column headings have changed.</p>"},{"location":"dev/how-to-update-agribalyse-ecoscore/#review-and-fix-any-changed-categories","title":"Review and fix any changed Categories","text":"<p>Review the changes to AGRIBALYSE_summary to determine if any codes have been removed or significantly edited and update the Categories taxonomy accordingly.</p> <p>Once the Categories have been updated you will need to build the taxonomies. You can then update unit test results with the update_tests_results.sh script to see if any have been affected.</p> <p>It is also worth checking the impact the update has had on the main product database. This can be downloaded locally and the differences determined by running the update_all_produycts script.</p> <p>The previous values of the Ecoscore are stored in the previous_data section under ecoscore_data. Before applying an update you will need to delete this section with the following MongoDB script:</p> <pre><code>db.products.update({}, { $unset: { \"ecoscore_data.previous_data\": 0 } });\n</code></pre> <p>You can then use the following script from a backend bash shell to update products:</p> <pre><code>./update_all_products.pl --fields categories --compute-ecoscore\n</code></pre> <p>The process will set the <code>en:ecoscore_grade_changed</code> and <code>en:ecoscore_changed</code> misc_tags, which can be queried to analyse the results. For example, the following script generates a CSV file that summaries all the categories where the grade has changed:</p> <pre><code>var results = db.products\n.aggregate([\n{\n$match: {\nmisc_tags: \"en:ecoscore-grade-changed\",\n},\n},\n{\n$group: {\n_id: {\nen: \"$ecoscore_data.agribalyse.name_en\",\nfr: \"$ecoscore_data.agribalyse.name_fr\",\ncode_before: \"$ecoscore_data.previous_data.agribalyse.code\",\ncode_after: \"$ecoscore_data.agribalyse.code\",\nbefore: \"$ecoscore_data.previous_data.grade\",\nafter: \"$ecoscore_data.grade\",\n},\ncount: { $sum: 1 },\n},\n},\n])\n.toArray();\nprint(\"en.Name,fr.Name,Code Before,Code After,Grade Before,Grade After,Count\");\nresults.forEach((result) =&gt; {\n// eslint-disable-next-line no-underscore-dangle\nvar id = result._id;\nprint(\n'\"' +\n(id.en || \"\").replace(/\"/g, '\"\"') +\n'\",\"' +\n(id.fr || \"\").replace(/\"/g, '\"\"') +\n'\",' +\nid.code_before +\n\",\" +\nid.code_after +\n\",\" +\nid.before +\n\",\" +\nid.after +\n\",\" +\nresult.count\n);\n});\n</code></pre> <p>The following script fetches the specific products that have changed:</p> <pre><code>var products = db.products\n.find(\n{\nmisc_tags: \"en:ecoscore-grade-changed\",\n},\n{\n_id: 1,\n\"ecoscore_data.agribalyse.name_en\": 1,\n\"ecoscore_data.agribalyse.name_fr\": 1,\n\"ecoscore_data_main.agribalyse.code\": 1,\n\"ecoscore_data.previous_data.agribalyse.code\": 1,\n\"ecoscore_data.agribalyse.code\": 1,\n\"ecoscore_data_main.grade\": 1,\n\"ecoscore_data.previous_data.grade\": 1,\n\"ecoscore_data.grade\": 1,\n\"ecoscore_data_main.score\": 1,\n\"ecoscore_data.previous_data.score\": 1,\n\"ecoscore_data.score\": 1,\n\"ecoscore_data_main.agribalyse.ef_total\": 1,\n\"ecoscore_data.previous_data.agribalyse.ef_total\": 1,\n\"ecoscore_data.agribalyse.ef_total\": 1,\ncategories_tags: 1,\n}\n)\n.toArray();\n\nprint(\n\"_id,en.Name,fr.Name,Code Before Main,Code Before Change,Code After,Grade Before Main,Grade Before Change,Grade After,Score Before Main,Score Before Change,Score After,ef_total Before Main,ef_total Before Change,ef_total After,Categories Tags\"\n);\nproducts.forEach((result) =&gt; {\nvar ecoscore_data_main = result.ecoscore_data_main || {};\nvar ecoscore_data_main_agribalyse = ecoscore_data_main.agribalyse || {};\n// eslint-disable-next-line no-underscore-dangle\nprint(\nresult._id +\n',\"' +\n(result.ecoscore_data.agribalyse.name_en || \"\").replace(/\"/g, '\"\"') +\n'\",\"' +\n(result.ecoscore_data.agribalyse.name_fr || \"\").replace(/\"/g, '\"\"') +\n'\",' +\necoscore_data_main_agribalyse.code +\n\",\" +\nresult.ecoscore_data.previous_data.agribalyse.code +\n\",\" +\nresult.ecoscore_data.agribalyse.code +\n\",\" +\necoscore_data_main.grade +\n\",\" +\nresult.ecoscore_data.previous_data.grade +\n\",\" +\nresult.ecoscore_data.grade +\n\",\" +\necoscore_data_main.score +\n\",\" +\nresult.ecoscore_data.previous_data.score +\n\",\" +\nresult.ecoscore_data.score +\n\",\" +\necoscore_data_main_agribalyse.ef_total +\n\",\" +\nresult.ecoscore_data.previous_data.agribalyse.ef_total +\n\",\" +\nresult.ecoscore_data.agribalyse.ef_total +\n',\"' +\nresult.categories_tags.join(\" \") +\n'\"'\n);\n});\n</code></pre>"},{"location":"dev/how-to-update-agribalyse-ecoscore/#link-existing-categories-to-new-agribalyse-codes","title":"Link existing Categories to new AGRIBALYSE codes","text":"<p>If a new AGRIBALYSE category matches and existing OFF Category then the two can be linked by adding an <code>agribalyse_food_code:en</code> tag. If there is not a precise match then add an <code>agribalyse_proxy_food_code:en</code> tag along with the <code>agribalyse_proxy_food_name:en</code> and <code>agribalyse_proxy_food_name:fr</code> tags.</p> <p>Re-run the <code>update_all_products</code> script after doing this to assess how many products now have an Ecoscore when they did not previously. Use the above scripts to analyse the MongoDB, the new categories will have previous values of <code>undefined</code>.</p>"},{"location":"dev/how-to-update-agribalyse-ecoscore/#add-new-categories-for-new-agribalyse-codes","title":"Add new Categories for new AGRIBALYSE codes","text":"<p>For any new categories, review the AGRIBALYSE category descriptions to ensure they are concise and unambiguous such that an OFF user is most likely to get a match on a type-ahead search. Give notice of the change on the taxonomies channel in Slack so that additional translations can be added for the new categories.</p> <p>It is not necessary to add a category for every single AGRIBALYSE entry. For example, AGRIBALYSE has over 80 codes for different mineral waters but these all have almost exactly the same environmental impact. In cases like this it is acceptable to pick a single representative AGRIBALYSE code as a proxy for the Category in general.</p> <p>It may be worth doing a final check to see how many categories cominations still do not have a match to AGRIBALYSE:</p> <pre><code>var missing = db.products\n.aggregate([\n{\n$match: {\n\"ecoscore_data.grade\": null,\n},\n},\n{\n$group: {\n_id: \"$categories_tags\",\ncount: { $sum: 1 },\n},\n},\n])\n.toArray();\nprint(\"Category,Count\");\nmissing.forEach((result) =&gt; {\n// eslint-disable-next-line no-underscore-dangle\nvar id = result._id;\nprint('\"' + (id.join(\",\") || \"\").replace(/\"/g, '\"\"') + '\",' + result.count);\n});\n</code></pre>"},{"location":"dev/how-to-use-direnv/","title":"How to use direnv","text":"<p>As a developer, it can be better not to think too much about setting right env variables as you enter a project. <code>direnv</code> aims at providing a solution.</p> <p>As a quick guide as an openfoodfacts developer:</p> <ul> <li>install direnv on your system using usual package manager</li> <li>in your .bashrc add:     <pre><code># direnv\neval \"$(direnv hook bash)\"\n</code></pre>   you have adapt the direnv line according to what you use, see direnv doc</li> <li>In your project directory add a file, where you superseed variables from <code>.env</code>   that you wan't to</li> </ul> <pre><code>echo \"setting up docker-compose env\"\nexport DOCKER_BUILDKIT=1\nexport USER_UID=${UID}\nexport USER_UID=$(id -g)\n</code></pre> <ul> <li>in project directory, run <code>direnv allow .</code></li> <li>in a new shell:<ul> <li>go in project directory</li> <li>you should have direnv trigger and load your variables</li> </ul> </li> </ul>"},{"location":"dev/how-to-use-gitpod/","title":"How to using Gitpod for Remote Development","text":"<p>If your computer performance restricts you from developing, you can use Gitpod.  Gitpod allows you to do the devs on an ephemeral environment. It is free for a maximum of 500 credits or  50 hours per months (https://www.gitpod.io/pricing).</p> <p>Gitpod provides a robust ready-to-code developer environment in the cloud eliminating the friction of setting up local environments and IDEs with Perl, Docker and plugins, making it possible for even new contributors to OpenFoodFacts Server to get started in minutes instead of hours!</p> <p>Note that while this how-to is tailored for Gitpod, using alternatives like GitHub Codespaces should be similar.</p> <p>For the most part, development on Gitpod is similar to developing locally as documented in the quickstart guide and docker-developer-guide, however accessing your dev-deployment of <code>openfoodfacts-server</code> requires an extra step.</p>"},{"location":"dev/how-to-use-gitpod/#connect-github-and-gitpod","title":"Connect GitHub and Gitpod","text":"<p>When you use Gitpod, you allow Gitpod to use your GitHub account.</p> <p>In GitHub, you can review (and revoke if you stop using Gitpod) the access granted to Gitpod: click on your avatar on top right of the screen, then, Settings. In the left panel, under Integrations, click on Applications, then, Authorized OAuth Apps.</p> <p>On the Gitpod side, you can also update what Gitpod is allowed to do with your GitHub account: click on your avatar on the top right of the screen, then, Settings. In the left panel, click on Integrations. The line for GitHub should be green. At the end of this line, click on the three dots, then Edit Permissions. '''If you want to create a pull request via Gitpod, you need to grant public_repo access.'''</p>"},{"location":"dev/how-to-use-gitpod/#get-started","title":"Get Started","text":"<p>Gitpod will automatically clone and open the repository for you in VSCode by default. It will also automatically build the project for you on opening and comes with Docker and other tools pre-installed making it one of the fastest ways to spin up an environment for <code>openfoodfacts-server</code>.</p> <p>Once the repository is open in Gitpod, other instructions in the quick-start guide can be generally followed.</p>"},{"location":"dev/how-to-use-gitpod/#accessing-your-development-instance-of-openfoodfacts-web","title":"Accessing your development instance of OpenFoodFacts Web","text":"<p>Since Gitpod runs your code in a remote machine, your dev-deployment spun up with <code>make dev</code> or <code>make up</code> will not accessible when you open the default http://openfoodfacts.localhost in your browser. This occurs because the server running on the remote machine is not accessible on your local network interface.</p> <p>To overcome this, we can make use of SSH tunnel that listens to your local port 80 and forwards traffic to the port 80 of the remote machine. Gitpod makes it really simple to SSH into your dev environment by letting you copy the <code>ssh</code> command required to reach your remote environment. To start, follow the ssh instructions on Gitpod's official guide: SSH for workspaces as easy as copy/paste. Once you have copied the ssh command and ensure it works as-is, add a <code>-L 80:localhost:80</code> to the command to make it look like: <code>ssh -L 80:localhost:80 'openfoodfac-openfoodfac-tok-openfoodfac-r9f61214h9vt.ssh.ws-c.gitpod.io'</code>.</p> <p>Once you execute the altered command in your terminal, you should be able to access OpenFoodFacts on http://openfoodfacts.localhost just as documented in the quickstart guide!</p> <p>Remark: for some Linux distributions, the port 80 is reserved. A workaround is to switch to port 8080: in gitpod, open the .env file and replace the line PRODUCT_OPENER_PORT=80 by PRODUCT_OPENER_PORT=8080, then replace -L 80:localhost:80 by -L 8080:localhost:8080. Rollback the changes on .env before to make a pull request!*  </p> <p>Remark: the address to connect with ssh can change after few days. If you get a <code>Connection closed by ... port 22</code> simply go back to https://gitpod.io/workspaces and copy the new address.  </p> <p>Remark: if you load the page after some changes but get a <code>502 Bad Gateway</code> check again your code. Something may be wrong with it. Eventually, try to comment the part you just coded to see if it works. </p> <p>Create an account to be able to edit products.</p>"},{"location":"dev/how-to-use-gitpod/#some-commands","title":"Some commands","text":"<p>After you made devs and want to apply changes and see them on the website, you can run: <pre><code>$ docker-compose restart \n</code></pre> <pre><code>$ make up  \n</code></pre></p> <p>If you face some difficulties, you can always look at the logs (use ctrl + c, to quit): <pre><code>$ make log  \n</code></pre> <pre><code>$ make tail  \n</code></pre> After development, before opening a pull request, run the following command: <pre><code>$ make checks  \n</code></pre></p>"},{"location":"dev/how-to-use-pages-from-openfoodfacts-web/","title":"How to use pages from openfoodfacts-web","text":"<p>To avoid messing product-opener repository with translations of web-pages, we moved most pages in  openfoodfacts-web repository specificly in the lang/ directory.</p> <p>This repo only has a really minimal lang directory named lang-default.</p> <p>If you want to have all contents locally,  you should first clone openfoodfacts-web repo locally,  and then:</p> <ul> <li>if you are using docker,    you can set the <code>WEB_LANG_PATH</code> env variable to a relative or absolute path   leading to openfoodfacts-web lang directory.</li> <li>else, make symlink <code>lang</code> point to openfoodfacts-web lang directory.</li> </ul>"},{"location":"dev/how-to-use-repl/","title":"How to use Perl REPL (re.pl)","text":"<p>NOTE:  New to Perl? Check how to learn perl!</p> <p>On your local dev instance, the \"backend\" container comes with Devel::REPL installed.</p> <p>This is a handy package to try out perl expressions and learn.</p> <p>Thanks to <code>PERL5LIB</code> variable which is already configured, you can load any module of <code>ProductOpener</code> from within it.</p> <p>Also it as the right</p>"},{"location":"dev/how-to-use-repl/#launch-repl","title":"Launch Repl","text":"<p>Just run</p> <pre><code>docker-compose run --rm docker-compose re.pl\n</code></pre> <p>If you want to access external services (like mongodb), do not forget to start them.</p>"},{"location":"dev/how-to-use-repl/#testing-perl-code","title":"Testing perl code","text":"<p>It can be a handy way to get your hand into perl by testing some code patterns, or seeing how they react.</p> <p>For example one can test a regular expression:</p> <pre><code>$ my $text = \"Hello World\";\nHello World\n$ $text =~ /Hello (\\w+)/i\nWorld\n</code></pre>"},{"location":"dev/how-to-use-repl/#reading-a-sto","title":"Reading a sto","text":"<p>Another use case is reading a sto file to see what it contains.</p> <p>Eg. for a user:</p> <pre><code>$ use ProductOpener::Store qw/:all/;\n$ my $user_id = \"xxxx\";\n$ my $user_ref = retrieve(\"/mnt/podata/users/$user_id.sto\");\n</code></pre>"},{"location":"dev/how-to-use-vscode/","title":"How to use VSCode","text":"<p>VSCode (or better the open source version VSCodium) may be used to edit files.</p> <p>Here are some useful tricks.</p>"},{"location":"dev/how-to-use-vscode/#perlcritic","title":"Perlcritic","text":"<p>One way to have perlcritic work is the following:</p> <ul> <li>install the perlcritic   extension</li> <li>add a <code>perlcritic.sh</code> at the root of your project with following content:   <pre><code>#!/usr/bin/env bash\n. .envrc &gt;/dev/null 2&gt;&amp;1\ndocker-compose run --rm --no-deps backend perlcritic \"$@\" 2&gt;/dev/null\n</code></pre>   the second line is useful only if you use direnv</li> <li><code>chmod +x perlcritic.sh</code></li> <li>patch perlcritic by editing its files, following sfodje/perlcritic issue #26</li> <li>the edit perlcritic configuration in workspace to set those values:<ul> <li>Executable: <code>/home/alex/docker/off-server/perlcritic.sh</code></li> </ul> </li> </ul>"},{"location":"dev/how-to-use-vscode/#perl-language-server","title":"Perl Language Server","text":"<p>The extension Language Server and Debugger is less easy to work with !</p> <p>Note: This setup does not work yet, but might not be so far. It is probably due to https://github.com/richterger/Perl-LanguageServer/issues/131</p> <ul> <li>install the extension</li> <li>add a script <code>shell-into-appserver.sh</code> in the project:   <pre><code>#!/usr/bin/env bash\ndeclare -x PATH=$PATH:/usr/local/bin/\nsource .envrc\nCOMMAND=$(echo \"$@\" | sed 's/^.*perl /perl /')\n&gt;&amp;2 echo \"launching $COMMAND\"\ndocker-compose run --rm --no-deps -T -p 127.0.0.  1:13603:13603 backend $COMMAND\n</code></pre>   Note: the second line is useful only if you use direnv</li> <li><code>chmod +x shell-into-appserver.sh</code></li> <li>Edit workspace settings to have those settings:   <pre><code>    \"perl\": {\n\"enable\": true,\n\"perlInc\": [\"/opt/product-opener/lib\", \"/opt/perl/local/lib/perl5\"],\n\"ignoreDirs\": [\"/opt/perl/local/lib/perl5\", \".  vscode\"],\n\"fileFilter\": [\".pm\", \".pl\", \".t\"],\n\"sshAddr\": \"dummy\",\n\"sshUser\": \"dummy\",\n\"sshCmd\": \"./shell-into-appserver.sh\",\n\"sshWorkspaceRoot\": \"/opt/product-opener\",\n\"logLevel\": 2\n},\n</code></pre></li> </ul>"},{"location":"dev/how-to-use-vscode/#remote-container","title":"Remote container ?","text":"<p>Note: at the moment we do not support the Remote Container extension. While we can consider using it, it has some drawback because not all the project is contained within the \"backend\" container. For example all that concern nodejs is in the \"frontend\" container. So it means making a quite complete Docker image on its own with all the tooling necessary.</p>"},{"location":"dev/how-to-write-and-run-tests/","title":"How to write and run tests","text":"<p>If you are a developer you are really encouraged to write tests as you fix bug or develop new features.</p> <p>Having a test is also a good way to debug a particular piece of code.</p> <p>We would really love to see our test coverage augment.</p> <p>If you are new to tests, please read:</p> <ul> <li>something about test pyramid to understand importance of unit tests and integration tests</li> <li>perldoc on test</li> <li>Test::More module doc</li> </ul>"},{"location":"dev/how-to-write-and-run-tests/#helpers","title":"Helpers","text":"<p>We have some helpers for tests.</p> <p>See mainly:</p> <ul> <li>Test.pm (notably <code>init_expected_results</code> and <code>compare_to_expected_results</code>)</li> <li>APITest.pm</li> </ul> <p>and other modules with Test in their name !</p>"},{"location":"dev/how-to-write-and-run-tests/#unit-and-integration-tests","title":"Unit and Integration tests","text":"<p>Unit tests are located in <code>tests/unit/</code>.</p> <p>Integration tests are in <code>tests/integration/</code>.</p> <p>Most integration tests issue queries to an open food facts</p>"},{"location":"dev/how-to-write-and-run-tests/#integration-with-docker-compose","title":"Integration with docker-compose","text":"<p>Using Makefile targets, tests are run </p> <ul> <li>with a specific `COMPOSE_PROJECT_NAME\u00b0 to avoid crashing your development data while running tests (as the project name changes container, network and volumes names)</li> <li>with a specific expose port for Mongodb, to avoid clashes with dev instance.</li> </ul>"},{"location":"dev/how-to-write-and-run-tests/#writing-tests","title":"Writing tests","text":"<p>You can read other tests to understand how we write them (inspire yourself from recently created tests).</p> <p>One effective way is to create a list of tests each represented by a hashmap with inputs and expected outputs and run them in a loop. Add an <code>id</code> and/or a <code>desc</code> (description) and use it as last argument to check functions (like <code>ok</code>, <code>is</code>, \u2026) to easily see tests running and identify failing tests.</p>"},{"location":"dev/how-to-write-and-run-tests/#using-json-files-to-save-expected-results-of-tests","title":"Using JSON files to save expected results of tests","text":"<p>If the output of the function you are testing is small (e.g. a function that returns one single value), the expected return value can be stored in the .t test file.</p> <p>If your outputs are complex and/or large (e.g. for unit tests of functions that return a complex structure, or for API integration tests that return a JSON response), you can use json files to save the expected result of each test, stored on disk. </p> <p>Test.pm contains helper functions to compare results to expected results and to update the expected results. For instance if your function returns a reference $results_ref to a complex object (like a product):</p> <p><code>compare_to_expected_results($results_ref, \"$expected_result_dir/$testid.json\", $update_expected_results);</code></p> <p>After writing the test, you need to use once <code>init_expected_results</code> (see below) to create a JSON file that contains the resulting object.</p> <p>Then the next time you run the test, the results will be compared to the stored expected results.</p> <p>You can also use <code>init_expected_results</code> to generate new expected results file and easily what has changed using it <code>git diff</code>. If the changes are expected, you can commit the new expected results.</p>"},{"location":"dev/how-to-write-and-run-tests/#running-tests","title":"Running tests","text":"<p>The best way to run all test is to run:</p> <pre><code>make tests\n</code></pre> <p>To run a single test you can use:</p> <ul> <li>for unit test:    <pre><code>make unit-test test=\"filename.t\"\n</code></pre></li> <li>for integration test:    <pre><code>make int-test test=\"filename.t\"\n</code></pre></li> </ul> <p>If you made change that impact stored expected results, you can use:</p> <ul> <li>to re-generate all expected results:   <pre><code>make update_tests_results\n</code></pre></li> <li>or to generate expected results for a single test   (here for integration test, <code>unit-test</code> otherwise)   <pre><code>make int-test=\"filename.t --update-expected results\"\n</code></pre></li> </ul> <p>If you re-generate test results, be sure to look carefully that the changes your commit are expected changes.</p>"},{"location":"dev/how-to-write-and-run-tests/#debugging-with-tests","title":"Debugging with tests","text":"<p>Launching a test is a very effective way to understand what's going on in the code using the debugger.</p> <p>This is done calling the test with <code>perl -d</code>. You can also use \"args\" argument with make target:</p> <pre><code>make test-unit test=\"my-test.t\" args=\"-d\"\n</code></pre> <p>Most of the time, you will have to use the next command \"n\" four times, before landing in you test, where you can easily set a breakpoint with <code>b &lt;line-number&gt;</code>.</p> <p>Read perldoc about debugger to learn. more.</p> <p>:pencil: Note: With this explanation, in integration tests that issue requests to the server, you won't be able to run the debugger inside the server code, only in the test.</p>"},{"location":"dev/ref-docker-commands/","title":"Reference Docker / Makefile commands","text":"<p>See also Docker best practice at Open Food Facts</p> <p>The docker/ directory contains <code>docker-compose</code> overrides for running Product Opener on Docker. The main docker-compose file <code>docker-compose.yml</code> is located in the root of the repository.</p> <p>The step-by-step guide to setup the Product Opener using Docker is available on dev environment quick start guide.</p>"},{"location":"dev/ref-docker-commands/#makefile-targets","title":"Makefile targets","text":"<p>Makefile targets are handy for beginners to start the project and for some usual tasks.</p> <p>It's better though, as you progress, if you understand how things work and be able to use targeted docker-compose commands.</p> <p>See also targets to run tests</p> Command Description Notes <code>make dev</code> Setup a fresh dev environment. Run only once, then use the <code>up</code>, <code>down</code>, <code>restart</code> commands. <code>make up</code> Start containers. <code>make down</code> Stop containers and keep the volumes. Products and users data will be kept. <code>make hdown</code> Stop containers and delete the volumes (hard down). Products and users data will be lost ! <code>make restart</code> Restart <code>frontend</code> and <code>backend</code> containers. <code>make reset</code> Run <code>hdown</code> and <code>up</code>. <code>make status</code> Get containers status (up, down, fail). <code>make log</code> Get logs. Include only logs written to container's <code>stdout</code>. <code>make tail</code> Get other logs (<code>Apache</code>, <code>mod_perl</code>, ...) bound to the local <code>logs/</code> directory. <code>make prune</code> Save space by removing unused Docker artifacts. Next build will take time (no cache) ! <code>make prune_cache</code> Remove Docker build cache. Next build will take time (no build cache) ! <code>make clean</code> Clean up your dev environment: removes locally bound folders, run <code>hdown</code> and <code>prune</code>. Run <code>make dev</code> to recreate a fresh dev env afterwards. <code>make import_sample_data</code> Load sample data (~100 products) into the MongoDB database. <code>make import_prod_data</code> Load latest prod data (~2M products, 1.7GB) into the MongoDB database. Takes up to 10m. Not recommended for dev setups !"},{"location":"dev/ref-perl-pod/","title":"Perl reference documentation","text":"<p>Do not write anything here, it is meant to be overwritten by html generated by perl pod.</p>"},{"location":"dev/ref-perl/","title":"Reference Perl code documentation","text":"<p>The documentation in Plain Old Format (aka POD) for perl module is compiled from in file documentation.</p> <p>See the Perl reference documentation</p>"}]}